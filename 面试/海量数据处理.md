1.大量重复-》hashMap

2.出现频数topK-》分治（hash取余+hashmap统计词频）+堆【维护等于分治个数的最大堆】，hashMap，

3.大小topK-》分治（任意分，使用最小堆找到每个文件前k大，再用一个最小堆统计这些最小堆前k大）

3.数据太大一次加载不进内存-》分治【哈希取余对于每个词x，hash(x) % 5000，分成5000个文件，相同的一定在一个文件】，归并

4.中位数-》双堆、分治

5.求重复、不同数据的个数-》位图、hashMap



**hashMap，分治（按hash分配到不同的小文件中），堆，位图**

分治【1.词频，按照hash分治：能把相同元素分在一起，适合重复元素多的场景。2：大小，任意分，使用最小堆找到每个文件前k大，再用一个最小堆】

```
对于大规模数据（如5亿个数）寻找前 kkk 大数，且在内存有限的情况下，可以使用**分块处理**和**最小堆**的方法。以下是解决这个问题的步骤：

### 1. 分块处理数据

将5亿个数分成若干较小的块，每次只加载一个块到内存进行处理，确保每块的数据量小于内存的限制。例如，如果内存可以容纳100万个数，可以将5亿个数分成500个块，每块包含100万个数。

### 2. 在每个块中找出前 kkk 大数

对于每个块的数据，使用最小堆来找出该块的前 kkk 大数。最小堆的大小为 kkk，从而保证只存储 kkk 个最大的数。最小堆的操作复杂度为 O(log⁡k)O(\log k)O(logk)，所以在一个块内找到前 kkk 大数的效率较高。

具体过程是：

- 初始化一个大小为 kkk 的最小堆。
- 遍历块中的每个数，将数加入堆中。
  - 如果堆中元素少于 kkk，则直接加入堆。
  - 如果堆中元素等于 kkk，则将当前数与堆顶元素（堆中最小的数）进行比较。如果当前数大于堆顶，则替换堆顶，并调整堆。

这样每个块的前 kkk 大数就会保存在最小堆中。

### 3. 合并每个块的前 kkk 大数

当每个块的前 kkk 大数都找到之后，将所有块的前 kkk 大数合并为一个新的集合，然后在这个集合中再次找出前 kkk 大数。可以用同样的最小堆方法处理。

这一步的具体操作是：

- 将所有块的前 kkk 大数加载到内存中（共 k×块数k \times \text{块数}k×块数 个数），如果块数较多，可以再次分批处理。
- 对加载的数据再使用最小堆找出前 kkk 大数，得到的即是最终的前 kkk 大数。

### 总结流程

1. 将数据分成多个块，每次加载一个块。
2. 对每个块使用最小堆找到该块的前 kkk 大数。
3. 合并每个块的前 kkk 大数，并使用最小堆找出整个数据集的前 kkk 大数。

### 优点

- **内存友好**：每次只处理一个块，不需要一次性加载全部数据。
- **效率高**：最小堆在处理前 kkk 大数时非常高效，时间复杂度为 O(Nlog⁡k)O(N \log k)O(Nlogk)。
- **通用性强**：该方法可以适用于任何规模的数据，只要能合理划分块。

这种方法可以在内存有限的情况下有效地找出前 kkk 大数。


```





#### 多路归并+败者树

```
这个描述是关于使用败者树来高效解决外部排序问题的方案。外部排序通常用于处理超出内存大小的大数据集，通过结合内存排序和磁盘操作来进行排序。这里我将详细举例说明如何使用败者树进行多路归并排序。

### 具体步骤及举例

假设我们需要对一个非常大的文件进行排序，该文件大小远远超过了内存容量，无法一次性加载到内存中进行排序。假如我们有一个 10 GB 的文件，而内存只能处理 1 GB 的数据，我们可以按照以下步骤来进行外部排序。

#### 步骤 1: 将大文件拆分为多个有序段
- **假设我们有一个 10 GB 的文件**，将其分成 **10 份，每份 1 GB**，这样每次可以加载 1 GB 到内存中进行排序。
- 对于每一个 1 GB 的数据块，使用内存排序算法（如**快速排序**或**归并排序**）对其进行排序，将每个 1 GB 的数据块排好序，得到 **10 个有序段**。
- 排序好的每个段都是有序的。假如我们现在有 10 个有序段（即 10 个 1 GB 的小文件），每个文件内部已经按照升序排好。

#### 步骤 2: 使用多路归并算法进行合并
- 接下来，我们使用**多路归并排序**将这 **10 个有序段**合并为一个整体的有序文件。这时我们用到了**败者树**的概念。
- 败者树是一种帮助从多个有序段中找到最小元素的高效结构。假设我们有 **10 个有序段**，可以构建一个 **败者树**，该树的每个节点代表一个有序段中的当前最小元素。通过败者树的结构，可以快速找到 **10 个有序段中最小的元素**，并将其输出到最终结果文件中。
  
#### 详细举例
- **初始化败者树**：将 **10 个有序段**中的第一个元素分别放入败者树的叶子节点。通过构建败者树，可以快速找到最小的那个元素。
  - 比如 10 个有序段的第一个元素分别是：`2, 5, 9, 4, 8, 11, 3, 7, 6, 1`。
  - 通过败者树比较，发现最小的元素是 `1`。
- **输出最小元素**：将 `1` 输出到最终结果文件中。
- **更新败者树**：从包含 `1` 的那个段中再取出下一个元素加入败者树，重新进行比较。
  - 假设第 10 个段的下一个元素是 `15`，现在败者树的叶子节点变成：`2, 5, 9, 4, 8, 11, 3, 7, 6, 15`。
  - 通过败者树比较，发现最小的元素是 `2`，将其输出到最终结果文件中。
- **重复操作**：重复上述过程，**不断从败者树中找出最小元素输出**，直到所有有序段中的元素都被输出。

#### 多路归并的好处
- **效率高**：败者树在进行多路归并时非常高效，因为它可以快速找到最小的元素。对于 `k` 个有序段，找到最小元素的时间复杂度为 `O(log k)`，而直接比较所有段的复杂度为 `O(k)`。
- **减少磁盘 I/O 次数**：由于每次都是从当前最小的有序段中读取下一个元素，并且将结果直接写入最终的文件中，整个过程的磁盘 I/O 操作次数相对较少，提升了整体效率。

#### 总结
1. **将大文件分成多个小块**，每个块在内存中进行排序，得到有序段。
2. **使用败者树进行多路归并**，每次从多个有序段中找到当前最小的元素输出，直到所有段中的数据全部合并。
3. 败者树通过高效的比较减少了查找最小值的时间，从而使多路归并过程更加高效，适合处理大数据量的外部排序问题。

这样，整个 10 GB 文件最终会变成一个完整的有序文件，整个过程结合了**内存排序**和**外部多路归并**，实现了对大文件的高效排序。
```



## 分治法

分治法是一种解决大规模数据问题的经典方法，通常用于数据量过大，无法一次性加载到内存中时。它的核心思想是将一个大问题分成若干个较小的问题分别处理，最后再将处理结果合并。为了帮助你理解分治法，我将更详细地解释并通过例子来说明。

### 分治法在大数据频次统计中的应用

假设你有 10 个文件，每个文件大小为 1G，总数据量是 10G。内存可能不足以一次性加载 10G 的数据来进行处理。因此，我们需要通过分治法将这些大数据拆分为多个可以加载到内存的小块，分别统计每个小块的数据，然后将结果合并。

#### 步骤分解：

### 1. **划分数据**
由于内存不足以加载所有数据，我们需要将数据拆分成多个小文件，每个文件的数据量较小，可以完全加载到内存中进行处理。

我们可以通过 **Hash 函数** 将 `query` 分散到不同的小文件中。具体方法是：
- 遍历每个原始文件中的数据。
- 对每一个 `query` 计算它的 **哈希值**，并通过哈希值将 `query` 分散到多个文件中。
  - 比如我们可以使用 `hash(query) % 10`，将 `query` 平均分配到 10 个小文件中。
- 每个小文件中的数据总量相对较小，可以完全加载到内存中进行进一步处理。

### 2. **对每个小文件进行统计**
每个小文件中的数据量较小，可以使用内存中常用的 **HashMap** 结构来统计每个 `query` 的出现次数。
- 将每个小文件加载到内存中。
- 使用 **HashMap** 统计该小文件中每个 `query` 的频次。

### 3. **对每个小文件进行排序**
在统计完每个小文件中的 `query` 出现次数后，我们需要对这些 `query` 进行排序。
- 对小文件中的 `query` 依据频次进行降序排序。
- 将排序后的结果存储到新的文件中。

### 4. **合并排序结果**
- 对所有小文件进行排序之后，每个小文件中的 `query` 是按照频次排好序的。
- 由于 `query` 在不同的小文件中是互相独立的（通过哈希函数分配），每个 `query` 只会出现在一个小文件中，因此合并时只需要将各个小文件的排序结果直接连接在一起，或者使用 **归并排序** 合并。

### 分治法的好处
- 通过将数据划分成多个小块，分治法解决了内存不足的问题。我们可以将小块数据完全加载到内存中处理，避免内存溢出。
- 分治法使得数据处理更加高效，可以并行处理每个小文件。

### 示例讲解

假设我们有一个文件 `data.txt`，里面有一些 `query` 数据如下：

```plaintext
apple
banana
apple
orange
banana
apple
orange
grape
```

#### 第一步：**划分数据**

我们用 `hash(query) % 2` 将这些 `query` 分成两个文件（假设这里我们只划分为 2 个小文件，实际情况可以划分更多）。

- **文件 1 (`part0.txt`)**：
  - 包含所有 `hash(query) % 2 == 0` 的 `query`。
  - 假设 `hash(apple) % 2 == 0`, `hash(banana) % 2 == 0`，那么 `part0.txt` 里会包含：
    ```plaintext
    apple
    banana
    apple
    banana
    apple
    ```

- **文件 2 (`part1.txt`)**：
  - 包含所有 `hash(query) % 2 == 1` 的 `query`。
  - 假设 `hash(orange) % 2 == 1`, `hash(grape) % 2 == 1`，那么 `part1.txt` 里会包含：
    ```plaintext
    orange
    orange
    grape
    ```

#### 第二步：**对每个小文件进行统计**

接下来，我们对每个小文件进行统计：

- **文件 1 (`part0.txt`)** 中 `query` 出现的频次：
  - `apple`: 3 次
  - `banana`: 2 次

- **文件 2 (`part1.txt`)** 中 `query` 出现的频次：
  - `orange`: 2 次
  - `grape`: 1 次

#### 第三步：**对每个小文件进行排序**

- **文件 1 (`part0.txt`)** 排序结果：
  ```plaintext
  apple: 3
  banana: 2
  ```

- **文件 2 (`part1.txt`)** 排序结果：
  ```plaintext
  orange: 2
  grape: 1
  ```

#### 第四步：**合并排序结果**

最后，我们将两个文件的结果合并，得到最终的排序结果：

```plaintext
apple: 3
orange: 2
banana: 2
grape: 1
```

由于不同的小文件中的 `query` 是通过哈希函数分配的，不会出现在其他文件中，因此直接合并即可。如果需要进一步排序，可以通过归并排序将多个小文件的结果合并。

### 分治法的 Java 实现思路

**Step 1: 划分文件**

```java
import java.io.*;
import java.util.*;

public class QueryPartitioner {
    public static void main(String[] args) throws IOException {
        // 创建 10 个小文件
        BufferedWriter[] writers = new BufferedWriter[10];
        for (int i = 0; i < 10; i++) {
            writers[i] = new BufferedWriter(new FileWriter("part" + i + ".txt"));
        }

        // 遍历所有文件，将 query 写入不同的小文件中
        BufferedReader reader = new BufferedReader(new FileReader("data.txt"));
        String query;
        while ((query = reader.readLine()) != null) {
            int hash = query.hashCode() % 10;
            writers[Math.abs(hash)].write(query);
            writers[Math.abs(hash)].newLine();
        }
        reader.close();

        // 关闭所有小文件
        for (int i = 0; i < 10; i++) {
            writers[i].close();
        }
    }
}
```

**Step 2: 统计并排序小文件的 `query`**

```java
import java.io.*;
import java.util.*;

public class QuerySorter {
    public static void main(String[] args) throws IOException {
        // 依次处理每个小文件
        for (int i = 0; i < 10; i++) {
            BufferedReader reader = new BufferedReader(new FileReader("part" + i + ".txt"));
            HashMap<String, Integer> queryCount = new HashMap<>();
            String query;
            while ((query = reader.readLine()) != null) {
                queryCount.put(query, queryCount.getOrDefault(query, 0) + 1);
            }
            reader.close();

            // 将每个小文件的统计结果写入排序后的输出文件
            List<Map.Entry<String, Integer>> sortedQueries = new ArrayList<>(queryCount.entrySet());
            sortedQueries.sort((entry1, entry2) -> entry2.getValue().compareTo(entry1.getValue()));

            BufferedWriter writer = new BufferedWriter(new FileWriter("sorted_part" + i + ".txt"));
            for (Map.Entry<String, Integer> entry : sortedQueries) {
                writer.write(entry.getKey() + ": " + entry.getValue());
                writer.newLine();
            }
            writer.close();
        }
    }
}
```

### 总结

**分治法** 的核心是将数据划分成多个小块，分别处理每个小块，并将处理结果合并。通过使用 **Hash 函数** 来划分数据，我们可以确保每个 `query` 分布到不同的小文件中进行统计，最终将所有小文件的排序结果合并，得到全局的排序结果。

这种方法适用于内存不足以一次性处理大规模数据的场景。