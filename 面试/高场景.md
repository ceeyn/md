

https://topjavaer.cn/advance/system-design/2-order-timeout-auto-cancel.html#

https://offercome.cn/md/arch/system/网关技术选型，为什么选择%20Openresty.html

实现二

问为什么就往cap、高可用【集群、分库分表、多级缓存】、高性能【网络、磁盘、cpu、缓存】、高并发(考虑读、写)【异步、缓存、加机器】上想



**加机器-》负载均衡**





高性能【多级缓存、异步、读写分离、负载均衡】、高可用【降级熔断限流排队、超时重试、配置监控告警】



# 六步

**验证码**（前端）

**令牌桶、漏桶**

【**消息队列**】

**redis【集群、分键、pipiline】**redis三步

**本地缓存【创建回调函数，进行更新】【秒杀大闸】**

**mysql【分库分表、索引】**

本地缓存和redis有点像mysql的从库



**通知方式：异步回调、mq、线程隔一段时间主动查询更新**【后台线程更新、请求线程更新】

降级和熔断主要是由**调用方**实现，而不是被调方。



### 高可用七策

多级缓存

异步

负载均衡【加机器，高】

超时重试【级联超时、traceId】

降级【层次降级、读写降级、rpc还有client和server】

熔断

限流

监控告警



### 分库分表

![image-20241016120154184](/Users/moon/Library/Application Support/typora-user-images/image-20241016120154184.png)

### 降级和熔断



【降级（人工、手动）【动态、静态，读降级、写降级、接入层、应用层、分布式缓存、数据库】，熔断，中间件（缓存、消息队列快速返回，异步处理）】



## 短链接系统

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20241215165503621.png" alt="image-20241215165503621" style="zoom: 33%;" />

重定向状态码 302,301 代表永久重定向，浏览器拿到长链之后就会对其缓存，

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20241215192149256.png" alt="image-20241215192149256" style="zoom:33%;" />

同一个长链可以根据不同的用户对应多个短链，



# 扫码登录原理

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20241215193323102.png" alt="image-20241215193323102" style="zoom:50%;" />











## 什么是热 Key 问题？

**热 Key** 问题是指在分布式系统或缓存系统中，某些键（Key）的访问量远远高于其他键，导致这些键所在的节点承受过大的压力，可能引发性能瓶颈或节点故障。热 Key 问题通常出现在高并发的场景下，如秒杀活动、抢购活动等，在这些场景中，某些热门商品的库存信息或用户数据会频繁被访问或修改。

**1.hot-key 检测热 key**

**2.redis 分片**

**3.本地缓存**

**4.消息队列**

**5.对于读的热 key 可以使用读写分离**

### 热 Key 问题的表现

- **节点负载过高**：由于大量请求集中在少数几个 Key 上，**导致这些 Key 所在的缓存节点或数据库节点的 CPU、内存和网络带宽负载过高。**
- **请求延迟增加**：当节点的负载超过其处理能力时，请求的响应时间会显著增加，可能导致用户体验不佳。
- **节点崩溃**：在极端情况下，热点键的过度访问可能导致节点资源耗尽，最终导致节点崩溃，进而引发系统的部分或全部不可用。
- **数据不一致**：在分布式缓存中，热 Key 可能导致数据一致性问题，尤其是在多个副本之间的同步延迟增加时。

### 热 Key 问题的成因

- **流量集中**：在某些业务场景中，用户对某个商品或某些资源的需求特别高，导致这些资源的访问量集中在少数几个 Key 上。
- **不均匀的负载分布**：即使使用了一致性哈希等分布式算法，如果某些 Key 的访问量远超其他 Key，会导致负载分布不均。
- **不合理的缓存设计**：缓存策略设计不合理，如没有考虑到热点数据的特性，也可能导致热 Key 问题。

### 热 Key 问题的应对策略【1。redis分片；2.本地缓存；3.对同一时间redis同一key的大量请求使用redis pipeline】

为了解决或缓解热 Key 问题，可以采用以下策略：

#### 1. **缓存层面的优化**

**1.1 缓存分片（Shard Key）**

将热 Key 的值分片，分散存储在多个不同的 Key 上，以降低单个 Key 的压力。可以通过在 Key 上添加前缀或后缀的方式，将一个 Key 拆分成多个不同的 Key。

**实现方式：**

- 如果 `product:12345` 是一个热 Key，可以将其拆分成 `product:12345:1`、`product:12345:2` 等多个 Key，并将这些 Key 分布在不同的 Redis 节点上。
- 读写时，随机选择一个分片 Key。

```java
public void accessHotKey(String productId) {
    int shardId = new Random().nextInt(10);  // 假设分成了10个片段
    String shardKey = "product:" + productId + ":" + shardId;
    String value = redisTemplate.opsForValue().get(shardKey);
    // 对 shardKey 进行操作
}
```

**1.2 使用本地缓存**

对于某些数据，可以将其缓存在应用服务器的本地缓存中，减少对远程 Redis 或数据库的访问压力。这种方式适合于读多写少的场景。

**实现方式：**

- 使用如 Caffeine 或 Guava 提供的本地缓存框架，将频繁访问的数据缓存在应用服务器的内存中。

```java
LoadingCache<String, String> cache = Caffeine.newBuilder()
    .maximumSize(10_000)
    .expireAfterWrite(10, TimeUnit.MINUTES)
    .build(key -> loadFromRedis(key));
```

**1.3 使用多级缓存**

多级缓存结合了本地缓存和集中缓存的优点，可以在应用服务器上设置一级缓存（如 Caffeine），然后将 Redis 作为二级缓存。这样可以减少热点数据对 Redis 的压力。

#### 2. **流量分散策略**

**2.1 请求去重和合并**

对于同一时间段内对同一 Key 的大量请求，可以采用请求合并策略，将多个请求合并为一个请求，并在结果返回后分发给各个请求。这种策略适合于查询类的操作。

**实现方式：**

- 使用如 Guava 的 `Cache` 或自定义队列，将相同的请求合并。

```java
public void processRequest(String key) {
    synchronized (key) {
        // 判断是否已经有同样的请求正在处理中
        if (isProcessing(key)) {
            waitForResult(key);
        } else {
            startProcessing(key);
        }
    }
}
```

**2.2 请求限流和削峰**

在高并发场景下，通过限流机制控制请求的速率，防止短时间内大量请求压垮系统。可以使用令牌桶、漏桶算法等进行限流，并使用消息队列削峰。

**实现方式：**

- 使用 Redis 的计数器实现简单的限流策略，或者使用如 Sentinel、RateLimiter 等工具。

```java
public boolean isAllowed(String userId) {
    long count = redisTemplate.opsForValue().increment("request:count:" + userId, 1);
    if (count > MAX_LIMIT) {
        return false;
    }
    return true;
}
```

**2.3 使用负载均衡**

将热 Key 的访问请求通过负载均衡器分散到多个应用服务器，避免集中请求压垮单个服务器。负载均衡可以基于请求的源 IP、用户 ID 或其他规则。

#### 3. **数据分布优化**

**3.1 数据预热**

在预期会有大量访问之前（如秒杀活动开始前），预先将热点数据加载到缓存中，减少活动开始时的冷启动问题，并通过分片将数据分布到多个节点。

**实现方式：**

- 在秒杀活动前，将相关商品信息加载到 Redis，并将流量进行提前测试。

**3.2 数据迁移**

对于已经发现的热 Key，可以通过手动或自动迁移，将其数据迁移到其他负载较低的节点，分散其压力。

**实现方式：**

- 监控 Redis 实例的负载情况，手动调整热 Key 所在的节点，或者使用 Redis Cluster 的重分片功能。

#### 4. **数据库层面的优化**

**4.1 异步更新数据库**

对于读多写少的场景，可以在缓存中先更新数据，然后异步地将数据写入数据库，减少数据库的写压力。

**实现方式：**

- 使用消息队列或异步任务处理缓存和数据库的同步。

```java
public void updateInventory(String productId, int quantity) {
    // 更新 Redis 中的库存
    redisTemplate.opsForValue().decrement("product:" + productId + ":stock", quantity);
    // 异步更新数据库
    asyncService.updateInventoryInDatabase(productId, quantity);
}
```

**4.2 分库分表**

将大表拆分成多个小表，分布在不同的数据库实例上，减少单个数据库实例的压力。这种策略适用于大规模数据存储的场景。

### 热 Key 问题的监控和预防

1. **实时监控**：使用 Redis 自带的 `INFO` 命令或集成的监控工具（如 Prometheus+Grafana、Redis Sentinel 等）实时监控 Redis 的负载情况和热点数据的访问情况。
2. **预警机制**：设置合理的预警阈值，当某个 Key 的访问量或某个节点的负载超过预期时，触发告警，及时处理。
3. **自动化调整**：结合自动化脚本或运维工具，动态调整热 Key 的分布或迁移，确保系统的稳定性。

### 总结

热 Key 问题是分布式缓存系统和数据库系统中的常见问题，尤其在高并发场景下，会导致节点性能瓶颈或故障。通过缓存层面的优化、流量分散策略、数据分布优化以及数据库层面的改进，可以有效解决或缓解热 Key 问题。同时，实时监控和预警机制也是预防热 Key 问题的重要手段。在设计高并发系统时，提前考虑和应对热 Key 问题，可以确保系统的稳定性和可扩展性。





# 如何汇总多个线程执行结果

1.countdownLatch

2.cylicbarrier

3.aotoInteger

4.future

5.completefuture





在讨论 JVM 本地缓存和 Redis 的 QPS（每秒查询次数）以及能承载的压力时，需要考虑两者的设计和使用场景。

### 1. **JVM 本地缓存**

- **QPS 和延迟**：
  - JVM 本地缓存（如 `ConcurrentHashMap`、Caffeine 等）因为数据直接存储在应用进程的内存中，查询时无需进行网络通信，所以在绝大多数情况下，本地缓存的 QPS 会比 Redis 高得多，通常可以达到数百万 QPS。
  - 由于数据存储在本地内存中，读取操作的延迟非常低，通常在纳秒级到微秒级别。

- **压力承载能力**：
  - 本地缓存的压力主要来自于 JVM 内存的大小和垃圾回收（GC）性能。大量数据存储在本地缓存中可能导致内存占用增加，从而引发频繁的 GC，进而影响应用的性能和稳定性。
  - 本地缓存通常适用于缓存热数据和小规模数据，因为它可以提供极高的访问速度，但受限于单机的内存容量。

### 2. **Redis**

- **QPS 和延迟**：
  - Redis 是一个基于内存的分布式缓存系统，通常能够达到几十万到上百万的 QPS（具体数值依赖于硬件配置、Redis 配置以及数据模型）。
  - 由于 Redis 运行在单独的服务器上，所有的查询都需要通过网络通信完成，因此相对于本地缓存，Redis 的读取延迟更高，通常在微秒级到毫秒级别。

- **压力承载能力**：
  - Redis 可以扩展为集群，横向扩展能够支持更大的数据规模和更高的并发量。Redis 的设计允许它在大规模、高并发的分布式环境中高效运行。
  - Redis 的内存管理和数据持久化功能使得它适用于需要跨多个服务器共享缓存数据的场景。

### **对比总结**

- **QPS**：JVM 本地缓存通常拥有更高的 QPS，因为它避免了网络开销，而 Redis 由于涉及网络通信，QPS 相对稍低，但在合理配置的情况下，Redis 的 QPS 仍然非常高。

- **压力承载**：Redis 的压力承载能力更强，尤其是在需要缓存大量数据、跨多个服务器共享数据的场景中。JVM 本地缓存虽然速度更快，但受限于单机内存，不适合缓存大规模数据或处理大范围的分布式请求。

### 选择的关键因素

- **应用场景**：如果你的应用是单机环境或单个服务的场景，且数据量不大，使用 JVM 本地缓存是合适的选择，能提供极高的性能。如果你需要在分布式环境中共享缓存，并且需要更好的扩展性和持久化支持，那么 Redis 是更好的选择。

- **数据量和扩展性**：当数据量较大且需要跨多个节点共享时，Redis 的扩展性和内存管理更具优势。

总之，JVM 本地缓存和 Redis 各有优劣，选择使用哪种缓存方案应根据应用的具体需求、架构、数据规模和对性能的要求来决定。



**服务降级和熔断都是为了提高系统的稳定性和可靠性，但它们在概念和实现上有一些区别。**

### 服务降级

**服务降级（Fallback）** 是一种应对服务故障或高负载的措施。当某个服务发生故障或无法正常工作时，通过预先定义的降级策略，提供一个简化的或备用的响应。服务降级通常是通过提供一个备用实现来替代原有服务的逻辑，以确保用户能够得到响应，尽管这个响应可能是简化的或者是不完全的。

#### 主要特点

1. **目标**：保证系统的可用性和用户体验，提供降级处理逻辑。
2. **实现**：通过Fallback机制，预定义备用实现，当主服务不可用时调用备用实现。
3. **触发条件**：主服务不可用、主服务性能下降、请求超时等。
4. **例子**：使用Feign和Hystrix的Fallback机制，在服务调用失败时返回预定义的响应。

#### 示例

```
java
复制代码
@FeignClient(name = "authconfigtest", fallback = AuthClientFallBack.class)
public interface AuthClient {
    @RequestMapping(value = "/auth/auth.do", method = RequestMethod.POST)
    String feignAuth();
}

@Component
public class AuthClientFallBack implements AuthClient {
    @Override
    public String feignAuth() {
        return "当前接口出现问题，请稍后访问";
    }
}
```

### 服务熔断

**服务熔断（Circuit Breaker）** 是一种应对雪崩效应的保护机制。它通过监控服务调用的成功和失败次数，当失败次数超过预设的阈值时，熔断器将断开后续的调用请求，直接返回错误响应，避免对已经过载或故障的服务进行更多的调用。熔断机制旨在防止服务的进一步恶化和系统的整体崩溃。

#### 主要特点

1. **目标**：防止系统过载或雪崩效应，保护服务的可用性。
2. **实现**：通过熔断器监控服务调用状态，当检测到大量失败时，短路后续调用，直接返回错误。
3. **触发条件**：一定时间窗口内的失败率超过设定阈值，响应时间过长等。
4. **例子**：使用Hystrix的熔断机制，配置熔断器参数，当满足熔断条件时，断开服务调用。

#### 示例

```
java
复制代码
@HystrixCommand(fallbackMethod = "fallbackMethod", commandProperties = {
    @HystrixProperty(name = "circuitBreaker.requestVolumeThreshold", value = "10"),
    @HystrixProperty(name = "circuitBreaker.sleepWindowInMilliseconds", value = "10000"),
    @HystrixProperty(name = "circuitBreaker.errorThresholdPercentage", value = "50")
})
public String someServiceMethod() {
    // 服务逻辑
}

public String fallbackMethod() {
    return "服务暂不可用，请稍后再试";
}
```

### 区别

#### 触发原因

- **服务降级**：主动触发，通常由于业务需要、服务压力过大、功能故障等原因手动或自动触发。
- **服务熔断**：被动触发，当服务调用失败率或响应时间超过设定阈值时，自动触发熔断。

#### 处理方式

- **服务降级**：提供简化的、备用的服务响应，以保证用户能够得到响应，通常是业务逻辑上的处理。
- **服务熔断**：直接断开调用请求，返回错误响应，避免对已过载的服务进行更多调用，通常是系统层面的保护措施。

#### 实现层次

- **服务降级**：通常在业务逻辑层面实现，通过备用逻辑或降级逻辑来处理。
- **服务熔断**：通常在框架或系统层面实现，通过监控和策略来保护系统。

### 共同点

- **目标**：都是为了提高系统的稳定性和可靠性，防止单点故障扩散。
- **使用场景**：都可以用于处理服务不可用、服务性能下降等问题。

### 总结

服务降级和熔断虽然在实现和触发条件上有所不同，但它们的目标都是为了提高系统的稳定性，防止系统因为部分服务的问题而导致整体崩溃。理解它们的区别和联系，可以更好地设计和实现高可用的分布式系统。



### 降级

当服务器压力不大的时候，我们一般调用HelloServiceImpl的方法，当服务器的压力大的时候，或者服务调用的成功率低于某个值的时候，就开始切换服务的提供对象，由HelloServiceImpl切换到HelloServiceMock上来，关于切换，简单的又分两种，手动和自动：

1）手动切换也很好理解，比如明天双11，或者618,818之类的促销节到来的时候，提前人工把某些非核心的功能，切换到Mock方法上来

2）自动切换也很好理解，比如某些服务相对比较重要，如果直接手动降级也是比较可惜，我们可以设定假如某个服务器的服务调用成功率低于某个值的时候，就开始自动降级



开发高并发系统时有三把利器用来保护系统：**缓存、降级**和**限流**。本文将详细聊聊降级。

**为什么需要降级：**当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。

**降级的最终目：**保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）

#### 降级预案

在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：

**一般：**比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；

**警告：**有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；

**错误：**比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；

**严重错误：**比如因为特殊原因数据错误了，此时需要紧急人工降级。

降级按照是否自动化可分为：**自动开关降级**和**人工开关降级**，按照功能可分为：**读服务降级、写服务降级**，按照处于的系统层次可分为：**多级降级**。

降级的功能点主要从服务端链路考虑，即根据用户访问的服务调用链路来梳理哪里需要降级：

**页面降级：**在大促或者某些特殊情况下，某些页面占用了一些稀缺服务资源，在紧急情况下可以对其整个降级，以达到丢卒保帅；

**页面片段降级：**比如商品详情页中的商家部分因为数据错误了，此时需要对其进行降级；

**页面异步请求降级：**比如商品详情页上有推荐信息/配送至等异步加载的请求，如果这些信息响应慢或者后端服务有问题，可以进行降级；

**服务功能降级：**比如渲染商品详情页时需要调用一些不太重要的服务：相 关分类、热销榜等，而这些服务在异常情况下直接不获取，即降级即可；

**读降级：**比如多级缓存模式，如果后端服务有问题，可以降级为只读缓存，这种方式适用于对读一致性要求不高的场景；

**写降级：**比如秒杀抢购，我们可以只进行Cache的更新，然后异步同步扣减库存到DB，保证最终一致性即可，此时可以将DB降级为Cache。

**爬虫降级：**在大促活动时，可以将爬虫流量导向静态页或者返回空数据从而降级保护后端稀缺资源。

**自动开关降级：**自动降级是根据系统负载、资源使用情况、SLA等指标进行降级。

**超时降级：**当访问的数据库/http服务/远程调用响应慢或者长时间响应慢，且该服务不是核心服务的话可以在超时后自动降级；比如商品详情页上有推荐内容/评价，但是推荐内容/评价暂时不展示对用户购物流 程不会产生很大的影响； 对于这种服务是可以超时降级的。如果是调用别人的远程服务，和对方定义一个服务响应最大时间，如果超时了则自动降级。

**统计失败次数降级：**有时候依赖一些不稳定的API，比如调用外部机票服务，当失败调用次数达到一定阀值自动降级；然后通过异步线程去探测服务是否恢复了，则取消降级。

**故障降级：**比如要调用的远程服务挂掉了（网络故障、DNS故障、http服务返回错误的状态码、rpc服务抛出异常），则可以直接降级。降级后的处理方案有：默认值（比如库存服务挂了，返回默认现货）、兜底数据（比如广告挂了，返回提前准备好的一些静态页面）、缓存（之前暂存的一些缓存数据）。

**限流降级：**当我们去秒杀或者抢购一些限购商品时，此时可能会因为访问量太大而导致系统崩溃，此时开发者会使用限流来进行限制访问量，当达到限流阀值，后续请求会被降级；降级后的处理方案可以是：排队页面（将用户导流到排队页面等一会重试）、无货（直接告知用户没货了）、错误页（如活动太火爆了，稍后重试）。

**人工开关降级：**在大促期间通过监控发现线上的一些服务存在问题，这个时候需要暂时将这些服务摘掉；还有有时候通过任务系统调用一些服务，但是服务依赖的数据库可能存在：网卡被打满了、挂掉了或者很多慢查询，此时需要暂停下任务系统让服务方进行处理；还有发现突然调用量太大，可能需要改变处理方式（比如同步转换为异步）；此时就可以使用开关来完成降级。

开关可以存放到配置文件、存放到数据库、存放到Redis/ZooKeeper；如果不是存放在本地，可以定期同步开关数据（比如1秒同步一次）。然后通过判断某个KEY的值来决定是否降级。

另外对于新开发的服务想上线进行灰度测试；但是不太确定该服务的逻辑是否正确，此时就需要设置开关，当新服务有问题可以通过开关切换回老服务。还有多机房服务，如果某个机房挂掉了，

此时需要将一个机房的服务切到另一个机房，此时也可以通过开关完成切换。

还有一些是因为功能问题需要暂时屏蔽掉某些功能，比如商品规格参数数据有问题，数据问题不能用回滚解决，此时需要开关控制降级。

**读服务降级：**对于读服务降级一般采用的策略有：暂时切换读（降级到读缓存、降级到走静态化）、暂时屏蔽读（屏蔽读入口、屏蔽某个读服务）。在《应用多级缓存模式支撑海量读服务》中曾经介绍过读服务，
即接入层缓存–>应用层本地缓存–>分布式缓存–>RPC服务/DB，我们会在接入层、应用层设置开关，当分布式缓存、RPC服务/DB有问题自动降级为不调用。当然这种情况适用于对读一致性要求不高的场景。

页面降级、页面片段降级、页面异步请求降级都是读服务降级，目的是丢卒保帅（比如因为这些服务也要使用核心资源、或者占了带宽影响到核心服务）或者因数据问题暂时屏蔽。

还有一种是页面静态化场景：

**动态化降级为静态化：**比如平时网站可以走动态化渲染商品详情页，但是到了大促来临之际可以将其切换为静态化来减少对核心资源的占用，而且可以提升性能；其他还有如列表页、首页、频道页都可以这么玩；
可以通过一个程序定期的推送静态页到缓存或者生成到磁盘，出问题时直接切过去；

**静态化降级为动态化：**比如当使用静态化来实现商品详情页架构时，平时使用静态化来提供服务，但是因为特殊原因静态化页面有问题了，需要暂时切换回动态化来保证服务正确性。

以上都保证出问题了有预案，用户还是可以使用网站，不影响用户购物。

**写服务降级：**写服务在大多数场景下是不可降级的，不过可以通过一些迂回战术来解决问题。比如将同步操作转换为异步操作，或者限制写的量/比例。

比如扣减库存一般这样操作：

**方案1：**

1、扣减DB库存

2、扣减成功后更新Redis中的库存

**方案2：**

1、扣减Redis库存

2、同步扣减DB库存，如果扣减失败则回滚Redis库存；

前两种方案非常依赖DB，假设此时DB性能跟不上则扣减库存就会遇到问题；

**方案3：**

1、扣减Redis库存

2、正常同步扣减DB库存，性能扛不住时降级为发送一条扣减DB库存的消息，然后异步进行DB库存扣减实现最终一致即可；

这种方式发送扣减DB库存消息也可能成为瓶颈；这种情况我们可以考虑方案4

**方案4：**

1、扣减Redis库存

2、正常同步扣减DB库存，性能扛不住时降级为写扣减DB库存消息到本机，然后本机通过异步进行DB库存扣减来实现最终一致性。

也就是说正常情况可以同步扣减库存，在性能扛不住时降级为异步；另外如果是秒杀场景可以直接降级为异步，从而保护系统。还有如下单操作可以在大促时暂时降级将下单数据写入Redis，然后等峰值过去了再同步回DB，当然也有更好的解决方案，但是更复杂，不是本文的重点。

还有如用户评价，如果评价量太大，也可以把评价从同步写降级为异步写。当然也可以对评价按钮进行按比例开放（比如一些人的看不到评价操作按钮）。比如评价成功后会发一些奖励，在必要的时候降级同步到异步。

**多级降级：**缓存是离用户最近越高效；而降级是离用户越近越能对系统保护的好。因为业务的复杂性导致越到后端QPS/TPS越低。

**页面JS降级开关：**主要控制页面功能的降级，在页面中通过JS脚本部署功能降级开关，在适当时机开启/关闭开关；

**接入层降级开关：**主要控制请求入口的降级，请求进入后会首先进入接入层，在接入层可以配置功能降级开关，可以根据实际情况进行自动/人工降级；这个可以参考《京东商品详情页服务闭环实践》，尤其在后端应用服务出问题时，通过接入层降级从而给应用服务有足够的时间恢复服务；

**应用层降级开关：**主要控制业务的降级，在应用中配置相应的功能开关，根据实际业务情况进行自动/人工降级。

**某东《服务降级背后的技术架构设计》PPT内容**

牺牲部分用户体验

- 商详页不显示特色服务icon、促销信息等
- 结算页不显示自提/311/411预约日历
- 订单详情页不显示GIS订单轨迹、催单等
- 评价列表禁止10页之后的翻页
- 实时统计和报表禁用
- 强制必选查询条件中的路由或索引字段
- 领豆豆防刷降级为拼图验证
- H5变PC页面
- 使用通用内容代替个性化推荐内容

　　降低安全级别

- 发放京豆、提交订单、发表评论、登录不调用风控接口
- 结算页前端下单不启用验证码
- 集中式session不可用，cookie解密即可
- ip limit服务，注册、登录不限制次数
- 商品修改内容不做敏感词过滤

　　牺牲部分业务逻辑

- 拍卖出价时不校验京豆数量
- 发表评价，不再校验是否退货

　　延缓任务处理

- WMS任务处理引擎，暂停调拨、节能补贴等任务
- OFW优先处理高优先级、拆分逻辑较简单的订单

　　损失数据持久性

- 用户地址更新，写redis，不回写数据库
- 库存预占，写redis，异步回写数据库
- 用户新增普票，写redis，不持久
- 订单二次拆分任务机制，由JMQ降为redis队列

　　降低准确性/实时性

- 实时价格过期不回源
- 动态页变静态拖底页
- 用户昵称接口降级，显示用户pin
- 库存状态接口降级，显示有货
- 抽奖异常，所有用户均显示未中奖

　　降低性能

- 数据库代替缓存防重、查询
- 数据库任务队列轮询代替MQ
- CDN降为源站
- 本地缓存降为RPC

　　降低容灾能力

- 自动调度变为手工调度
- VIP降级为real ip



# 熔断

# 服务熔断处理

服务的熔断及降级是系统鲁棒性的关键之一，这与我们常听说的服务雪崩有着紧密的关系。

> 🔆 鲁棒性（Robustness）意为健壮、强壮，在计算中指的是系统的健壮性，用于表示容忍可能影响系统功能的扰动的能力。[详见此处](https://en.wikipedia.org/wiki/Robustness)

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20240715174902988.png" alt="image-20240715174902988" style="zoom:50%;" />

上图是服务雪崩的示例，假设我们的系统由5个服务组成，服务1调用服务2、3，服务3调用服务4、5，但由于某些原因（到达性能极限、未知bug、网络分区等）服务4访问很慢，这时如果没有服务的熔断与降级那么调用者服务3会因为服务4异常而积累过多请求导致产生大量等待的线程（BIO模型），进而服务3也会引发访问慢或中止服务的问题，对其调用服务1也会重复服务3的问题，如此一来，由服务4本身的问题而引发依赖服务的整个链路都出问题，这就是典型的服务雪崩效应。

雪崩效应之所以这么被重视是因为它极容易在被人们忽视的情况下发生，对微服务而言，服务实例成百上千，我们很难一个个服务地检查以保证每个服务的质量，并且很多情况下只有在达到一定压力问题才会暴露，常规的代码Reivew或是针对单个服务的压力测试未必可以发现问题，再则这些依赖服务未必都是我们自己的服务，如果说我们自己服务尚有一定的排查优化方法的话那么对三方服务依赖而言那几乎只能是凭经验了，只要我的依赖服务中存在一处不起眼的bug，或是过少的连接池配置，抑或是网络波动都有可能引发雪崩。

怎么有效地避免呢？我们可以尝试这样解决：

设置每个请求的超时时间以避免服务请求一直等待下去。还是以上图为例，如果服务4僵死了，那么发往服务4的请求必定会超时，假定超时时间1s，那么原本可能在100ms内完成的请求都要等到1s，在请求量大的情况下，服务3还是会出现请求堆积，逐级向上进而导致雪崩。

所以我们还要做这样的约束：设置每个请求的超时时间，超时N次就不再请求。再套用上图的例子我们发现这的确起到立竿见影的效果，可有效地防止雪崩，这也是服务熔断的核心实现之一。

但这也存在一定的问题，在很多情况下服务异常是瞬时的，比如网络波动、中间件异常（多可自我恢复）、并发量陡增等，我们的策略是超时N次不再请求，这就导致了服务恢复后系统也会处于不用状态。

更好的做法是可以自动探测服务是否正常以进行熔断和恢复请求。比如我们的规则可以修改成：请求超时N次后在X时间不再请求实现熔断，X时间后恢复M%的请求，如果M%的请求都成功（未超时）则恢复正常关闭熔断，否则再熔断Y时间，依此循环。

这样看上去好多了，但这种粗放式的熔断服务会导致这些请求返回错误，更优雅的方式是实现服务降级，即在依赖服务异常后可以有替代的逻辑以提供备用，比如某一流程很重要，在依赖的服务异常后我们尝试让调用方临时查询依赖服务的缓存数据，缓存数据的实时性可能比较低，但不失为一种临时的解决方案。

当然我们还可以做得更多，服务异常很多情况下是并发量陡增导致，那么我们可以引入限流或排队的方案（见后续章节），使用限流直接挡掉一批请求，或是使用排队机制让多过的请求等待。

当我们把这样因素及实现都思考之后就形成了一个比较理想的熔断处理方案了，其核心是通过熔断实现服务保护，它体现在对请求做限流或排队以防止并发超过服务预设指标，通过熔断及智能的恢复防止系统雪崩，引入可选的降级流程提升服务的可用性。

下面举个例子进一步说明熔断与降级，对于数据查询场景我们可以会引入ES以提升查询的性能降低数据库的压力，这是正常的逻辑，但如果ES查询异常时要确保服务可用就需要走降级灾备方案，灾备方案1是使用资源有限定的只读账号查询生产数据库，如果达到限定值为不影响其它业务只能进入灾备方案2，灾备方案2查询的是备份库，备份库在性能、数据实时性上都有损失，但起码可以让服务尽可能地可用（当然要视具体场景而言）。

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20240715174921812.png" alt="image-20240715174921812" style="zoom:50%;" />

熔断器要求可以根据服务的情况自动升降级。一个标准的熔断器一般都包含三个状态：

- 

  Closed：熔断器关闭状态，即服务正常

- 

  Open：熔断器打开状态，直接返回错误，不再发起请求（没有网络开销）

- 

  Half-Open：半熔断状态，介于关闭和打开之间，此状态下会发送少量请求给对应的服务，如果调用成功且达到一定比例则恢复服务关闭熔断器，反之回到熔断器打开状态

Hystrix是目前最成熟的熔断器之一，它是一个类库，方便与我们的系统继承，实现了上述功能外还提供监控、报警及可视化操作能力，目前成熟的JVM微服务框架都有其集成方案。





