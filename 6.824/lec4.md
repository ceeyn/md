

1。论文

master 写操作时，先发日志，各个follower将日志写入本地后返回ack，然后master 发写的控制信号，master和follower一起写入

共享磁盘：master 和 follower 之间同时访问一个磁盘，为了防止脑裂，会在以为master死掉后，同时和分区的master进行原子操作更新共享磁盘，只有一个可以成功，相当于成为leader

主备切换是通过 garp，将原来master的 ip mac 地址到 端口的映射换成将备份的 端口的映射，主要指交换机【mac——》端口】





非确定性事件可以分成几类。

- 客户端输入。假设有一个来自于客户端的输入，这个输入随时可能会送达，所以它是不可预期的。客户端请求何时送达，会有什么样的内容，并不取决于服务当前的状态。我们讨论的系统专注于通过网络来进行交互，所以这里的系统输入的唯一格式就是网络数据包。所以当我们说输入的时候，我们实际上是指接收到了一个网络数据包。而一个网络数据包对于我们来说有两部分，一个是数据包中的数据，另一个是提示数据包送达了的中断。当网络数据包送达时，通常网卡的DMA（Direct Memory Access）会将网络数据包的内容拷贝到内存，之后触发一个中断。操作系统会在处理指令的过程中消费这个中断。对于Primary和Backup来说，这里的步骤必须看起来是一样的，否则它们在执行指令的时候就会出现不一致。所以，这里的问题是，中断在什么时候，具体在指令流中的哪个位置触发？对于Primary和Backup，最好要在相同的时间，相同的位置触发，否则执行过程就是不一样的，进而会导致它们的状态产生偏差。所以，我们不仅关心网络数据包的内容，还关心中断的时间。



网络数据包送达时，有一个细节会比较复杂。当网络数据包到达网卡时，如果我们没有运行虚拟机，网卡会将网络数据包通过DMA的方式送到计算机的关联内存中。现在我们有了虚拟机，并且这个网络数据包是发送给虚拟机的，在虚拟机内的操作系统可能会监听DMA并将数据拷贝到虚拟机的内存中。因为VMware的虚拟机设计成可以支持任何操作系统，我们并不知道网络数据包到达时操作系统会执行什么样的操作，有的操作系统或许会真的监听网络数据包拷贝到内存的操作。

我们不能允许这种情况发生。如果我们允许网卡直接将网络数据包DMA到Primary虚机中，我们就失去了对于Primary虚机的时序控制，因为我们也不知道什么时候Primary会收到网络数据包。所以，实际中，物理服务器的网卡会将网络数据包拷贝给VMM的内存，之后，网卡中断会送给VMM，并说，一个网络数据包送达了。这时，VMM会暂停Primary虚机，记住当前的指令序号，将整个网络数据包拷贝给Primary虚机的内存，之后模拟一个网卡中断发送给Primary虚机。同时，将网络数据包和指令序号发送给Backup。Backup虚机的VMM也会在对应的指令序号暂停Backup虚机，将网络数据包拷贝给Backup虚机，之后在相同的指令序号位置模拟一个网卡中断发送给Backup虚机。这就是论文中介绍的Bounce Buffer机制。



所以，几乎每一个复制系统都有这个问题，在某个时间点，Primary必须要停下来等待Backup，这对于性能是实打实的限制。即使副本机器在相邻的机架上，Primary节点发送消息并收到回复仍然需要0.5毫秒的延时。如果我们想要能承受类似于地震或者城市范围内的断电等问题，Primary和Backup需要在不同的城市，之间可能有5毫秒的差距。如果我们将两个副本放置在不同的城市，每次生成一个输出时，都需要至少等待5毫秒，等Backup确认收到了前一个Log条目，然后VMM才能将输出发送到网络。对于一些低请求量的服务，这不是问题。但是如果我们的服务要能够每秒处理数百万个请求，那就会对我们的性能产生巨大的伤害。

所以如果条件允许，人们会更喜欢使用在更高层级做复制的系统（详见4.2 最后两段）。这样的复制系统可以理解操作的含义，这样的话Primary虚机就不必在每个网络数据包暂停同步一下，而是可以在一个更高层级的操作层面暂停来做同步，甚至可以对一些只读操作不做暂停。但是这就需要一些特殊的应用程序层面的复制机制。



当Backup接管服务时，因为它的状态与Primary相同，所以它知道TCP连接的状态和TCP传输的序列号。当Backup生成回复报文时，这个报文的TCP序列号与之前Primary生成报文的TCP序列号是一样的，这样客户端的TCP栈会发现这是一个重复的报文，它会在TCP层面丢弃这个重复的报文，用户层的软件永远也看不到这里的重复。



这里可以认为是异常的场景，并且被意外的解决了。但是事实上，对于任何有主从切换的复制系统，基本上不可能将系统设计成不产生重复输出。为了避免重复输出，有一个选项是在两边都不生成输出，但这是一个非常糟糕的做法（因为对于客户端来说就是一次失败的请求）。当出现主从切换时，切换的两边都有可能生成重复的输出，这意味着，某种程度上来说，所有复制系统的客户端需要一种重复检测机制。这里我们使用的是TCP来完成重复检测，如果我们没有TCP，那就需要另一种其他机制，或许是应用程序级别的序列号。



### 🔄【3】为什么 primary 崩溃时可能无法确保“输出操作只发生一次”？

因为在执行输出（如发送 TCP 数据包）和 backup 确认之间，如果发生崩溃，系统无法判断：

- 是已经完成输出了？
- 还是刚准备输出？

#### 🎯 VMware 的补救机制：

- **依赖 TCP 重传机制**（处理丢包）
- **应用层幂等性**（重复处理不会出错）

这也是现代分布式系统的常规容错设计思路。





<img src="https://files.oaiusercontent.com/file-GQXeC7D2NzjFNyw4R819CT?se=2025-04-11T15%3A10%3A44Z&sp=r&sv=2024-08-04&sr=b&rscc=max-age%3D299%2C%20immutable%2C%20private&rscd=attachment%3B%20filename%3Dimage.png&sig=kzk4S5JIYMe7HKuCSTCay5pnYM0h8X85M%2BWDVvAnrxw%3D" alt="已上传的图片" style="zoom:25%;" />

| ✅ 优点                                        | ❌ 缺点                                                       |
| --------------------------------------------- | ------------------------------------------------------------ |
| 不依赖共享存储 适合跨数据中心部署（远程容灾） | 磁盘起始需要同步 故障切换后需要磁盘再次比对同步【让leader的数据传输给follower】 |
| 更强的物理隔离，提升可靠性                    | 无法利用存储原子操作，需要额外仲裁机制                       |
| 容灾可延伸至广域网                            | 实施复杂度更高                                               |





### 💡为什么使用 TCP 与幂等性？

由于无法判断主机是否在**发出最后一个输出后立刻宕机**，可能导致：

- 输出操作重复（Output 被发送两次）
- 输出操作丢失（Output 根本没发送）

这就要求：

- **TCP 的传输机制能自动应对丢包/重复包问题**
- **应用层必须具备幂等性设计**（重复执行不产生额外副作用）





> **保持主备 VM 同步时只需要少量的日志带宽（如 10~20 Mbit/s），远远低于传统热备方案（动辄上百兆甚至几 Gbit/s）。**

#### 💡 这意味着：

- 即使主备 VM 分布在 **地理上相隔较远的物理机房（比如 1~100 公里）**，只要网络带宽能稳定提供 100~1000 Mbit/s，就可以实现 FT。
- 不需要放在同一个机架、数据中心或者局域网。

#### 📦 实际好处：

- **高可用性（HA）提升**：即使一个城市或园区故障，另一个地方的备 VM 可立即接管。
- **容灾能力提升**：属于跨站点的灾备能力（geo-disaster recovery）。
- ✅ 对比传统共享存储/热迁移方案，这是一个巨大的进步。





#### 那么当 Primary VM 发生磁盘读取（disk read）时：

1. **Primary VM 执行磁盘读取操作**，从虚拟磁盘中取出数据（比如读出一个数据库页）；
2. 这块读取到的数据（比如512KB）会**作为“输入结果”被写入日志通道（logging channel）**；
3. 然后传输给 **Backup VM**；
4. Backup VM 在重放（replay）过程中不去读取磁盘，而是**直接从日志里获取 Primary 传来的数据**，这样就能保持执行一致。

------

### 🚨 但这样会有一个问题：

如果某个应用（比如 Oracle 或 MS SQL）**频繁执行磁盘读取**，那么这些磁盘读取的结果全都得通过日志传给 Backup VM。

- 📈 这就会导致 **日志带宽使用量非常高**（比如高达几十甚至上百 Mbit/s）；
- 对于 **跨地域部署（geo-distributed FT）**，日志带宽成为**性能瓶颈**。

------

### 💡 优化方式：Disk Read on Backup

这时，VMware 提出一种**优化方法** —— 允许 **Backup VM 自己执行磁盘读取操作**，流程变成这样：

1. Primary VM 执行磁盘读取操作；
2. VMware **不会把读取结果写入日志**；
3. Backup VM 在 replay 时，**自己从本地磁盘读取** 相同位置的数据；
4. 由于 Primary 和 Backup 虚拟磁盘**初始时是同步的**（initially synced disks），并且每次写入都严格按顺序同步，因此这块数据也应该一致。

------

### ✅ 优势：

- 🚀 **大幅降低日志带宽**；
  - Oracle Swingbench 测试中，带宽从 12 Mbps 降到 3 Mbps；
  - MS SQL DVD Store 带宽从 18 Mbps 降到 8 Mbps。
- ✅ 对于长距离（跨城/跨数据中心）FT 非常适合。



“Backup VM 自己从本地磁盘读取数据” 这句话的意思是：

✅ 在某些配置下（尤其是非共享磁盘或带宽受限的远程容灾部署），我们可以让 Backup VM **不再从日志通道获取磁盘读取结果**，而是**自己执行同样的磁盘读操作**，从而**减少日志带宽消耗**，提高系统的可扩展性和部署灵活性。





**GARP（Gratuitous ARP，全称：Gratuitous Address Resolution Protocol）广播**是一种特殊类型的 ARP 报文，用于网络中的主机主动广播自己的 IP-MAC 映射，无需请求另一台主机的地址。

------

## ✅ GARP 的本质：

> GARP 是主机**主动发送**的一种 ARP 报文，其作用并不是为了获取目标 MAC 地址，而是为了**通知网络上其他设备：这个 IP 地址现在由我拥有，它对应这个 MAC 地址。**

------

## 📦 报文内容：

GARP 报文的结构和普通 ARP 请求几乎一样，但具有以下特点：

- **源 IP = 目的 IP**（告诉别人“我是这个 IP 的主人”）
- **ARP 操作类型 = request**（虽然是广播，但并不是寻求应答）
- **目标 MAC 通常为 00:00:00:00:00:00**

这是一种 **“无害自问自答”** 的操作。

------

## 📌 GARP 的常见用途：

### 1️⃣ 网络故障切换（如 VMware FT）

当原有主机宕机、备用主机接管（Failover）时：

- 备用主机 **继承原主机的 IP**
- 使用 GARP 广播一个 ARP 报文：“这个 IP 的 MAC 是我！”
- 通知所有网络设备更新 ARP 缓存（比如路由器、交换机、客户端）

否则，网络设备可能还缓存着旧主机的 MAC，**请求仍然发往已失效主机**，造成短暂不可用。

------

### 2️⃣ 动态 IP 切换（如 DHCP 变化后）

当主机从 DHCP 得到一个新的 IP 后，会发出 GARP：

- 避免 IP 冲突（如果局域网中有其他设备使用了这个 IP，它会做出回应）
- 更新其他设备的 ARP 缓存

------

### 3️⃣ 负载均衡、漂移 IP 场景

如 LVS、Keepalived、VRRP 等高可用系统，当 VIP（虚拟 IP）漂移时：

- 新的主机绑定 VIP 后发送 GARP；
- 保证客户端请求能够立即发送到新的主机上。

------

## 💡 举个例子：

假设有三台设备：

- 原主机 A：IP = 192.168.1.10，MAC = aa:aa:aa:aa:aa:aa
- 客户端 C：缓存了 192.168.1.10 → aa:aa:aa:aa:aa:aa
- 备用主机 B：接管 IP = 192.168.1.10，MAC = bb:bb:bb:bb:bb:bb

**如果 B 不发送 GARP：**

- C 仍然将包发给 aa:aa:aa:aa:aa:aa，流量丢失。

**如果 B 发送 GARP：**

- C 更新缓存：192.168.1.10 → bb:bb:bb:bb:bb:bb
- 之后发给 B，接管成功！

------

## 🧠 总结一句话：

> **GARP 是主机在不请求任何信息的情况下，主动广播“这个 IP 是我的”信息，从而通知网络更新 ARP 缓存，保证流量正确路由。**

------

如果你正在实现高可用切换、FT 或 VRRP 等场景，这一步通常是必须的。如果你想要我给你展示一个 GARP 报文的真实例子（tcpdump/wireshark 抓包）、伪代码或使用 Linux 命令手动触发 GARP，也可以继续问我！



你提到的这段机制，是 VMware vSphere Fault Tolerance（FT）论文中的**关键创新点之一** —— **Bounce Buffer 机制**，它用于确保 **主备虚拟机（Primary & Backup）之间的精确同步性，特别是在处理异步网络数据时**，下面我详细为你解释这个机制的动机、原理和作用。

------

## ✅ 背景：为什么网络接收需要“精确同步”？

在 FT 架构中：

- **Primary 虚机在前面执行**，Backup 虚机**在后面同步重放** Primary 的执行结果（包括外部输入）。
- 为了保障一致性，**Backup 必须精确执行与 Primary 完全一样的指令序列**，包括处理输入、I/O 等。

------

## 🚫 问题：如果直接让网络包进 Primary，会怎样？

> 如果网卡直接通过 DMA 把网络数据包送进 Primary 虚机的内存中，Primary 会**“不受控制”地**接收到数据。

- 比如：Primary 虚机可能正在运行用户程序，它在第 1,000 条指令后接收了网络包；
- Backup 虚机可能才刚执行到第 900 条指令，**网络包一旦送入 Primary，Backup 来不及“同步感知”，就会出现分歧**。
- 最终：**主备状态不一致，FT 失败！**

------

## 💡 解决方案：**Bounce Buffer机制**

为了控制 Primary 接收网络数据的时序，**不能让网卡直接 DMA 给 Primary 虚机**，而是：

### 🔁 多了一层中转 —— Bounce Buffer：

1. **网络数据包**先写入 **VMM（Hypervisor）的 Bounce Buffer 内存区域**，而**不是直接送进 Primary 虚机**；

2. VMM 收到网卡中断，表示有网络包到了：

   ✅ 它就 **“暂停” Primary 虚机的运行**，记录它的 **当前指令序号（比如 PC 值）**；

   ✅ 然后将网络数据包从 Bounce Buffer **拷贝到 Primary 的虚拟内存**；

   ✅ 并在该指令处**模拟一个网卡中断**（即主机程序感知“我收到网络包了”）；

3. 同时，VMM 会将：

   - 当前指令序号
   - 网络数据内容

   ➜  **发送给 Backup 虚机的 VMM**。

4. Backup 虚机的 VMM：

   ✅ 等 Backup 执行到相同的指令序号（PC）；

   ✅ 拷贝相同的网络数据包；

   ✅ 模拟相同的网卡中断。

------

## 📦 这个过程的要点总结如下：

| 步骤                       | 作用                                                         |
| -------------------------- | ------------------------------------------------------------ |
| 1. 拦截网卡数据            | 不让 Primary“提前”接收到网络包                               |
| 2. 记录当前指令序号        | 精确地控制何时插入网络中断                                   |
| 3. Primary/Backup 同步中断 | 主备接收到 **完全相同的数据**，在**相同时间点（指令序号）**进行处理 |

------

## 📐 为什么叫“Bounce Buffer”？

- “Bounce”意味着反弹/缓冲的意思；
- 它**并不直接进入目标（Primary 虚机）**，而是先暂存在 VMM 的缓存区中；
- 类似于数据先“撞”在中间的缓冲区，再由 VMM“手动”交给虚拟机。

------

## ✅ 总结一句话：

> **Bounce Buffer 是 VMware FT 中控制网络输入一致性的核心机制，它确保网络数据包在主备虚拟机中以“完全相同的方式、在完全相同的时刻”被处理，从而避免状态发散。**

------

如果你希望我配图解释、结合代码、或者和其他输入设备（比如磁盘、定时器中断）做对比，也可以继续问我。这个机制非常适合深入学习分布式系统与虚拟化之间的边界同步。





这个问题非常关键，涉及到虚拟化环境中的网络配置和高可用策略。我们来一步步解释：

------

### 🧠 你问的是两件事：

1. **VM 的所有备份是不是在同一台物理机器上？**
2. **为什么 Primary 和 Backup 虚拟机可以拥有相同的 MAC 和 IP？**

我会分别解释：

------

## ✅ 1. 备份 VM（Backup VM）是否在同一台物理服务器上？

**不是！**

> **VMware FT（Fault Tolerance）架构要求：Primary 和 Backup 虚拟机必须运行在不同的物理主机上。**

📌 原因：

- **容灾**：如果 Primary 所在的物理服务器故障（宕机、电源断、主板坏等），Backup 仍然健在；
- **可靠性要求**：要实现真正的高可用，不能把所有鸡蛋放在一个篮子里。

------

## ✅ 2. 为什么 Primary 和 Backup 可以有**相同的 MAC 和 IP**？

这是 VMware FT 的核心设计点之一，正是为了实现**无缝主备切换（zero downtime failover）**。

------

### 🔍 网络视角：Primary 和 Backup 虚机的网络配置

| 配置项   | Primary VM        | Backup VM         |
| -------- | ----------------- | ----------------- |
| IP 地址  | 10.0.0.1          | 10.0.0.1          |
| MAC 地址 | AA:BB:CC:DD:EE:FF | AA:BB:CC:DD:EE:FF |

**完全一样！**

📌 原因是：从客户端的视角，始终是**一台虚拟机在工作**，哪怕主备发生了切换。

------

### 🧩 技术细节：为什么不会冲突？

1. **平时只有 Primary VM 在发出网络数据包**
   - Backup VM 是“冷备”状态，虽然同步执行，但不会向外发包。
   - 所以即使 MAC / IP 一样，外部网络中只“看见”一个地址。
2. **切换时通过 GARP 更新网络路径**
   - Backup 接管后主动广播 GARP，更新交换机 MAC 表。
   - 之后客户端流量就自动流向 Backup。
3. **网络卡虚拟化技术支持此方案**
   - VMware 的 vNIC（虚拟网卡）和 vSwitch（虚拟交换机）层提供支持。
   - 在切换后，虚拟交换机会允许 MAC/IP 重复，并正确转发。

------

## 🎯 结论

| 问题                                      | 回答                                                         |
| ----------------------------------------- | ------------------------------------------------------------ |
| Primary 和 Backup VM 在同一台物理机上吗？ | ❌ 否，它们部署在不同物理服务器上，防止单点故障               |
| 它们为什么可以使用相同的 IP 和 MAC 地址？ | ✅ 为了实现快速切换，VMware 使用相同的网络配置，并依靠虚拟交换机和 GARP 保证网络无缝接力 |
| 会有冲突吗？                              | 不会。因为 Backup 平时不发包，不会对网络造成困扰。只有当 Primary 挂了，Backup 才开始“说话” |

------

如果你想，我可以给你画一张图表示 Primary 和 Backup 网络切换的过程，要不要？