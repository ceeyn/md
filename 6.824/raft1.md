

以下是 Raft 论文第 5 节（包含 5.1 和 5.2）以及后续部分中有关安全性和设计理念的完整中文翻译：

------

## 第 5 节 Raft 共识算法

Raft 是一个用于管理复制日志的算法，其结构如第 2 节所述。图 2 总结了该算法的简要形式以便参考，图 3 列出了该算法的关键属性；这些图中的元素将在本节其余部分中逐一进行讨论。

Raft 通过首先选举出一个唯一的 Leader，然后赋予该 Leader 完全的责任来管理复制日志，从而实现共识。Leader 接收客户端的日志条目请求，负责将它们复制到其他服务器，并在日志条目可以安全应用于状态机时通知各服务器。引入 Leader 简化了复制日志的管理。例如，Leader 可以决定新日志条目的放置位置而无需征询其他服务器的意见，数据也能以简单的方式从 Leader 流向其他服务器。如果 Leader 失败或与其他服务器失联，将会选举出一个新的 Leader。

基于 Leader 的方式，Raft 将共识问题分解为三个相对独立的子问题，这些子问题在接下来的子节中讨论：

- **Leader 选举**：当现有 Leader 失败时，必须选出一个新的 Leader（详见第 5.2 节）；
- **日志复制**：Leader 必须接收客户端的日志条目请求并在集群中复制它们，强制所有日志与其保持一致（详见第 5.3 节）；
- **安全性保证**：Raft 最关键的安全属性是图 3 中的“状态机安全性属性”：**如果某个服务器已经将一个特定的日志条目应用到了其状态机，那么没有其他服务器可以为相同的日志索引应用不同的命令。** 第 5.4 节描述了 Raft 如何保证这一属性，其解决方法涉及对选举机制的额外限制（扩展自第 5.2 节的规则）。

在介绍完整的共识算法之后，本节还会讨论系统的可用性问题，以及时间机制在系统中的作用。

------

## 5.1 Raft 基本原理

一个 Raft 集群由多个服务器组成；典型的配置是 5 台服务器，这样系统可以容忍最多两个节点失效。

在任意时刻，每个服务器都处于以下三种状态之一：

- **Leader（领导者）**
- **Follower（跟随者）**
- **Candidate（候选人）**

在正常运行中，集群中正好只有一个 Leader，其余的服务器都是 Follower。

- Follower 是被动的：它们不会主动发出任何请求，只响应来自 Leader 或 Candidate 的 RPC 请求；
- 所有客户端请求都交由 Leader 处理（如果客户端联系了 Follower，Follower 会将请求重定向至 Leader）；
- 第三种状态 Candidate 是在选举新 Leader 时使用的状态，如第 5.2 节所述；
- 图 4 展示了这三种状态及其转换关系，具体转换将在下文中讲述。

Raft 将时间划分为若干个任意长度的 **任期（term）**，如图 5 所示。任期按连续整数编号。每个任期开始时会进行一次选举，由一个或多个 Candidate 试图成为 Leader。如果某个 Candidate 赢得选举，它就会在接下来的任期内担任 Leader。某些情况下，选举会出现 **分票（split vote）**，这会导致该任期无 Leader 产生；随后将开始一个新的任期，并进行新的选举。

任期在 Raft 中扮演逻辑时钟的角色，它允许服务器检测过期信息，例如已过时的 Leader。每台服务器都保存一个当前任期号，该数值随着时间单调递增。在服务器通信时会交换当前任期信息；如果发现自己的任期落后，就立即更新为更大的那个。如果 Candidate 或 Leader 发现自己的任期过期，将立即转变为 Follower。如果一个服务器接收到包含旧任期号的请求，则拒绝该请求。

Raft 服务器通过远程过程调用（RPC）进行通信，基本共识算法只需要两种类型的 RPC：

- `RequestVote`：选举过程中由 Candidate 发起（详见第 5.2 节）；
- `AppendEntries`：由 Leader 发起，用于日志复制和心跳维持（详见第 5.3 节）；

第 7 节将引入第三种 RPC，用于在服务器之间传输快照（Snapshot）。如果 RPC 在指定时间内未收到响应，服务器会进行重试，并行发起多个 RPC 以实现最佳性能。

------

## 5.2 Leader 选举

Raft 使用心跳机制来维持 Leader 的权威并触发 Leader 选举。

服务器启动时以 Follower 状态运行。如果 Follower 能够持续接收到来自 Leader 或 Candidate 的有效 RPC，它就会一直保持 Follower 状态。Leader 会周期性地向所有 Follower 发送心跳（即不包含日志条目的 AppendEntries RPC）以维持其领导权。如果 Follower 在一个称为 **选举超时（election timeout）** 的时间段内未收到任何通信，它就会认为当前没有可用的 Leader，并发起一次选举以选出新的 Leader。

启动选举时，Follower 会执行以下操作：

1. 增加当前任期；
2. 将自身状态转变为 Candidate；
3. 给自己投票；
4. 向所有其他服务器并行发送 RequestVote RPC。

Candidate 将保持该状态，直到发生以下三种情况之一：

- (a) 获胜并成为 Leader；
- (b) 收到另一个服务器的 AppendEntries RPC，表明已有合法 Leader；
- (c) 一段时间内无人胜出（可能出现分票）。

我们分别讨论这三种情况：

### (a) 胜选

Candidate 若在当前任期中获得了多数服务器的投票，则赢得选举成为 Leader。

- 每台服务器在一个任期内最多只会投票给一个 Candidate（按先到者优先原则）；
- 多数投票规则保证了一个任期中最多只能有一个 Candidate 赢得选举（图 3 中的选举安全性属性）；
- 一旦成为 Leader，它就会立即向其他所有服务器发送心跳以确立其权威，并阻止其他服务器发起新选举。

### (b) 接收到有效 Leader 的 AppendEntries

如果 Candidate 在等待投票过程中接收到一个来自其他服务器的 AppendEntries RPC（表示该服务器宣称是 Leader），则：

- 如果该 RPC 中的任期不小于当前 Candidate 的任期，则接受其为合法 Leader，并回到 Follower 状态；
- 否则，继续作为 Candidate，忽略该 RPC。

### (c) 无人胜出（分票）

如果多个 Follower 同时变为 Candidate，可能出现分票现象，导致没有 Candidate 获得多数投票。

这种情况下，所有 Candidate 都会超时并重新发起新一轮选举（递增任期号、再次发送 RequestVote）。

### 为减少分票：随机选举超时机制

Raft 使用随机选举超时机制来降低分票概率，并快速收敛：

- 每个服务器的选举超时在固定范围内随机选择（例如 150~300 毫秒）；
- 这样一来，大多数情况下只有一台服务器先超时并发起选举，它更容易胜出；
- 如果真的出现分票，每个 Candidate 会在新一轮选举中重新随机一个新的超时值，从而避免再次分票。

第 9.3 节表明这种机制能快速完成 Leader 的选举。

### 可理解性设计取舍

选举机制是 Raft 以“易于理解”为目标做出的设计取舍之一。

起初，我们尝试引入“排名（rank）”机制：每个 Candidate 被赋予一个唯一的排名值，用于在竞争中选出 Leader。如果某个 Candidate 发现其他 Candidate 排名更高，它会主动让出，以便更高排名者胜选。

但我们发现这个设计会在可用性上带来微妙问题：当高排名服务器失败时，低排名服务器必须重新变为 Candidate，但如果重试得太快，可能反而阻碍选举进展。我们尝试多次修正该算法，但总会出现新的极端情况。最终我们认为使用随机超时的重试机制更加直观和可理解。



### 5.3 日志复制

一旦一个 Leader 被选举出来，它就会开始处理客户端请求。每个客户端请求都包含一个将由状态机执行的命令。Leader 会将该命令追加为日志中的一个新条目，然后并行地向其他服务器发送 AppendEntries RPC，以复制该日志条目。一旦该条目被安全地复制（如下文所述），Leader 就会将该条目应用到它的状态机，并将执行结果返回给客户端。如果某些 Follower 崩溃或运行缓慢，或网络丢包，Leader 会无限期地重试发送 AppendEntries RPC（即使它已经向客户端返回了结果），直到所有 Follower 最终都存储了全部日志条目为止。

日志如图 6 所示进行组织。每个日志条目保存一个状态机命令和该条目被 Leader 接收到时的任期号。日志条目中的任期号用于检测日志间的不一致，并确保图 3 中列出的某些属性。每个日志条目还有一个整数索引来标识它在日志中的位置。

Leader 决定何时可以安全地将日志条目应用到状态机；此类条目被称为 **已提交（committed）**。Raft 保证：一旦一个日志条目被提交，它就是持久的，最终一定会被所有可用的状态机执行。一个日志条目被提交的条件是：生成该条目的 Leader 已经将它复制到大多数服务器上（例如图 6 中的 entry 7）。这也意味着 Leader 日志中所有在该条目之前的条目也一并被提交，包括由以前的 Leader 创建的条目。第 5.4 节将讨论在 Leader 更换后应用这条规则时的一些微妙问题，并证明这种提交定义是安全的。Leader 会记录它知道的最大已提交索引，并在后续 AppendEntries RPC（包括心跳）中包含这个索引，以便其他服务器最终得知。Follower 一旦得知某个日志条目已提交，就会按照日志顺序将该条目应用到其本地状态机中。

我们设计 Raft 的日志机制时，力求在各服务器日志之间保持高度一致性。这不仅简化了系统行为、提升了可预测性，也是确保系统安全性的重要因素。Raft 保持以下属性，这些属性共同构成图 3 中的 **日志匹配属性（Log Matching Property）**：

- 如果两个日志中的条目具有相同的索引和任期，那么它们存储的是相同的命令；
- 如果两个日志中的条目具有相同的索引和任期，那么在该条目之前的所有条目也完全一致。

第一个属性的理由是：一个 Leader 在一个给定的任期中、某个索引位置只会创建一个日志条目，且日志条目一旦创建其位置不会更改。第二个属性是通过 AppendEntries 实现的一个一致性检查来保证的。在发送 AppendEntries RPC 时，Leader 会包含其日志中新条目前一个条目的索引和任期。Follower 如果在本地日志中找不到与之相同索引和任期的条目，就会拒绝接受新条目。

这个一致性检查就像归纳的“归纳步”：日志的初始状态满足日志匹配属性，而一致性检查在日志扩展时持续维护这个属性。因此，只要 AppendEntries 返回成功，Leader 就可以确定 Follower 的日志从头到新的条目都与自己一致。

在正常运行中，Leader 和 Follower 的日志是一致的，所以 AppendEntries 的一致性检查通常不会失败。然而，Leader 崩溃可能导致日志不一致（前任 Leader 可能没来得及完全复制其日志条目）。这种不一致可能在一系列 Leader 和 Follower 崩溃中积累加剧。图 7 展示了新 Leader 上任时 Follower 的日志可能会出现的各种差异：Follower 可能缺失 Leader 的条目，或者包含 Leader 没有的额外未提交条目，甚至两者兼有。这些缺失和冗余的日志条目可能跨越多个任期。

在 Raft 中，Leader 通过强制 Follower 的日志与自己一致来处理不一致问题。也就是说，Follower 日志中与 Leader 冲突的条目将被 Leader 的日志所覆盖。第 5.4 节将进一步证明这种方式在一个额外限制下是安全的。

为使 Follower 的日志与 Leader 一致，Leader 必须找出两者最后一次一致的位置（即最后一个相同的索引和任期），然后删除 Follower 中该点之后的所有条目，并发送自己从该点之后的所有日志条目。这一切都通过 AppendEntries RPC 的一致性检查机制完成。

Leader 为每个 Follower 维护一个 `nextIndex`，表示下一个将发送给该 Follower 的日志条目索引。当一个 Leader 刚上任时，会将所有 `nextIndex` 初始化为其日志的最后一个索引加一（如图 7 中的索引 11）。如果 Follower 的日志与 Leader 不一致，则下一次 AppendEntries RPC 会触发一致性检查失败。之后，Leader 会将 `nextIndex` 减 1 并重试 AppendEntries RPC。最终，`nextIndex` 会减少到一个使得 Leader 和 Follower 日志一致的位置，AppendEntries 成功返回，此时会删除 Follower 冲突的日志条目并追加 Leader 的条目。一旦 AppendEntries 成功，Follower 的日志就与 Leader 一致，并在整个任期内保持一致。

如果愿意，该协议可以进行优化，以减少失败的 AppendEntries RPC 次数。例如，当 Follower 拒绝 AppendEntries 请求时，可以在响应中返回冲突条目的任期以及该任期的第一个索引。Leader 利用这些信息可以一次跳过该任期内的所有冲突条目，而不是一条条重试。尽管如此，我们认为该优化并非必要，因为实际中失败比较少见，而且不太可能存在大量不一致条目。

通过这个机制，当 Leader 上任后无需做特殊处理来恢复日志一致性。它只需执行常规的操作，日志的一致性会随着 AppendEntries 的失败和重试自动恢复。Leader 永远不会覆盖或删除自己日志中的条目（这是图 3 中的 “Leader 追加只写” 属性）。

这种日志复制机制展示了第 2 节中提出的分布式一致性特性：Raft 在大多数服务器正常运行的情况下可以接受、复制并应用新的日志条目；在正常情况下，一个新日志条目只需一次 RPC 轮次即可复制到多数派；而单个缓慢的 Follower 不会影响整体性能。



## 5.4 安全性（Safety）

前面几节介绍了 Raft 如何选举 Leader 并复制日志条目。但到目前为止，这些机制尚不足以确保每台状态机都以相同顺序执行相同的命令。例如，一个 Follower 在 Leader 提交多个日志条目期间可能处于离线状态，然后它被选为 Leader 并用新条目覆盖了这些旧条目，结果导致不同状态机执行了不同的命令序列。

本节将通过对被允许当选为 Leader 的服务器增加一个限制，来完善 Raft 算法。这个限制保证任何任期中的 Leader 都包含前一任期中已提交的所有条目（即图 3 所示的 Leader 完整性属性）。有了这个限制后，我们还将对日志条目的提交规则做出更精确的定义，并给出一个该属性的证明草图，说明它如何确保状态机的正确性。

------

### 5.4.1 选举限制

在任何基于 Leader 的共识算法中，Leader 最终都必须包含所有已提交的日志条目。一些共识算法（如 Viewstamped Replication）允许某个不包含所有已提交条目的服务器被选为 Leader，这类算法需要额外机制来发现缺失条目，并在选举期间或之后将其传输给新 Leader。这增加了系统的复杂性。

Raft 采用更简单的方法：保证所有新 Leader 在选举时就已包含所有已提交的条目，无需后续再传输。这也意味着日志条目的传递只会从 Leader 向 Follower 单向进行，Leader 永远不会覆盖自身已有的日志条目。

Raft 利用投票机制来实现这个限制：Candidate 必须联系多数派服务器才能赢得选举，而每个已提交条目必然存在于该多数派中的某个节点上。只要 Candidate 的日志“足够新”（下文定义），它就一定包含所有已提交条目。

这个“日志新旧”由 RequestVote RPC 实现：

- RPC 中携带 Candidate 的最后日志索引和任期；
- 如果投票者的日志比 Candidate 更新，则拒绝投票。

Raft 通过以下规则判断哪个日志更新：

- 如果最后条目的任期不同，则任期大的更新；
- 如果任期相同，则日志长度更长的更新。

------

### 5.4.2 提交来自前一任期的日志条目

如第 5.3 节所述，Leader 可以在将当前任期的某个条目复制到多数节点后将其提交。但 Leader 不能因此就认为来自前任期的条目也被提交。

图 8 展示了一个例子：一个旧任期的日志条目虽然复制到了多数节点，但仍可能被后来的 Leader 覆盖。

为避免此类问题，Raft **只允许提交当前任期的日志条目**。也就是说：

- 只有当前任期的条目才能通过“多数派复制”方式被提交；
- 一旦该条目被提交，根据日志匹配属性，之前所有日志条目也会间接被认为是已提交的；
- 虽然某些情况下 Leader 可以判断旧日志已提交（例如它存在于所有节点），但 Raft 为了简化实现，采用更保守的方式。

这种提交限制之所以必要，是因为 Raft 在复制旧任期日志条目时保留原任期号，而不是用当前任期号“重写”。这有利于条目的可追溯性，并减少重复复制；而其他算法往往需要对旧条目“编号重写”以便提交。

------

### 5.4.3 安全性证明

在完整的 Raft 算法基础上，我们可以正式证明 Leader 完整性属性成立（详见第 9.2 节安全性证明）。我们通过反证法说明：

假设某任期 T 的 LeaderT 提交了一个日志条目，但某个后续任期 U 的 LeaderU 却不包含这个条目。设 U 是最小的这种任期。

1. LeaderT 将条目复制到多数派；LeaderU 也从多数派中赢得选举。因此，存在一个“投票者”同时拥有该条目且给 LeaderU 投了票（图 9）。
2. 投票者在投票前必然已接收该条目（否则会拒绝 LeaderT 的 RPC 请求）。
3. 投票者在投票时仍保留该条目（因为 Leader 从不删除条目，Follower 只在发生冲突时删除）。
4. 投票者能投票给 LeaderU，说明 LeaderU 的日志至少与投票者一样新。
5. 若两者最后条目的任期相同，LeaderU 的日志至少与投票者一样长 ⇒ LeaderU 应该包含该条目 ⇒ 矛盾；
6. 若 LeaderU 的最后条目任期更大 ⇒ 说明更早的 Leader 已经包含该条目 ⇒ 根据日志匹配属性，LeaderU 也应包含该条目 ⇒ 再次矛盾；
7. 因此假设不成立，LeaderU 必然包含 LeaderT 所提交的条目；
8. 所有后续 Leader 也必然包含该条目；
9. 根据日志匹配属性，任何间接提交的旧条目也都将被保留。

由此，我们可以进一步证明 **状态机安全性**（图 3）：

> 如果某个服务器已经将某个日志条目应用到状态机，则不会有其他服务器将不同的条目应用到相同索引上。

原因在于：

- 日志被应用前必须已提交，且 Leader 的日志与该节点一致；
- 任意条目的第一次应用都由某任期 Leader 提交；
- 所有后续 Leader 必然包含该条目；
- 所有服务器最终都会以相同顺序应用相同条目。

------

## 5.5 Follower 和 Candidate 的崩溃处理

目前我们主要讨论了 Leader 崩溃的情况。相比之下，Follower 和 Candidate 的崩溃处理要简单得多，且方式一致。

- 若 Follower 或 Candidate 崩溃，后续对其的 RequestVote 或 AppendEntries RPC 会失败；
- Raft 通过无限重试机制处理这些失败；
- 若节点在响应 RPC 前崩溃，重启后会收到相同请求；
- 因为 Raft 的 RPC 是幂等的（如重复追加相同日志会被忽略），所以不会带来问题。

------

## 5.6 时间与可用性

Raft 要求其 **安全性不依赖于时间**：系统不应因某些事件发生得过快或过慢而产生错误。但可用性则不可避免地依赖时间。例如：

- 若消息传播时间长于服务器平均故障间隔，Candidate 可能在选举成功前就崩溃；
- 没有稳定的 Leader ⇒ 系统无法前进。

Raft 选举过程对时间最为敏感。选举能够正常进行的条件如下：

```
nginx


复制编辑
broadcastTime ≪ electionTimeout ≪ MTBF
```

其中：

- `broadcastTime`：并行发送 RPC 并接收响应的平均时间（典型值为 0.5ms ~ 20ms）；
- `electionTimeout`：选举超时时间（Raft 中通常设为 10ms ~ 500ms）；
- `MTBF`：服务器的平均故障间隔（通常为数月以上）。

该不等式保障：

- Leader 能可靠发送心跳，防止无谓选举；
- 分票概率小；
- 崩溃后能快速恢复系统可用性。





这是Raft论文中关于集群成员变更的一部分描述。下面是其中文翻译：

### 6. 集群成员变动

到目前为止，我们假定集群配置（参与共识算法的服务器集群）是固定的。在实际应用中，偶尔需要更改配置，比如替换出现故障的服务器或更改复制的程度。虽然可以通过离线整个集群、更新配置文件，然后重新启动集群的方式来完成这一操作，但在变更过程中集群将不可用。此外，如果有任何人工步骤，则可能会出现操作员错误。为了避免这些问题，我们决定自动化配置变更并将其纳入Raft共识算法中。

为了使配置变更机制安全，在变更过程中，不能在任何时刻允许为同一任期选举出两个领导。这很不幸，因为任何服务器直接从旧配置切换到新配置的方法都是不安全的。无法原子性地同时切换所有服务器，因此集群可能在过渡过程中裂变为两个独立的多数群（见图10）。

为了确保安全性，配置变更必须使用两阶段方法。可以有多种方式实现这两个阶段。例如，一些系统（例如[22]）使用第一阶段来禁用旧配置，以便它无法处理客户端请求；然后在第二阶段启用新配置。在Raft中，集群首先切换到一个过渡配置，我们称之为联合共识；一旦联合共识提交后，系统便会过渡到新配置。联合共识结合了旧配置和新配置：

- 日志条目会复制到两个配置中的所有服务器。联合共识允许单个服务器在不同时间之间进行配置过渡而不破坏安全。此外，联合共识允许集群在整个配置变更过程中继续服务客户端请求。

集群配置通过复制日志中的特殊条目进行存储和通信；图11展示了配置变更过程。当领导者收到将配置从旧（Cold）更改到新（Cnew）的请求时，它将在日志条目中存储联合共识配置（图中的Cold,new），并使用之前描述的机制复制该条目。一旦某个服务器将新的配置条目添加到其日志中，它便使用该配置来进行所有未来决定（服务器总是使用日志中的最新配置，无论该条目是否提交）。这意味着领导者将使用Cold,new的规则来决定何时提交Cold,new的日志条目。如果领导者崩溃，则可能根据Cold或Cold,new选择新的领导者，具体取决于获胜候选者是否已接收到Cold,new。无论如何，在此期间Cnew不能单方面做出决策。

一旦Cold,new提交后，Cold和Cnew都不能在没有其他配置批准的情况下做出决策，而领导者完整性属性确保只有具有Cold,new日志条目的服务器才能被选为领导者。此时，领导者可以安全地创建描述Cnew的日志条目并将其复制到集群中。再次提醒这个配置将在每个服务器首次见到时开始生效。当根据Cnew的规则提交新配置后，旧配置将无关紧要，未在新配置中的服务器可以关闭。正如图11所示，没有时间点能让Cold和Cnew同时单方面做出决策；这确保了安全性。

关于重新配置还有三个问题需要解决。第一个问题是新服务器可能最初不存储任何日志条目。如果它们以这种状态加入集团，可能需要相当长时间才能赶上，这段时间内可能无法提交新的日志条目。为了避免可用性缺口，Raft在配置变更之前引入额外的阶段，在该阶段中新服务器作为不具有投票权的成员加入集群（领导者将日志条目复制给它们，但它们不算多数群）。一旦新服务器赶上集群的其余部分，重新配置便可按照上述描述进行。

第二个问题是集群领导者可能不在新配置中。在这种情况下，一旦它提交了Cnew日志条目，它便下台（返回追随者状态）。这意味着在提交Cnew的期间，领导者正在管理一个不包括其自身的集群；它复制日志条目但不算多数群。领导者过渡发生在提交Cnew时，因为这是新配置可以独立操作的第一个时间点（可以总是选择一个来自Cnew的领导者）。在此之前，可能只有一个来自Cold的服务器可以被选为领导者。

第三个问题是被移除的服务器（不在Cnew中的）可能会扰乱集群。这些服务器不会收到心跳，因此它们会超时并发起新选举。然后，它们将发送新的任期号的RequestVote RPC，导致当前领导者返回至追随者状态。最终会选举出新的领导者，但被移除的服务器会再次超时，过程再次重复，导致可用性降低。

为了防止这个问题，服务器在它们认为当前存在领导者时忽略RequestVote RPC。具体来说，若一个服务器在听到当前领导者的最小选举超时内收到RequestVote RPC，它将不会更新其任期或授予投票。这不会影响正常选举，在正常选举中，每个服务器至少等到最小选举超时后才开始选举。然而，它有助于避免来自被移除服务器的干扰：如果一个领导者能够将心跳发送给其集群，它就不会被更大的任期号推翻。

```
"联合配置"（Joint Consensus）是Raft共识算法中处理集群重新配置的一个关键机制。在这个机制中，系统允许旧配置和新配置的服务器同时认可并运作，确保在过渡期间不会出现安全问题。

### 联合配置的内容
联合配置包含两个部分：
1. **旧配置（Cold）**：当前正在使用的服务器集合。
2. **新配置（Cnew）**：即将启用的新服务器集合。

联合配置的主要目的是在变更期间，确保这两个配置的服务器可以协同工作，避免出现决策上的不一致和多个领导者的问题。

### 举例解释
假设我们有一个初始集群配置（Cold）包括以下5台服务器：
- **Cold**: A, B, C, D, E

现在，我们计划将集群配置变更为包括一台新服务器（F）的6台服务器（Cnew）：
- **Cnew**: A, B, C, D, E, F

#### 联合配置示例
在进行配置变更的过程中，系统创建一个**联合配置**，它将同时包含旧配置和新配置的服务器。联合配置的内容可以写作：
- **联合配置**: (A, B, C, D, E), (A, B, C, D, E, F)

这里的联合配置意味着：
1. **第一部分**（A, B, C, D, E）是旧配置的服务器。
2. **第二部分**（A, B, C, D, E, F）是新配置的服务器。

### 具体步骤
1. **创建联合配置**：
   - 当集群领导者收到请求，准备进行配置变更时，它首先将这个联合配置（Cold,new）记录到日志中。

2. **记录联合配置**：
   - 所有服务器都会收到这个联合配置的日志条目。比如，记录为：
     - 日志条目示例：`{"type": "config_change", "content": {"old": ["A", "B", "C", "D", "E"], "new": ["A", "B", "C", "D", "E", "F"]}}`

3. **继续服务**：
   - 在这个联合配置状态下，A、B、C、D、E和F都可以继续联动和处理请求。例如，如果收到客户端的写请求，所有服务器都需要记录这个请求，并且基于这个联合配置来做决策。

4. **稳定过渡**：
   - 当所有服务器都确认并记录了这个联合配置后，领导者就可以继续提交新的配置（Cnew）为主配置。

5. **保持一致性**：
   - 在整个过程中，Cold和Cnew都不会单方面做决策，任何影响集群的选择（如选举领导者）都需要双方的同意。

### 最终作用
通过使用联合配置，Raft确保了在集群配置变更过程中依然保持一致性、可用性和安全性。成员间的相互承认和合作，有效避免了因成员变动而导致的不可预期行为或分裂。

希望这个详细的解释和示例可以帮助你理解“联合配置”的内容及其在Raft中的作用！如果还有任何问题，请随时问我。
```



### 日志压缩

Raft 的日志在正常操作中会不断增长，以包含更多客户端请求，但在实际系统中，日志不能无限制地增长。随着日志的增长，它占用的存储空间会增加，并且重放日志所需的时间也会更长。这最终会造成可用性问题，因此需要某种机制来丢弃在日志中积累的过期信息。

快照是压缩的最简单方法。**在快照中，整个当前系统状态被写入稳定存储的快照中，然后丢弃该点之前的整个日志**。Chubby 和 ZooKeeper 也使用快照，接下来的部分将描述 Raft 中的快照。增量压缩方法，如日志清理和日志结构合并树（LSM trees）也是可行的。这些方法一次处理数据的一部分，从而在时间上更均匀地分散压缩负载。它们首先选择一段积累了许多已删除和重写对象的数据区域，然后以更紧凑的方式重写该区域的活动对象并释放该区域。与快照相比，这需要显著额外的机制和复杂性，快照通过始终在整个数据集上操作来简化问题。虽然日志清理需要对 Raft 进行修改，但状态机可以使用与快照相同的接口来实现 LSM 树。

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250505141839654.png" alt="image-20250505141839654" style="zoom:25%;" />

图 12 展示了 Raft 中快照的基本思想。每个服务器独立进行快照，覆盖其日志中的已提交条目。**大部分工作是由状态机将其当前状态写入快照。**Raft 中的快照还包含少量元数据：最后包含的索引是快照替代的日志中最后一条条目的索引（状态机已经应用的最后一条条目），最后包含的任期是该条目的任期。**这些信息被保留以支持快照后的第一个日志条目的 AppendEntries 一致性检查，因为该条目需要一个先前的日志索引和任期**。**为了启用群集成员变更（第 6 节），快照还包含截至最后包含的索引的最新日志中的配置。一旦服务器完成写入快照，它可以删除所有到最后包含索引的日志条目，以及任何先前的快照。

尽管服务器通常独立进行快照，但领导者必须偶尔向落后于的追随者发送快照。这种情况发生在领导者已经丢弃了需要发送给追随者的下一个日志条目时。幸运的是，在正常操作中，这种情况不太可能发生：一个跟得上领导者的追随者应该已经有了这个条目。然而，一个异常缓慢的追随者或一个新加入集群的服务器（第 6 节）不会拥有该条目。使此类追随者保持最新状态的方法是，领导者通过网络向其发送快照。

领导者使用一种新的 RPC（远程过程调用）称为 InstallSnapshot 来向落后于的追随者发送快照；见图 13。当追随者通过这个 RPC 接收到快照时，它必须决定如何处理现有的日志条目。通常情况下，快照将包含在接收者的日志中并不存在的新信息。在这种情况下，追随者会丢弃整个日志；所有内容都被快照取代，且可能包含与快照冲突的未提交条目。**如果追随者接收到的快照描述其日志的前缀（由于重传或错误），则由快照覆盖的日志条目会被删除，但快照之后的条目仍然有效并必须保留**。

这种快照方法偏离了 Raft 的强领导原则，因为追随者可以在没有领导者知识的情况下进行快照。然而，我们认为这种偏离是合理的。虽然有领导者有助于避免在达成共识时出现冲突决策，但在快照时已经达成了共识，所以不会出现冲突。数据仍然只从领导者流向追随者，追随者现在可以重新组织它们的数据。

我们曾考虑一种替代的基于领导者的方法，即只有领导者创建快照，然后将这个快照发送给它的每一个追随者。然而，这有两个缺点。首先，将快照发送给每个追随者会浪费网络带宽，并减慢快照过程。每个追随者已经拥有生成自己快照所需的信息，对于服务器来说，从其本地状态生成快照通常比通过网络发送和接收快照便宜得多。其次，领导者的实现将更加复杂。例如，领导者需要在并行将新日志条目复制给追随者的同时发送快照，以便不阻塞新的客户端请求。

还有两个问题影响快照性能。首先，服务器必须决定何时进行快照。如果服务器快照太频繁，会浪费磁盘带宽和能量；如果快照太少，则可能会耗尽存储容量，并增加在重启时重放日志所需的时间。一种简单的策略是，当日志达到固定的字节大小时进行快照。如果这个大小设置得明显大于快照的预期大小，则快照的磁盘带宽开销将很小。

第二个性能问题是，写入快照可能需要相当长的时间，我们不希望这延迟正常操作。解决方案是使用写时复制技术，以便在写入快照时能接受新的更新而不影响快照。例如，使用功能性数据结构构建的状态机自然支持这一点。或者，可以使用操作系统的写时复制支持（例如，在 Linux 上的 fork）来创建整个状态机的内存快照（我们的实现使用这种方法）。

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250505144136987.png" alt="image-20250505144136987" style="zoom:33%;" />

```
用于领导者向跟随者发送快照块。领导者始终会按顺序发送块。

参数：

term

领导者的任期

leaderId

以便跟随者可以重定向客户端


快照替代所有条目，直到包括此索引

lastIncludedTerm

lastIncludedIndex 的任期

offset

块在快照文件中的字节偏移位置

data[]

从偏移处开始的快照块的原始字节数据

done

如果这是最后一个块，则为真

结果：

term
当前任期，以便领导者更新自身
接收者实现：

如果 term 小于 currentTerm，立即回复。
如果第一个块（偏移量为 0），则创建新的快照文件。
在给定的偏移量写入数据到快照文件。
回复并等待更多数据块，如果 done 为假。
保存快照，丢弃任何现有的或部分快照（具有较小的索引）。
如果现有日志条目具有与快照的最后包括条目相同的索引和任期，保留后面的日志条目并回复。
丢弃整个日志。
使用快照内容重置状态机（并加载快照的集群配置）。
图13：InstallSnapshot RPC 的概述。

```



### 客户端交互

本节描述了客户端如何与 Raft 进行交互，包括客户端如何找到集群领导者，以及 Raft 如何支持线性化语义。对于所有基于共识的系统，这些问题都适用，而 Raft 的解决方案与其他系统类似。

Raft 的客户端将所有请求发送给领导者。当客户端首次启动时，它连接到一个随机选择的服务器。如果客户端首先连接的服务器不是领导者，该服务器将拒绝该客户端的请求，并提供其所知道的最新领导者的信息（AppendEntries 请求中包括领导者的网络地址）。如果领导者崩溃，客户端请求将超时；客户端随后将再次尝试连接随机选择的服务器。

我们对 Raft 的目标是实现线性化语义（每个操作似乎在其调用和响应之间的某个时间点瞬时执行且仅执行一次）。然而，正如之前所述，Raft 可能会多次执行一个命令：例如，如果领导者在提交日志条目后但在响应客户端之前崩溃，客户端将会在新领导者下重试该命令，这会导致该命令被执行第二次。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机会跟踪**每个客户端处理的最新序列号以及相应的响应**。如果它接收到的命令的序列号已经被执行，则立即响应，而不重新执行请求。

只读操作可以在不将任何内容写入日志的情况下处理。然而，如果没有额外的措施，这样做可能会有返回过时数据的风险，因为响应请求的领导者可能已被一个更高版本的领导者替代，而它并未意识到这一点。线性化读取不得返回过时数据，因此 Raft 需要采取两个额外的预防措施，以保证在不使用日志的情况下实现这一点。首先，领导者必须具备有关哪些条目已提交的最新信息。领导者完整性属性保证领导者拥有所有已提交的条目，但在其任期开始时，它可能不知道哪些条目是已提交的。为了了解这一情况，领导者需要提交一个来自其任期的条目。Raft 通过让每个领导者在其任期开始时向日志中提交一个空的无操作（no-op）条目来处理这一点。其次，领导者在处理只读请求之前必须检查自己是否被罢免（如果选举产生了更近期的领导者，则其信息可能过时）。Raft 通过让领导者在响应只读请求之前与集群中过半数节点交换心跳消息来处理这一点。或者，领导者也可以依靠心跳机制来提供一种租约机制，但这将依赖于时间的安全性（假设时钟漂移有限）。