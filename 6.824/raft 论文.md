共识算法使一组计算机能够作为一个**一致的整体**协同工作，即使其中部分成员发生故障也能继续运行。因此，它们在构建**可靠的大规模软件系统**中扮演着关键角色。

在过去的十年中，**Paxos** [15, 16] 主导了关于共识算法的讨论：大多数共识实现都是基于 Paxos 或受到它的影响，并且 Paxos 成为了教授学生关于共识概念的主要工具。

然而，不幸的是，**Paxos 非常难以理解**，尽管已有许多努力试图让它变得更容易接近。此外，Paxos 的架构在支持实际系统时需要做出复杂的调整。因此，无论是系统构建者还是学生，往往都难以掌握 Paxos。

在我们自己也深受 Paxos 困扰之后，我们决定设计一种**新的共识算法**，希望它能够为**系统构建与教学**提供更好的基础。我们的方法不同寻常，**首要目标是“易于理解”**：我们能否定义一个适用于实际系统的共识算法，并以一种比 Paxos 显著更易于学习的方式进行描述？此外，我们还希望该算法有助于开发人员形成构建系统所需的直觉。我们追求的不仅仅是算法“可行”，更重要的是让人**清晰地理解它为什么可行**。

这项工作的成果就是一个名为 **Raft** 的共识算法。在设计 Raft 的过程中，我们采用了特定的策略来提升其可理解性，包括：

- **模块化设计**（Raft 将领导者选举、日志复制与安全性逻辑进行分离）
- **状态空间简化**（与 Paxos 相比，Raft 减少了系统中非确定性行为的程度和服务器之间的不一致方式）

我们在两个大学对 43 名学生进行的用户研究显示：**Raft 比 Paxos 明显更容易理解**。这些学生在学习了两种算法之后，有 33 人在 Raft 相关问题上的回答优于 Paxos。

Raft 在很多方面与现有的共识算法相似（最值得一提的是 Oki 和 Liskov 提出的 Viewstamped Replication [29, 22]），但它也有一些**独特的特性**：

- **强领导者模型**：Raft 使用一种比其他共识算法更强的领导者机制。例如，日志条目**只从领导者流向其他服务器**，这简化了日志复制的管理，使 Raft 更易理解。
- **领导者选举机制**：Raft 使用**随机定时器**来选举领导者。这只在原本共识协议中就需要的心跳机制基础上增加了很小的复杂度，同时能够**简单、快速地解决冲突**。
- **成员变更机制**：Raft 引入了一种新的**联合共识（joint consensus）机制**来变更集群中的服务器集合。该机制在过渡期间让旧配置与新配置的多数成员**重叠**，从而确保在变更过程中，集群能够**持续正常运行**。

我们相信，Raft 优于 Paxos 和其他共识算法，**无论是用于教学，还是作为系统实现的基础**。它比其他算法更简单、更易理解；它的描述**足够完整，能够满足实际系统的需求**；它已有多个开源实现，并被多家公司实际使用；其安全性已被**形式化地定义与证明**；而且其效率也可与其他算法相媲美。

本文接下来的结构如下：

- **第2节**介绍了复制状态机问题；
- **第3节**讨论了 Paxos 的优缺点；
- **第4节**描述了我们在提升可理解性方面的总体设计方法；
- **第5至第8节**介绍了 Raft 共识算法；
- **第9节**对 Raft 进行了评估；
- **第10节**讨论了相关研究工作。



## 2. 复制状态机（Replicated State Machines）

共识算法通常用于实现**复制状态机（replicated state machines）** [37]。在这种方法中，一组服务器上的状态机维护相同状态的副本，即使部分服务器宕机也能继续运行。复制状态机被广泛用于分布式系统中的各种容错问题。例如，一些拥有集群领导者的大规模系统，如 GFS [8]、HDFS [38] 和 RAMCloud [33]，通常使用一个独立的复制状态机来管理**领导者选举**并存储需在领导者崩溃时仍能保留的配置信息。复制状态机的具体例子包括 **Chubby** [2] 和 **ZooKeeper** [11]。

复制状态机通常通过**复制日志（replicated log）**实现，如图 1 所示。每台服务器都维护一个包含一系列命令的日志，其状态机会按照顺序执行这些命令。所有日志中的命令顺序相同，因此每个状态机处理的命令序列也相同。由于状态机是确定性的（deterministic），因此它们计算出的状态和输出结果也完全一致。

保持日志一致的任务由**共识算法**负责。每台服务器上的共识模块接收来自客户端的命令并将其添加到自己的日志中。它与其他服务器的共识模块通信，确保每台服务器的日志最终包含相同顺序的请求，即便部分服务器发生故障。一旦命令被正确复制，每台服务器的状态机就会按照日志顺序处理它们，并将结果返回给客户端。结果就是，**整个集群看起来就像是一台高度可靠的状态机**。

为满足实际系统需求，共识算法通常具备以下特性：

- **安全性保证**：在所有非拜占庭（non-Byzantine）错误场景下（包括网络延迟、分区、丢包、重复或乱序），都**不会返回错误结果**；
- **高可用性**：只要**多数服务器仍可运行并彼此通信**（包括能与客户端通信），整个系统就可以继续提供服务。例如，一个由 5 台服务器组成的集群可以容忍任意 2 台服务器的故障。假设服务器只会“停止运行”（crash-stop），它们可以通过读取稳定存储（stable storage）上的状态来恢复并重新加入集群；
- **与时间无关的正确性**：共识算法**不依赖时间**来保证日志的一致性，哪怕时钟出现故障或消息极端延迟，最多也只会影响可用性，不会破坏一致性；
- **快速达成共识**：在常见情况下，当集群多数节点对一次远程过程调用（RPC）轮次响应时，命令即可被提交。系统性能**不会被少数缓慢服务器所拖累**。



## 3. Paxos 的问题在哪里？

在过去十年中，**Leslie Lamport 的 Paxos 协议** [15] 已几乎成为共识算法的代名词：它是课程中最常被教授的协议，也是大多数共识实现的起点。

Paxos 最初定义了一种能够在**单次决策（single-decree）**中达成一致的协议，例如一个复制日志条目的达成一致。我们将这一子集称为 **单次 Paxos（single-decree Paxos）**。之后，Paxos 将多个单次决策实例组合起来，以实现一个序列的决策（即日志），称为 **Multi-Paxos**。

Paxos 能够保证**安全性与活性（safety and liveness）**，支持集群成员变更，其正确性也已被证明，在常规情况下效率也不差。

但遗憾的是，Paxos 有两个重大缺陷：

------

**第一个缺陷**是：**Paxos 极其难以理解。** 其完整的原始描述 [15] 被认为**晦涩难懂**；很少有人能真正搞懂 Paxos，且往往需要付出巨大努力。因此，已有很多尝试试图用更简洁的方式重新解释 Paxos [16, 20, 21]。这些解释主要集中在单次 Paxos 上，但即使如此，依然非常复杂。在 NSDI 2012 的一项非正式调查中，即使是资深研究人员中，也很少有人能自信地说他们真正理解了 Paxos。

我们自己在学习 Paxos 的过程中也非常痛苦：在阅读了多个简化版本并尝试自己设计替代协议之后，才最终理解了完整协议，这个过程花费了**将近一年的时间**。

我们猜测 Paxos 的晦涩之处**源自于以“单次 Paxos”作为基础出发点**。单次 Paxos 十分复杂且微妙：它被划分为两个阶段，这两个阶段既没有直观的解释，也不能独立理解。这就导致了我们很难建立关于其工作原理的直觉理解。而多次 Paxos（Multi-Paxos）的组合规则更是增加了极大的复杂性与微妙之处。

我们认为，要想实现“对多个条目的共识”这一目标（即日志，而非单个决策），可以通过其他**更直接、更易理解的方式**来实现。

------

**第二个问题**是：Paxos **无法为实际系统提供良好的实现基础**。

原因之一是：**当前并没有一个被广泛接受的 Multi-Paxos 实现算法**。Lamport 的描述主要集中于单次 Paxos；他只是简单勾勒了 Multi-Paxos 的可能方法，很多实现细节都缺失了。虽然已经有多个尝试对 Paxos 进行扩展和优化的工作，如 [26]、[39] 和 [13]，但这些工作彼此之间，甚至与 Lamport 自己的描述都存在差异。

像 Chubby [4] 这样的系统实现了类似 Paxos 的算法，但多数情况下，它们的具体实现细节**并未公开发表**。





**此外，Paxos 的架构并不适合构建实际系统；这也是其采用“单决议”划分方式的另一个后果。**
 例如，单独选择一组日志条目并再将它们拼接成一个顺序日志，其实并无太大好处，反而增加了复杂性。相反，更简单、高效的方式是直接围绕一个顺序追加的日志进行系统设计。另一个问题是，Paxos 的核心使用了一种对等对称的方式（虽然后来提出了一种弱领导者模型作为性能优化）。在仅需要做出一个决策的简化场景中，这种方式是合理的，但在实际系统中，很少采用这种方式。如果系统需要连续做出多个决策，那么首先选出一个领导者并由其协调决策会更简单、更快速。

因此，**实际系统与 Paxos 差异巨大。** 每个实现者都会从 Paxos 开始，随后发现实现起来困难重重，最终不得不构建出一个架构大为不同的系统。这一过程既耗时又容易出错，而理解 Paxos 的困难又进一步加剧了问题。虽然 Paxos 的形式化定义在证明正确性方面非常有用，但现实中实际的实现方式与它偏差甚远，这也使得原有的形式化证明几乎失去了价值。下面是来自 Chubby 系统实现者的典型评论：

> “Paxos 算法的描述与实际系统的需求之间存在显著差距……最终的系统基于一个未经证明的协议 [4]。”

正因为这些问题，**我们认为 Paxos 并不是系统构建或教学的良好基础。** 鉴于一致性协议在大规模软件系统中的关键地位，我们决定尝试设计一个在多个方面优于 Paxos 的新一致性算法。Raft 就是我们这一实验的产物。

------

### 4. 为了可理解性而设计

在设计 Raft 时，我们有几个目标：

- 它必须是一个**完整且实用的系统构建基础**，能够显著减少开发人员所需的设计工作量；
- 它在所有条件下都必须是**安全的**，并且在典型运行环境下应具有**可用性**；
- 它在常见操作中必须是**高效的**。

但我们最重要的目标、也是最具挑战性的目标是：**可理解性**。
 它必须足够易于理解，能让更广泛的受众感到舒适地掌握算法。同时，它也必须能帮助系统构建者形成直觉，这样他们才能在现实场景中对系统进行扩展。

在 Raft 的设计过程中，我们在多个地方面临不同实现路径的选择。这些时候，我们会以可理解性为评估标准：

- 每个选择的解释是否复杂？
- 状态空间是否庞杂？
- 是否容易产生难以察觉的副作用？
- 读者是否能轻松理解设计的含义及其后果？

我们当然知道这种分析本身具有一定的主观性，但仍采用了两种通用的技术来提高可理解性：

1. **问题分解（Problem Decomposition）**：
    在可能的地方，我们将问题拆解成多个子问题，从而能分别解决、解释和理解。例如，在 Raft 中，我们将领导者选举、日志复制、安全性和成员变更等模块进行了明确划分。
2. **简化状态空间（State Space Reduction）**：
    通过减少可能出现的系统状态，我们提高了系统的一致性，并尽可能消除了非确定性。具体来说，Raft **不允许日志中出现“洞”**，并且限制了不同节点之间日志不一致的形式。
    虽然我们多数时候试图消除非确定性，但也有些场景下**适当的非确定性反而有助于理解。**
    特别是在**随机化机制**中，虽然引入了非确定性，但它减少了状态空间的复杂度（例如，“从多个中随机选择任一个——无所谓”）。我们就采用了随机计时器来简化 Raft 的领导者选举过程。

------

### 5. Raft 一致性算法

Raft 是一种用于管理**复制日志（replicated log）**的一致性算法，如第 2 节所描述。图 2 给出了该算法的简要参考总结，图 3 列出了该算法的关键属性；我们将在本节剩下部分逐一展开讨论。

Raft 的核心思想是：**先选举出一个唯一的领导者（leader）**，再由该领导者全权负责管理复制日志。领导者从客户端接收日志条目，将其复制到其他服务器上，并告诉各节点何时可以将日志条目应用到状态机中。
 由于采用领导者机制，日志管理得到了简化。例如，领导者可以在不咨询其他节点的前提下决定日志条目的位置，数据的流动也变得更简单——从领导者流向其他服务器。如果领导者失败或与其他服务器失联，就会触发新的领导者选举过程。

基于该领导者机制，Raft 将共识问题拆分为三个相对独立的子问题（我们将在后续小节详细介绍）：

- **领导者选举（Leader Election）**：在现有领导者失效时，如何选出新的领导者（见 5.2 节）。

- **日志复制（Log Replication）**：领导者如何接收来自客户端的日志条目并在集群内进行复制，使各副本日志保持一致（见 5.3 节）。

- **安全性（Safety）**：Raft 最核心的安全属性是**状态机安全性（State Machine Safety）**：

  > “一旦某个服务器将某个日志条目应用到状态机中，则其他任何服务器都不允许在该日志索引处应用不同的命令。”
  >  5.4 节将介绍 Raft 如何保证这一安全性，其中还引入了对选举机制的进一步限制（也在 5.2 节中讨论）。

在介绍完一致性算法之后，本节还将进一步讨论系统的**可用性（availability）\**以及系统中\**时间（timing）的作用**。



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250412163901704.png" alt="image-20250412163901704" style="zoom:50%;" />

### 🧠 小结：

| 步骤 | 动作             | 模块                   |
| ---- | ---------------- | ---------------------- |
| ①    | 客户端发送请求   | Client → Server        |
| ②    | 共识模块复制日志 | Consensus Module → Log |
| ③    | 日志应用到状态机 | Log → State Machine    |
| ④    | 返回结果给客户端 | Server → Client        |





- Raft 要求：**日志不能“回滚”已提交的条目，但可以丢弃未提交的冲突日志**。
- 只有在多数节点都接收了日志项时，才能被视为已“commit”。
- 所以未 commit 的日志项（如分区时写入的）是可以被丢弃的，保持系统整体一致性。



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250412184442613.png" alt="image-20250412184442613" style="zoom:50%;" />



## 🧠 为什么不能只用一个？

因为：

- 如果你**只用 `nextIndex`**：
  - 你不知道哪些日志已经同步成功了，只能“猜测”。
  - 判断提交（commitIndex）就会出错。
- 如果你**只用 `matchIndex`**：
  - 你无法知道下一次应该从哪里开始补发，会导致每次都从头开始尝试。

这两个变量，一个负责“**发**”，一个负责“**确认**”。一个是前瞻性的控制指针，一个是落后的确认指针。

------

## 🧪 举个场景说明

假设 leader 要将日志 `[1,2,3,4,5]` 复制给 follower：

1. 初始化时：

   ```
   复制编辑
   nextIndex[i] = 6  （从 6 开始发，但其实 follower 可能最多只同步到 3）
   matchIndex[i] = 0 （还没确认任何复制成功）
   ```

2. 第一次 AppendEntries(prevLogIndex=5)，follower 拒绝（没有这条）

   - Leader：`nextIndex[i] -= 1 → 5`，继续尝试

3. 第二次 AppendEntries(prevLogIndex=4)，follower 还是否

   - Leader：`nextIndex[i] -= 1 → 4`

4. 第三次 AppendEntries(prevLogIndex=3)，follower 成功接受

   - Leader：`matchIndex[i] = 3`，`nextIndex[i] = 4`

这时候：

```
bash


复制编辑
nextIndex[i] = 4       # 下一条准备发第 4 条
matchIndex[i] = 3      # follower 已同步到第 3 条
```

👉 **如果你没有 `matchIndex`，你怎么知道他已经同步到 3？**



## 🟧 **Persistent state on all servers**

（**所有服务器的持久化状态**：必须写入稳定存储后才能响应 RPC）

这些数据必须在宕机后保持不变（持久化），因为它们对一致性保证至关重要。

### 1. `currentTerm`

- 含义：当前服务器所知的最新**任期号**。
- 初始化为 0，**每次进入新任期时递增**。
- 该值随时间单调递增，确保不重复使用旧任期。

### 2. `votedFor`

- 含义：当前任期内，**本节点已投票的候选人 ID**。
- 初始为 null（未投票），每个任期最多只能投一票。

### 3. `log[]`

- 含义：日志条目数组，每条包含：
  - 客户端的命令；
  - 以及该条目被写入时的任期号。
- 所有节点维护相同顺序的日志是 Raft **一致性核心**。

------

## 🟨 **Volatile state on all servers**

（**所有服务器的易失状态**：内存中的数据，宕机会丢失）

这些状态不需要持久化，在重新启动时可以从日志中恢复。

### 4. `commitIndex`

- 含义：已**提交的最高日志索引**。
- “已提交”表示大多数节点都复制了该条日志，并可以被应用到状态机中。
- 初始值为 0，单调递增。

### 5. `lastApplied`

- 含义：已**应用到状态机的最高日志索引**。
- 通常满足：`lastApplied <= commitIndex`
- 应用过程通常由后台线程定期检查推进。

------

## 🟩 **Volatile state on leaders**

（**仅领导者维护的易失状态**，每次当选后重新初始化）

这些状态是为了实现日志复制给 follower 服务的。

### 6. `nextIndex[]`

- 含义：**每个 follower 的下一个要发送的日志索引**。
- 初始化为 `leader 最后日志索引 + 1`。
- 如果 follower 的日志落后，leader 会回退 `nextIndex`，并重发日志。

### 7. `matchIndex[]`

- 含义：**每个 follower 已知的最后一条复制成功的日志索引**。
- 用于判断哪些日志可以被“提交”（大多数节点已复制）。

------

## ✅ 举个例子说明作用：

假设有 5 个节点（A-E），A 是 leader：

| 节点       | log[]   | nextIndex[A] | matchIndex[A] |
| ---------- | ------- | ------------ | ------------- |
| A (leader) | 1 2 3 4 | -            | -             |
| B          | 1 2 3   | 4            | 3             |
| C          | 1 2 3   | 4            | 3             |
| D          | 1 2     | 3            | 2             |
| E          | 1 2 3 4 | 5            | 4             |

此时 A 会根据 `matchIndex[]` 发现有三个节点已经复制到 3 或以上，因此可以**提交第3条日志**，并推进 `commitIndex=3`。

------

## 🔁 状态分类汇总表：

| 分类            | 字段           | 说明                                     |
| --------------- | -------------- | ---------------------------------------- |
| **持久化**      | `currentTerm`  | 当前任期                                 |
|                 | `votedFor`     | 当前任期投票对象                         |
|                 | `log[]`        | 日志列表                                 |
| **易失**        | `commitIndex`  | 已提交的日志索引                         |
|                 | `lastApplied`  | 已应用到状态机的日志索引                 |
| **Leader 专属** | `nextIndex[]`  | 要发给每个 follower 的下一个日志索引     |
|                 | `matchIndex[]` | 每个 follower 上已成功复制的最大日志索引 |



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250412184539621.png" alt="image-20250412184539621" style="zoom:50%;" />

这张图详细描述了 Raft 中一个关键的 RPC（远程过程调用）——**AppendEntries RPC** 的使用方式、参数、响应结果以及 follower 的处理逻辑。

这个 RPC 是 **由 Leader 发起** 的，用于：

- 日志复制（主用途，详见 §5.3）
- 心跳信号（即定期发送空 `entries[]` 保持领导权，详见 §5.2）

------

## 🔹参数说明（Arguments）

| 参数           | 说明                                                         |
| -------------- | ------------------------------------------------------------ |
| `term`         | Leader 当前的任期号。Follower 用它来更新自己落后的 `currentTerm`。 |
| `leaderId`     | Leader 的 ID。Follower 可以将客户端请求重定向给这个 leader。 |
| `prevLogIndex` | 当前要发送的第一条新日志项的前一个索引。用于验证日志一致性。 |
| `prevLogTerm`  | `prevLogIndex` 对应的日志项的任期号。配合上一个参数用于**日志匹配**。 |
| `entries[]`    | 要追加的日志条目数组，可能为空（用于心跳）；也可能一次发送多个条目以提高效率。 |
| `leaderCommit` | Leader 当前的 `commitIndex`，用于通知 Follower 该提交哪些日志。 |

------

## 🔸结果返回（Results）

| 字段      | 说明                                                         |
| --------- | ------------------------------------------------------------ |
| `term`    | Follower 返回自己的当前任期号。如果 Leader 收到更高 term，需要更新并转为 follower。 |
| `success` | 若 Follower 的日志中存在匹配的 `prevLogIndex` 和 `prevLogTerm`，返回 true，否则返回 false。 |

------

## 🔸Receiver Implementation（Follower 端处理逻辑）

这部分是实现的精髓，核心就是**保持 Leader 与 Follower 之间的日志一致性**。

------

### 1. `Reply false if term < currentTerm`

- 若 Leader 的 `term` 比自己小，说明是**过期的 Leader**，直接拒绝。

------

### 2. `Reply false if log doesn't contain entry at prevLogIndex whose term matches prevLogTerm`

- 检查 Follower 的日志中是否存在符合 `prevLogIndex` 且任期为 `prevLogTerm` 的条目。
- 如果没有，说明 **Leader 和 Follower 的日志已经出现冲突**，返回 false。

------

### 3. `If an existing entry conflicts with a new one... delete the existing entry and all that follow it`

- 冲突的定义：**相同的 index 不同的 term**
- 一旦发现冲突，Follower 会：
  - 删除该条日志；
  - 以及它之后所有的日志（这一步很重要，是修复日志不一致的关键）。

------

### 4. `Append any new entries not already in the log`

- 将 Leader 提供的 `entries[]` 中的新条目**追加到日志中**（不重复追加已存在项）。

------

### 5. `If leaderCommit > commitIndex...`

- 如果 Leader 的 `commitIndex` 比自己大：

  - 将自己的 `commitIndex` 提升为两者中的较小者：

    ```
    go
    
    
    复制编辑
    commitIndex = min(leaderCommit, index of last new entry)
    ```

  - 这一步保证 Follower 不会**过度提交**（提交了还没同步完的日志）。

------

## 🔁 整体流程图解（结合图像）

配合你上传的图像，可以将过程简化为：

```
markdown


复制编辑
1. Leader 定期给所有 Follower 发 AppendEntries（心跳或日志）。
2. Follower 检查日志是否匹配（index & term）。
3. 若匹配：
   - 删除冲突日志；
   - 追加新日志；
   - 更新 commitIndex；
   - 返回 success。
4. 若不匹配：
   - 返回失败，Leader 会回退 nextIndex 重新试。
```

------

## ✅ 总结：为什么 AppendEntries 是 Raft 的核心？

- 保证了 **日志的一致性**；
- 驱动了 **客户端请求复制与提交**；
- 实现了 **Leader 与 Follower 的日志同步机制**；
- 支持了 **心跳维持领导地位**；
- 通过 `term` 字段完成了 **任期合法性判断**



## 🔍 冲突发生的典型场景

冲突会出现在 **Follower 的日志与 Leader 的日志不一致**时，主要有两种情况：

------

### ✅ **情况 1：Leader 崩溃后，Follower 成为 Leader 写入了日志，但之后被另一个 Leader 推翻**

#### 示例过程：

1. 节点 A 是 Leader，日志到达 index 5。
2. 节点 A 崩溃，节点 B 被选为新的 Leader，写入了 index 6（term 2）。
3. 节点 B 也崩溃了，节点 C 成为新的 Leader，term 3，认为 index 6 应该是自己的，于是发送 `AppendEntries(prevLogIndex=5, prevLogTerm=1)`。

#### 此时：

- Follower（原来的 B）日志中有 index 6，但 term 是 2。
- 当前 Leader C 要求的是 prevLogIndex 5 + entries 6，term 是 3。
- 由于 index 相同（6），但 term 不同（2 vs 3），判定为冲突，触发删除。

------

### ✅ **情况 2：网络分区，分区中的 Leader 追加了日志**

#### 示例过程：

1. 网络分区，原 Leader A 与部分 Follower 通信，term 1，写入 index 6。
2. 分区恢复后，新 Leader B（term 2）当选，发现旧 Leader A 的日志不合法（term 过期），开始 AppendEntries。
3. 新 Leader B 发送的 prevLogIndex=5，prevLogTerm=1，但 Follower（旧 Leader）日志中 index 6 的 term 是 1（旧 term）。
4. 一旦 Leader B 的 term 是 2，而 Follower 的日志在 index 6 是 term 1，就会触发删除操作。

------

## ✏️ 图示类比

| Index | A（旧 Leader） | B（新 Leader）     |
| ----- | -------------- | ------------------ |
| 5     | term 1         | term 1             |
| 6     | term 1         | term 2（准备写入） |

Leader B 发来的 `AppendEntries` 希望 index 5 是 term 1，然后写入 term 2 的新条目。

如果发现 A 的 index 6 是旧 term 1，发生冲突 —— 删除 index 6 及其后续日志，统一由新 Leader 重写。

------

## 🧠 为什么 Raft 这么做？

- Raft 要求：**日志不能“回滚”已提交的条目，但可以丢弃未提交的冲突日志**。
- 只有在多数节点都接收了日志项时，才能被视为已“commit”。
- 所以未 commit 的日志项（如分区时写入的）是可以被丢弃的，保持系统整体一致性。

------

## ✅ 总结：什么时候会触发冲突删除？

- 节点因网络或崩溃落后，然后被新 Leader 更新日志；
- 相同 index 不同 term 出现（可能由多个 Leader 写入造成）；
- Follower 根据 `AppendEntries` 的参数识别冲突；
- 删除冲突条目及其之后所有条目，确保日志与 Leader 一致。



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250412184702025.png" alt="image-20250412184702025" style="zoom:50%;" />









## ✅ RequestVote RPC（候选人用来拉选票）

### 🔹 用途：

候选人通过发送 RequestVote RPC 给其他服务器，来拉取选票，试图成为新的 Leader。

### 🔹 参数（Arguments）：

- `term`：候选人的当前任期（term）。
- `candidateId`：候选人自己的 ID。
- `lastLogIndex`：候选人日志的最后一条的 index。
- `lastLogTerm`：候选人最后一条日志的任期。

> 这两个日志参数是用来判断候选人日志是否“足够新”的依据。

### 🔹 返回值（Results）：

- `term`：接收者的当前任期，候选人可能需要据此更新自己的任期。
- `voteGranted`：布尔值，表示是否投票给了该候选人。

------

### 🔹 接收方执行逻辑（Receiver Implementation）：

1. **拒绝旧任期请求**：如果 `term < currentTerm`，拒绝投票（返回 false）。
2. **判断是否投票**：
   - 如果该服务器本轮还没投票（`votedFor == null`），或已投给该候选人（`votedFor == candidateId`）；
   - 并且候选人的日志**不比自己旧**（Raft 的日志“新”定义见论文 §5.4）；
   - 那就**投票给候选人**（返回 true）。

------

## ✅ Rules for Servers（所有服务器的行为规则）

------

### 🟨 所有服务器通用行为：

- 如果 `commitIndex > lastApplied`，说明还有没提交给状态机的日志，执行应用到状态机的操作。
- 如果收到的 RPC 请求/响应中 term 比当前大：更新当前任期为 T，并转为 Follower。

------

### 🟩 Follower 状态行为：

- 响应来自候选人或领导者的 RPC。
- 如果超时时间到达，既没收到心跳（AppendEntries），也没投票给其他人： ➜ 说明 Leader 挂了，自己转为候选人。

------

### 🟥 Candidate 状态行为：

- 一旦转为候选人，开始选举：
  - 增加当前任期（`currentTerm++`）
  - 给自己投票
  - 重置选举计时器
  - 向所有其他节点发送 RequestVote RPC
- 如果获得多数投票 ➜ 成为 Leader。
- 如果收到合法 Leader 的 AppendEntries ➜ 转为 Follower。
- 如果选举超时还没当选 ➜ 再发起新一轮选举。

------

### 🟦 Leader 状态行为：

- 一旦当选 Leader：

  - 向所有节点发送空的 AppendEntries（心跳）来维持权威。
  - 周期性发送心跳防止触发选举超时。

- 接收到客户端指令：

  - 追加到自己的日志；
  - 日志提交成功后再回复客户端（§5.3）。

- 日志复制规则：

  - 若 Follower 缺日志，从 nextIndex 开始向它发送 AppendEntries。
  - 发送成功 ➜ 更新该 Follower 的 `nextIndex` 和 `matchIndex`。
  - 如果失败（日志冲突） ➜ `nextIndex--`，重试。

- **提交规则**（最重要的条件）：

  > 如果存在一个日志索引 `N`，满足：

  - `N > commitIndex`
  - **超过半数的 matchIndex[i] ≥ N**
  - `log[N].term == currentTerm`（不能是旧 Leader 写的）

  ➜ 那就更新 `commitIndex = N`，这条日志就可以被应用到状态机了。

------

## 🧠 总结重点

| 角色          | 行为                                 |
| ------------- | ------------------------------------ |
| **Follower**  | 被动等待，响应 RPC，超时转 Candidate |
| **Candidate** | 主动拉票，胜选变 Leader，失败继续选  |
| **Leader**    | 发心跳、日志复制、提交管理           |

RequestVote 是 Raft 中实现 Leader 选举的关键，配合心跳（AppendEntries）机制一起维持系统的稳定性和活性。





## **Raft 的整体共识流程**

Raft 是一个用来管理“复制日志”（replicated log）的算法，如第 2 节所述，它通过选出一个**领导者（leader）**，由该领导者负责处理所有客户端请求，并将日志复制到其他服务器上。

Raft 将共识问题拆解为三个相对独立的子问题：

1. **领导者选举**（Leader Election）— §5.2
2. **日志复制**（Log Replication）— §5.3
3. **安全性保证**（Safety）— §5.4

这样的分解设计，使得 Raft 更容易理解和实现。

------

## **5.1 Raft 基本机制**

Raft 集群由多个服务器组成，通常使用 5 个服务器，这样能容忍最多 2 台故障。

每台服务器在任意时刻有三种状态之一：

- **Follower（追随者）**：默认状态，只响应请求；
- **Candidate（候选人）**：当选举超时触发时转换为候选人；
- **Leader（领导者）**：唯一对客户端提供服务的节点，负责处理所有请求。

状态的转换如图 4 所示：

- Follower 超时没收到心跳 → 成为 Candidate；
- Candidate 获得多数投票 → 成为 Leader；
- Candidate 收到有效 leader 消息 → 回退为 Follower。

------

## **任期 Term 和逻辑时钟**

Raft 把时间划分为连续编号的**任期（term）**，每个任期开始于一轮**选举**。可能的结果：

- 有人赢得选举：本任期由该 Leader 管理；
- 无人胜出（平票）：该任期结束，无 Leader，开启下一轮新选举。

任期相当于 Raft 的**逻辑时钟（logical clock）**：

- 所有通信中都携带 term；
- 节点每次通信都会比较 term，谁小就更新为大的，并转换成 Follower。

这样可以**避免老旧 leader 的错误操作**，确保“一个 term 最多一个 leader”。

------

## **RPC 通信机制**

Raft 的共识流程只需两个 RPC：

- `RequestVote`：候选人发起的请求投票；
- `AppendEntries`：领导者发起的日志追加请求（也当作心跳）。

所有 RPC 都可以并发执行，如果超时未响应会自动重试。

------

## **5.2 领导者选举机制**

选举是通过心跳机制触发的：

- 初始时所有节点都是 Follower；
- 如果 Follower 在一段时间内**没有收到来自 leader 或候选人的有效 RPC**，就会触发选举。

选举过程如下：

1. Follower 变成 Candidate，进入新 term；
2. 投票给自己；
3. 并发向其他服务器发送 RequestVote RPC；
4. 等待投票结果或其他情况发生。

候选人可能遇到三种情况：

### **(a) 获胜：**

- 若获得**集群过半节点**的投票（如 5 个节点需至少 3 票），就当选为 Leader；
- 随后开始发送心跳（AppendEntries）以维持地位，防止其他节点发起选举。

### **(b) 失败：**

- 候选人可能收到一个来自**新 Leader** 的 `AppendEntries RPC`；
- 如果对方的 term 不小于自己的 term，说明它是合法 leader，自己应回退为 follower。

### **(c) 平票：**

- 多个节点几乎同时成为候选人，彼此投票打平，**无人赢得选举**；
- 所有候选人等待自己的**随机选举超时**后重新发起下一轮选举。

> ✨ 为避免长时间平票，Raft 设计了“**随机化的选举超时**”（通常 150–300ms 之间）：

- 每个节点选一个不同的超时；
- 大多数情况下，第一个超时的人先发起选举，并赢得领导权；
- 平票也能很快自动打破。



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250412190628900.png" alt="image-20250412190628900" style="zoom:50%;" />

### **5.1 Raft 基础**

一个 Raft 集群包含多个服务器；五个是一个典型数量，允许系统容忍两个故障。在任意时刻，每个服务器处于三种状态之一：**Leader（领导者）**、**Follower（跟随者）\**或\**Candidate（候选者）**。在正常操作中，恰好有一个领导者，其余服务器都是跟随者。跟随者是被动的：它们不主动发起请求，仅响应来自领导者和候选者的请求。领导者负责处理所有客户端请求（如果客户端联系的是跟随者，跟随者会将其重定向至领导者）。候选者状态则用于选举新领导者（详见 §5.2）。

Raft 将时间划分为任意长度的**任期（term）**，如图 5 所示。任期由连续的整数编号组成。每个任期从一次选举开始，一个或多个候选者尝试成为领导者。如果某个候选者赢得选举，它将在本任期内担任领导者。在某些情况下，选举可能出现票数均分的情况，此时该任期会以“无领导者”结束，随后开启新的任期和新的选举。

图 4 展示了状态转换过程。不同服务器可能在不同时间观察到任期转换；有些服务器甚至可能错过整个任期。Raft 使用任期作为**逻辑时钟**来检测过时信息（如过时的领导者）。每个服务器都维护一个当前任期号（currentTerm），该值会单调递增。服务器之间通信时会交换该任期值；如果发现自己任期落后，会立即更新为更大的值。如果候选者或领导者发现自己任期过时，它会立即转为跟随者。如果收到一个任期过时的请求，则会拒绝该请求。

Raft 服务器使用远程过程调用（RPC）进行通信，基本共识算法仅需两种 RPC：

- **RequestVote RPC**：由候选者在选举期间发起（§5.2）
- **AppendEntries RPC**：由领导者用于日志复制及心跳保活（§5.3）

第 7 节介绍了一种第三类 RPC，用于在服务器之间传输快照。服务器会在超时未收到回复时重试 RPC，并采用并行发送方式提升性能。

------

### **5.2 领导者选举**

Raft 使用心跳机制来触发领导者选举。服务器启动后初始状态为**跟随者**。只要它能收到来自候选者或领导者的有效 RPC，它就会保持跟随者状态。领导者会周期性地向所有跟随者发送心跳（即不携带日志条目的 AppendEntries RPC），以维持其“权威地位”。

如果某个跟随者在一定时间内（称为**选举超时 election timeout**）未收到任何通信，它将假设当前没有可用的领导者，并启动新一轮选举。

选举流程如下：

1. 跟随者自增当前任期，并转换为候选者状态；
2. 候选者为自己投票，并并行向其他所有服务器发送 RequestVote RPC；
3. 候选者状态将持续，直到发生以下三种情况之一：
   - (a) 它赢得选举；
   - (b) 它检测到其他服务器已成为领导者；
   - (c) 一段时间过去后无人胜出，进入下一轮选举。

若候选者获得**集群大多数服务器**的投票，则赢得选举并成为领导者。每个服务器在一个任期中最多投一票（先到先得）。多数投票规则保证每个任期最多只有一个候选者能胜出（见图 3 中的**选举安全性**属性）。当候选者当选后，它立即向所有服务器发送心跳，宣布自己是领导者并防止进一步选举。

候选者在等待投票期间，可能会收到来自其他声称为领导者的 AppendEntries RPC。如果该 RPC 的任期不小于当前任期，则该候选者承认该领导者的合法性，并回退为跟随者；否则继续为候选者状态。

如果多个服务器同时成为候选者，可能导致投票均分、无人胜出。此时，每个候选者会等待选举超时并自增任期，重新发起新一轮 RequestVote RPC。为避免“无限选不出领导者”的情况，Raft 采用**随机化选举超时（如 150–300ms）**，使服务器超时时间不一致，大概率只有一个候选者最先超时并赢得选举。

------

### **5.3 日志复制**

一旦选出领导者，它便开始处理客户端请求。每个请求携带一个要由状态机执行的命令。领导者会将命令作为新日志条目追加到本地日志中，然后并行向其他服务器发送 AppendEntries RPC 进行复制。

当该日志条目被成功复制至大多数服务器后，领导者会将其提交（commit）并应用到本地状态机，然后将执行结果返回给客户端。即使客户端响应已发出，若有 follower 崩溃或变慢，领导者仍会持续重试 AppendEntries 直至日志被复制成功。

如图 6 所示，日志是按顺序编号的，每个日志条目包含一个状态机命令及其创建时的任期号（term）。这些 term 信息用于检测日志不一致，进而保障图 3 中的一致性属性。

日志条目被视为“已提交”，当且仅当创建该条目的领导者已将其复制到**大多数服务器**。这同时也意味着该条目前的所有日志也被提交（包括上一任领导者创建的）。一旦 Follower 得知某条目已提交，它会按日志顺序将其应用到状态机中。

为保证日志一致性和简洁性，Raft 定义了**日志匹配属性**：

- 若两个日志在相同 index 和 term 上有条目，则它们的命令相同；
- 若两个日志在相同 index 和 term 上有条目，则它们之前的日志完全一致。

AppendEntries 实现了这种一致性检查：领导者在 RPC 中包含新日志条目的前一条的 index 和 term。如果 Follower 无法匹配该 index-term，则拒绝新条目。

------

### **5.4 安全性保障**

尽管前述机制已经支持日志复制和领导者选举，但仍未能完全保证每台状态机始终执行**相同顺序的命令**。例如，某 follower 崩溃后重启，未同步部分已提交的条目却成为新领导者，可能会覆盖这些已提交条目，从而导致状态机执行不同命令。

为防止上述问题，Raft 对候选者设定了额外限制：**新当选的领导者必须包含所有已提交的条目**（见图 3 中的**领导者完整性属性**）。

Raft 的实现方式是：

- 服务器投票时，会比较候选者和自身日志的最新条目（index 和 term）；
- 如果候选者日志“不够新”，则拒绝投票；
- Raft 定义“更新”的判断方式为：**term 较大者更新；term 相同时 index 较大者更新**。

此外，Raft 还明确区分不同 term 的条目提交方式：

- **当前任期内**的条目，只要被复制到大多数服务器即可提交；
- **先前任期的**条目，即使也被大多数复制，也不能立即提交；
- 原因：旧领导者可能在 crash 前只部分复制这些旧条目，而新领导者可能将其覆盖。

详见图 8 中的示例，展示了错误提交旧任期日志所导致的回滚风险。因此 Raft 要求：**只有在当前任期内新增条目成功复制后，才能一起“顺带”提交旧条目**。 

必须 **还要检查 log[N].term == currentTerm**，否则你会把 **前任 Leader 的日志条目**错误地认为是已提交的，而实际上这些条目未来可能被覆盖！

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413092934961.png" alt="image-20250413092934961" style="zoom: 33%;" />

## 🧩 图中各个阶段详解

图中有五个服务器：S1–S5。日志条目的格式为 `[term]`，位置表示 log index。

### (a)

- S1 是 Leader，正在写入 index=2（term=2）的日志。
- 它仅部分复制到了 S2、S3。

### (b)

- S1 崩溃了。
- S5 被选为 term=3 的 Leader，它写入了 `[2]=3`（与 S1 的 `[2]=2` 冲突！）。

### (c)

- S5 崩溃了。
- S1 恢复并再次当选 Leader（term=4），继续把 `[2]=2` 复制到 S2、S3。

🧠 **此时 `[2]=2` 在多数节点上（S1, S2, S3），但不能认定它已提交！**

### (d)

- S1 又崩溃了。
- S5 再次当选 Leader，它的 `[2]=3` 会覆盖掉 S2、S3 的 `[2]=2`！

💥 原本在 term=2 写下的日志 `[2]=2` 被覆盖了。

**说明**：如果你让 term=2 的日志提交了，那么现在就出现了“提交的日志被覆盖”的灾难，❌ 违背线性一致性（Linearizability）。

------

### (e) 正确的做法

如果 S1 **在崩溃前**成功写入了 `[3]=4`（term=4），并且这条 log entry 被复制到了多数节点（S1, S2, S3），

那么：

- **这一条 `[3]=4` 来自当前 term（term=4）✅**
- 因此，**它可以被提交**。
- 一旦 `[3]=4` 提交，那么它前面的 `[2]=2` 也就连带被提交了（因为日志必须顺序提交）。

此时：

- S5 永远无法再次当选（因为任期落后，日志短），
- 所有节点最终日志一致，✅ 安全。

------

## ✅ 结论

这张图说明了 Raft 中一个关键规则：

> **Leader 只能提交其当前任期的日志条目，并且该条目已经被多数节点复制。**

只有这样，才能确保：

- 日志一旦被“提交”，就永远不会被改动。
- 日志提交是单调向前、不可撤销的。
- Linearizability（线性一致性）得以保证。

------

## 🔒 5.4.1 选举限制（Election Restriction）

### 📌 背景：

在某些共识算法中（如 Viewstamped Replication），**新 leader 可以没有完整的 committed 日志条目**，然后在当选后补齐。但这会引入额外的同步机制和复杂度。

### ✅ Raft 的做法：

Raft 通过 **“选举前日志检查”** 来解决这个问题，避免复杂的补日志机制。

### 🧠 核心思想：

> **候选人必须拥有所有已提交的日志条目，才能当选 leader。**

### 🚦 如何实现：

- `RequestVote` RPC 中包含候选人最后一条日志的 `(Term, Index)`。
- 如果投票者发现自己日志“更新”（即更长或更高 term），就**拒绝投票**。
- 这样能确保：若某个日志条目已被提交（即在多数节点中），则所有可能投票给新 leader 的节点中一定**至少有一个包含该条目**，而如果这个候选人的日志不足，它就无法当选。

### 📏 如何判断谁的日志更新：

Raft 使用如下逻辑：

1. **比较最后一条日志的 term**：term 更大的更更新；
2. **term 相同则比较 index**：index 更大的更更新。

------

## 🔐 5.4.2 提交旧任期日志的限制（Committing entries from previous terms）

### 🚫 问题来源（Figure 8）：

即使某条日志 entry 存在于多数节点，但只要它**不是当前 leader 任期**的，它就仍然**不能被视为 committed**。

👉 **为什么？** 因为它可能还会被未来的 leader 覆盖（见 Figure 8(c) → (d) 场景）。

------

### ✅ Raft 的做法：

> **只有当前任期的 leader 日志 entry 被复制到多数节点后，才允许 commit**。

一旦 leader 当前 term 的日志 entry 被提交，由于 **Log Matching Property**，其前面的所有 entry 也都可以安全视为 committed。

### 🧠 保守的设计优点：

- 简化了协议逻辑；
- 保证了一致性，不需要像其他算法一样在新 leader 上修改日志 term。

------

## 📐 5.4.3 安全性证明（Leader Completeness & State Machine Safety）

这部分是形式化证明 Raft 的安全性，重点是两个属性：

------

### ✅ Leader Completeness Property：

> 如果某条日志 entry 被 commit 于 term T 的 leader 上，那么之后所有 term > T 的 leader 也一定包含这条 entry。

**证明核心思路**：

1. leaderT 的 entry 已 commit，说明它在多数节点中；
2. leaderU（term U > T）能当选，说明也得到了多数节点的投票；
3. 那么至少有一个节点同时拥有 commit entry 且投了票 → 它一定要求 leaderU 的日志至少一样“更新”；
4. 若 leaderU 比它还更新 → 就必须包含这条 entry（因为 term 更高或 index 更长）；
5. 若没有包含 → 与投票者投票条件矛盾 → ❌。

于是得证：所有未来 leader 一定包含该 entry。

------

### ✅ State Machine Safety Property：

> 所有 server 在同一 index 上只能提交同一个日志 entry。

**为什么成立：**

- 任意被应用的 entry 一定是 committed；
- committed → 一定在 leader 的 log 中；
- leader completeness 保证未来 leader 也有这条；
- 所有 server 应用顺序相同 → 所有 server 应用同一条 entry。

------

## 🔁 总结：Raft 的提交与选举机制设计逻辑闭环



| 机制                         | 目的                        | 关键保障                  |
| ---------------------------- | --------------------------- | ------------------------- |
| **日志条目 term 保留不变**   | 明确谁写的，便于判断安全性  | Entry 不随 leader 改 term |
| **只能提交当前 term 的日志** | 防止老日志伪 commit         | Figure 8                  |
| **日志更新性检查才能投票**   | 保证候选人有 committed 日志 | Leader Completeness       |
| **Log Matching Property**    | 日志一致性前提              | 一致性复制                |



### **5.4.1 选举约束（Election Restriction）**

在任何基于领导者的共识算法中，新 leader 必须拥有所有已提交的日志项。其他算法（如 Viewstamped Replication）会在 leader 上任后查缺补漏，而 Raft **在选举期间就强制要求候选人必须拥有所有已提交的日志**，以避免增加额外机制与复杂度。

Raft 的方法：

- **只有日志“最新”的候选人才能赢得选举**；
- 所谓“最新”，是指：
  - 若最后一条日志的任期较新，则其日志更新；
  - 若任期相同，则日志更长者更“新”。

**RequestVote RPC** 中附带候选人最后一条日志的索引和任期，**voter 会拒绝日志落后的候选人**。这样，一旦某日志项被大多数节点接受（即已提交），候选人只有拥有该日志项，才有可能赢得选举。

#### **5.4.2 提交旧任期日志的限制（Committing Entries from Previous Terms）**

- 当前任期内的日志项：**leader 知道一旦在大多数节点上复制成功即为安全提交**；
- 旧任期日志：不能简单地依赖“复制到多数”来判断是否已提交。

原因示意见图 8：

1. term 2：S1 领导，但日志尚未完全复制；
2. term 3：S5 被选为 leader，写入新的不同日志项；
3. term 4：S1 恢复并重新当选 leader，继续复制其旧的 term 2 日志；
4. term 5：S5 再次当选 leader，并覆盖 S1 的 term 2 日志项 —— 说明该 term 2 日志并未真正“提交”。

**解决方案**：Raft 限制为：

> 只有当**当前任期的日志项**被复制到大多数节点时，才能认为这些日志项（及其前所有日志项）真正被提交。

这样就避免了旧任期日志项在领导者更换期间被覆盖的风险，确保状态机的一致性。



### 5.4.2 提交前一任期的日志条目

如第 5.3 节所述，**领导者知道只要当前任期的一个日志条目已经被大多数服务器复制，则该条目就被提交**。如果领导者在该条目提交前宕机，之后新的领导者将尝试完成该条目的复制。

然而，**对于前任期的日志条目，即使它已经被多数服务器保存，领导者也不能立刻认定该条目已经提交**。图 8 展示了一个示例，在该示例中，一个旧的日志条目虽然已经存在于多数服务器上，但之后仍有可能被后续领导者覆盖。

为了避免如图 8 所示的问题，Raft **从不依赖“多数副本”来判断旧任期的日志是否提交**。**只有当前任期内的日志条目**，Raft 才允许通过“多数复制”来认定其为已提交。一旦当前任期的某个条目被这样提交，那么根据**日志匹配性原则（Log Matching Property）**，之前所有的日志条目也就被间接地认定为已提交。

虽然在某些特殊情况下，领导者可以安全地认定某些旧条目（例如，它已存在于所有服务器上）已经提交，但**Raft 为了简化设计，采取了更保守的策略**。

Raft 在提交规则上额外的复杂性，是因为它保留了日志条目的原始任期号 —— 即便是新领导者在复制旧条目时，也不修改它的任期。在其他共识算法中，领导者复制旧条目时，往往会将其“重编号”为当前任期。Raft 的方式使得人们更容易理解和推理日志内容的时序与一致性，并减少了不必要的冗余条目复制。

------

### 5.4.3 安全性论证（Safety Argument）

在 Raft 算法设计完成后，我们可以更准确地证明 **领导者完整性（Leader Completeness）** 属性成立（该证明思路参考第 9.2 节的安全性证明）。我们采用反证法：

假设某个任期 T 的领导者（leaderT）提交了一个来自其本任期的日志条目，但在某个更高任期 U 中，新领导者（leaderU）却没有保存该条目。

根据假设，我们推导出如下矛盾：

1. leaderU 在其当选时的日志中没有该条目（因为领导者不会删除或覆盖日志）。
2. leaderT 将该日志条目复制到了集群的多数节点上，而 leaderU 也获得了多数节点的投票 —— 这意味着至少有一个服务器既保存了 leaderT 的条目，又给 leaderU 投了票（见图 9）。
3. 该服务器（称为“投票者”）在给 leaderU 投票前，必然已经接受了 leaderT 的日志条目。
4. 而且在它投票时仍保留了该条目，因为之前的所有领导者都包含此条目（根据假设），而领导者不会删除日志，追随者也只会在和领导者冲突时才删除。
5. 根据 Raft 的投票规则，leaderU 的日志必须比投票者的日志更新或至少一样新，否则就不会被投票。
6. 那么就有两种情况：
   - 若两者日志末尾的任期相同，则 leaderU 的日志必须**至少一样长或更长**，所以也包含了投票者的日志条目（矛盾）；
   - 若 leaderU 的末尾任期更新（大于 T），则之前那个创建该条目的领导者也一定有该日志条目，而根据 **日志匹配性原则**，leaderU 也应该有这个条目（矛盾）。

综上，矛盾成立，所以假设不成立。**即所有大于 T 的任期的领导者必须包含 T 任期已提交的条目。**

进一步，根据领导者完整性属性，我们可以推导出**状态机安全性（State Machine Safety）**：若某个服务器在某索引位置应用了某条日志条目，则之后任何服务器在该索引位置都必须应用相同的条目。

------

### 5.5 追随者和候选人宕机

到目前为止我们主要关注领导者崩溃的情况。相对来说，追随者和候选者宕机的处理要简单得多，且处理方式一致：

- 若某个追随者或候选者宕机，Raft 将不断尝试向其发送 RequestVote 和 AppendEntries 请求。
- 当它重新上线后，请求即可成功响应。
- 如果服务器在完成一次 RPC 后宕机，但还没来得及回应，也没关系 —— 重启后会重新收到该 RPC。
- Raft 的 RPC 是幂等的，例如：
  - 如果追随者收到了重复的 AppendEntries，其日志中已经包含这些条目，它会直接忽略重复部分。

------

### 5.6 时序与可用性（Timing and Availability）

Raft 的一个重要目标是：**安全性不能依赖时间** —— 系统不能因为某些事件发生得太快或太慢就产生错误。

但**可用性**却不可避免地依赖时间。比如：

- 若网络往返时间比服务器的宕机频率还高，那么候选者可能还没选上就宕机，导致系统无法推进。

Raft 能选出领导者并保持其稳定性的前提是：

```
nginx


复制编辑
broadcastTime ≪ electionTimeout ≪ MTBF
```

解释如下：

- `broadcastTime`：一次 RPC 广播（并行向所有节点发出请求并收到回复）的平均时间。
- `electionTimeout`：候选人等待领导者心跳超时时间（如 150ms–300ms）。
- `MTBF`：服务器平均故障间隔时间（Mean Time Between Failures）。

选举超时时间需要比广播时间大一个数量级，以确保领导者可以稳定发送心跳，避免不必要的选举。

而选举超时时间应远小于服务器的 MTBF，以确保系统能长期稳定运行，宕机恢复后能及时完成领导者选举。

------

### 图 10：配置变更中的安全风险

图 10 展示了为什么不能直接从旧配置切换到新配置：

- 假如集群从 3 台服务器扩展到 5 台，若节点切换时间不同步，可能在同一个任期内出现两个领导者，一个获得旧配置多数，一个获得新配置多数，导致**双领导者问题**。

为确保安全，Raft 采用“两阶段配置变更机制”：

1. **第一阶段**：切换到“联合共识（Joint Consensus）”配置，集群同时包含旧配置和新配置的所有节点。
2. **第二阶段**：完成联合共识提交后，正式切换到新配置。

在联合共识阶段：

- 所有日志条目都要同时复制到旧配置和新配置的服务器；
- 必须获得两者的多数确认，确保无缝过渡且不会出现双领导者。

<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250412235244138.png" alt="image-20250412235244138" style="zoom: 33%;" />



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413072323565.png" alt="image-20250413072323565" style="zoom: 25%;" />



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413084349013.png" alt="image-20250413084349013" style="zoom: 33%;" />









<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413093127875.png" alt="image-20250413093127875" style="zoom: 33%;" />

### 配置变更中的联合共识机制（Joint Consensus）

任何来自旧配置或新配置的服务器都可以成为领导者。

- **选举和日志提交时**，需要在“旧配置”和“新配置”中分别获得多数同意。

联合共识允许服务器在不同时间切换配置，同时不破坏一致性安全性。此外，在配置变更过程中，Raft 集群仍可以继续处理客户端请求。

集群配置以特殊的日志条目形式存储和传播。如下图（Figure 11）所示，假设当前配置为 `Cold`，领导者收到一个请求，希望将配置变更为 `Cnew`，此时领导者会将 `Cold,new`（联合配置）写入日志，并通过 AppendEntries RPC 进行复制。一旦某个服务器接收到该配置条目，它就会在今后的所有决策中使用该配置（无论该配置是否已提交，服务器都使用日志中最新的配置）。

> 这意味着，领导者将使用 `Cold,new` 的规则来判断该配置是否已经提交。如果领导者宕机，新领导者可能是基于 `Cold` 或 `Cold,new` 当选的（取决于它是否已接收到该配置条目）。但在此期间，`Cnew` 无法单独做决定。

一旦 `Cold,new` 被提交，无论 `Cold` 还是 `Cnew` 都必须获得彼此的多数同意才能作出决定。Leader Completeness Property 保证：只有包含 `Cold,new` 配置的服务器才能当选为领导者。此时，领导者可以创建一个新的日志条目 `Cnew`，将其广播复制。一旦 `Cnew` 条目根据新规则被提交，旧配置就不再重要，旧服务器可被安全移除。

整个过程中，始终不会存在旧配置和新配置可以**单独做决定**的时间段，从而保证一致性安全。

------

### 其他配置变更相关问题

#### 1. 新加入服务器没有任何日志条目：

新加入的服务器如果空白，可能在很长时间内无法赶上主流日志，这将阻止日志提交。为避免这种可用性问题，Raft 提出在变更配置前先引入 **非投票成员**（non-voting member）机制：新服务器首先加入集群但不计入多数投票，等它日志追上后，再执行正式的配置变更。

#### 2. 当前领导者不在新配置中：

如果当前领导者不包含在新配置中，一旦 `Cnew` 被提交，它应主动退位为 follower。也就是说，在提交 `Cnew` 过程中，领导者可能正在管理一个自己并不属于的集群。在这种情况下，领导者不再参与多数投票，只负责复制日志。等 `Cnew` 被正式提交，集群就能选出新的合法领导者。

#### 3. 被移除的旧服务器可能干扰新集群：

被剔除的旧服务器将不再收到心跳，因而会误认为领导者宕机并发起选举，造成新领导者不断被替换，从而影响集群稳定性。为避免这种情况，Raft 要求服务器**在心跳间隔内仍感知到当前领导者时，不响应选票请求（RequestVote RPC）**。这不会影响正常的领导人选举，但可有效避免旧节点误导选举的行为。

------

### 日志压缩机制（Snapshot）

随着日志持续增长，它将消耗越来越多空间，恢复重放的时间也会变长，影响系统可用性。Raft 采用 **快照（snapshot）机制** 来清理无用日志。

#### 快照的基本原理：

- 每个服务器独立创建快照，快照内容包括：
  - 当前状态机的完整状态；
  - `lastIncludedIndex`：被快照替代的最后一个日志条目的索引；
  - `lastIncludedTerm`：对应的任期；
  - 最新的集群配置。
- 创建快照后，可安全删除 `lastIncludedIndex` 及其之前的日志条目。

#### 快照传输机制：

某些情况下（如追赶严重滞后的 follower 或加入新节点），leader 的日志已删除了 follower 所需的日志条目，这时会使用 `InstallSnapshot` RPC 发送完整快照。
 收到快照的 follower：

- 如果快照包含的新数据不在当前日志中，它会删除整个日志；
- 如果是日志前缀的重复快照，仅删除与快照重叠的旧部分，保留后续日志。

此机制虽然允许 follower 在不通知 leader 的情况下创建快照，略偏离了 Raft 的“强领导者”理念，但由于快照仅基于**已提交的数据**，不会破坏一致性，也无须多余协调。



## 🔧 场景背景：为什么需要 InstallSnapshot RPC？

Raft 使用日志（log）作为复制状态的核心方式，但长期运行会导致日志无限增长，占用大量存储空间，并拉长启动/恢复时间。

为了解决这个问题，**Raft 引入了快照机制（Snapshotting）来进行日志压缩**。

但这里就产生了一个**关键问题**：

> 如果某个 follower 落后太多，leader 早就把他需要的日志删掉了，那还怎么补齐？

这就是 `InstallSnapshot RPC` 出场的时机。

------

## 📦 InstallSnapshot RPC 的使用时机

当 **follower 丢失了 leader 所持有的日志前缀**，leader 就无法用普通的 `AppendEntries` 同步数据。这时，leader 会：

1. **读取自己最新的快照文件**；
2. **通过 InstallSnapshot RPC 把这个快照整体发给 follower**；
3. follower 收到后覆盖掉自身旧日志，使其“跳跃式”赶上 leader 的状态。

这种机制特别适用于以下几种情况：

- follower 长时间离线；
- follower 刚新加入；
- follower 磁盘被清理、日志丢失。

------

## 📍 follower 收到 InstallSnapshot 的两种情况

### ✅ 情况一：快照数据不在 follower 日志中（最常见）

也就是说，**快照是完全更新的内容**，follower 当前的日志已经和 leader 快照严重偏离或丢失。

**处理流程：**

- follower **删除整个旧日志**；
- 使用快照更新状态机；
- 把 `lastIncludedIndex` 和 `lastIncludedTerm` 设置为快照里的值；
- 未来 leader 会从 `lastIncludedIndex+1` 开始用 AppendEntries 补充日志。

这种方式恢复速度快，逻辑干净。

------

### 🔁 情况二：快照是 follower 已经拥有的日志前缀（如：重复发送或重传）

这可能是因为网络重传、leader状态回滚、RPC失败等导致的重复安装快照。

**处理流程：**

- follower **检查日志是否已经包含了快照的最后条目**；
- 如果有，那就只删掉那些和快照重复的部分（冗余日志）；
- 保留快照之后的条目（例如 leader 已经发过的新日志条目）；
- 然后继续接收 AppendEntries。

这样能最大限度避免无谓的数据丢弃与复制。

------

## 🚦 与“强领导者原则”是否冲突？

Raft 强调“**强领导者**”（strong leader）：数据只能由 leader 发出，其他节点不主动变更数据。

而这里似乎有个“例外”——follower 可以在 **不通知 leader 的情况下自己创建快照**（比如每 10 分钟压缩一次日志）。

是否破坏一致性？

**不会的！原因如下：**

- follower 只对“**已提交的日志**”做快照，而这些日志已经被大多数节点确认，不存在不一致；
- 快照只是一种存储优化，本质上并不改变日志的顺序或内容；
- 日志应用的一致性仍由 AppendEntries 保证，快照只是其“物理表现形式”；
- 未来如果 leader 还要重发某个老的 AppendEntries，follower 会比对 `prevLogIndex` 和 `prevLogTerm`，如已被快照覆盖，会直接拒绝并要求 InstallSnapshot。

因此：**虽然 follower 是“主动快照”，但不脱离 leader 掌控，也不影响一致性。**





假设某个 Raft 状态机是一个 key-value 存储，在 `index = 10` 时状态是：

```
json


复制编辑
{
  "x": 5,
  "y": 12,
  "z": -1
}
```

并且 `lastIncludedIndex = 10`，`lastIncludedTerm = 4`，则快照内容就可以表示为：

```
json


复制编辑
{
  "state": {"x":5, "y":12, "z":-1},
  "lastIncludedIndex": 10,
  "lastIncludedTerm": 4,
  "config": {...}  // 可选字段，表示集群成员
}
```

------

## 📤 二、快照是怎么生成的（快照创建）

当节点发现日志长度变长、提交状态机慢时，会主动触发快照。流程如下：

1. **StateMachine 创建快照**：将当前状态写入磁盘；
2. **Raft 层保存元数据**：记录当前 snapshot 所对应的 `lastIncludedIndex` 和 `lastIncludedTerm`；
3. **清除旧日志**：丢弃 log 中索引 ≤ `lastIncludedIndex` 的日志；
4. **生成 InstallSnapshot RPC**（给 follower 用）；



### 日志条目结构（Log Entry）

```
go


复制编辑
type LogEntry struct {
    Term    int         // 当前日志条目由哪个 leader 任期创建
    Index   int         // 当前日志条目的索引
    Command interface{} // 客户端请求的操作（比如 "set x = 3"）
}
```

------

### 📘 举个例子：假设我们正在实现一个 key-value 状态机

以下是日志条目列表的一种可能长相（简化视图）：

| Index | Term | Command     |
| ----- | ---- | ----------- |
| 1     | 1    | `set x = 1` |
| 2     | 1    | `set y = 2` |
| 3     | 2    | `set x = 5` |
| 4     | 2    | `delete y`  |
| 5     | 3    | `set z = 9` |
| 6     | 3    | `set x = 7` |

这些操作顺序地送给状态机执行，最终系统状态可能是：

```
json


复制编辑
{
  "x": 7,
  "z": 9
}
```

------

### ✅ 日志条目的作用：

1. **复制：** Leader 会将日志条目复制给 Follower；
2. **一致性：** 保证所有服务器最终拥有相同顺序的命令；
3. **重放/恢复：** 服务器崩溃重启后，可重放日志恢复状态；
4. **提交控制：** 只有提交的日志才能被应用到状态机。

------

### 💡 其他字段（扩展用法）：

如果你实现分布式数据库或需要支持 snapshotting，还可能有：

```
go


复制编辑
type LogEntry struct {
    Term       int
    Index      int
    Command    interface{}
    IsSnapshot bool   // 是否为快照触发标志（特殊用途）
}
```

------

### 



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413130036765.png" alt="image-20250413130036765" style="zoom:50%;" />





在 Raft 中，**每个服务器（Server）都可以独立创建自己的 Snapshot（快照）**，但不同角色在不同场景下执行快照的方式略有差异：

------

### ✅ 一、**Follower 和 Candidate：自行创建快照（最常见）**

- **触发时机**：当本地日志累积过多（如达到某个大小阈值），Follower 会根据自己的策略生成快照；
- **内容**：包含本地状态机已应用的最后一个 committed 日志条目的状态、`lastIncludedIndex`、`lastIncludedTerm` 以及配置状态；
- **影响**：生成快照后，可以删除对应日志前缀，减少存储和加快重启恢复时间；
- **无需 Leader 协调**：这是 Raft 在保证一致性的前提下做出的灵活设计（尽管略偏离 strong leader 模型）。

------

### ✅ 二、**Leader：在特殊情况下向滞后 Follower 发送 Snapshot**

- **场景**：某个 Follower 落后太多，Leader 的日志中已经没有它需要的那部分日志；
- **处理方式**：Leader 会使用 `InstallSnapshot` RPC 主动发送一个完整快照给该 Follower；
- **注意**：Leader 不会主动为自己创建快照，但它会从日志中读取快照数据并网络发送。

------

### 🔄 三、快照传播流程示意

```
text


复制编辑
Client -> Leader（提交请求）
        |
        v
[日志条目已复制并提交]
        |
        v
各 Server 根据需要自行创建 Snapshot（状态机层）
        |
        v
若有 Follower 严重落后：
Leader -> Follower 发 InstallSnapshot RPC
```

------

### ✅ 结论总结

| 角色          | 是否主动创建 Snapshot | 是否发送 Snapshot | 场景说明                   |
| ------------- | --------------------- | ----------------- | -------------------------- |
| **Follower**  | ✅ 是                  | ❌ 否              | 正常日志压缩、自身落后太多 |
| **Candidate** | ✅ 是                  | ❌ 否              | 类似 Follower              |
| **Leader**    | ❌ 否（本地不创建）    | ✅ 是              | Follower 严重滞后时发送    |





以下是你提供的这段关于 **Raft 快照机制（Snapshotting）** 的完整中文翻译：

------

快照（snapshot）机制也被 Chubby 和 ZooKeeper 采用，其基本思路是：在某个时间点将整个系统状态保存为快照，并丢弃此前所有日志。接下来的内容将介绍 Raft 中的快照机制。

相比之下，日志清理（log cleaning）[36] 和 LSM 树（log-structured merge tree）[30, 5] 等增量压缩方法也是可行的。它们每次只处理数据的一小部分，从而更均衡地分摊压缩所需的资源。这类方法会首先选择一段包含大量已删除或被覆盖对象的数据区，然后将其中仍然有效的对象以更紧凑的方式重写，并释放原区域。但与快照相比，这类方法机制更复杂，实现成本更高。而快照始终作用于整个数据集，逻辑更简单。如果采用日志清理，将需要对 Raft 协议进行修改；但如果状态机自身采用 LSM 树实现，其接口与快照类似，也可直接支持。

图 12 展示了 Raft 中快照的基本过程。每个服务器会**独立地创建快照**，覆盖其日志中所有已提交（committed）的条目。快照的大部分工作由状态机完成：将当前状态写入稳定存储。Raft 同时还在快照中保留一些元数据（metadata）：

- `lastIncludedIndex`：快照所替代的日志的最后一条记录的索引（即状态机应用的最后一条日志）；
- `lastIncludedTerm`：该条日志所在的任期（term）。

这些元数据用于在快照之后接收新日志条目时执行一致性检查（AppendEntries）。此外，为了支持集群配置变更（第 6 节），快照还会记录在 `lastIncludedIndex` 处的最新集群配置信息。一旦快照写入完成，服务器可删除从头到 `lastIncludedIndex` 的所有日志条目，以及旧快照。

尽管服务器通常是**各自独立地创建快照**，但有时 Leader 必须将快照发送给滞后严重的 Follower。当 Follower 所需的日志条目已被 Leader 删除时，就需要通过网络将完整快照发送过去。

Leader 使用一种新的 RPC，称为 `InstallSnapshot`，将快照发送给滞后太多的 Follower（详见图 13）。Follower 收到快照后需要判断该如何处理自身已有的日志条目：

- 如果快照中包含了当前日志中未曾出现的新数据，则说明其内容已取代本地日志。此时，Follower 会**丢弃全部日志**，避免快照与日志中潜在的未提交记录冲突。
- 如果收到的是本地日志前缀的重复快照（可能由于重复传输或误发），则仅会**删除快照所覆盖的前缀部分**，而保留其后的有效日志条目。

这种快照机制虽然**稍微偏离了 Raft 的“强领导者”（strong leader）模型**——因为 Follower 可以在不通知 Leader 的情况下自行快照——但这是合理的设计。原因在于：

- 快照操作只基于已提交（committed）数据，**不会引入任何一致性问题**；
- 共识已达成，无需再协调；
- 实际上，数据仍然是**从 Leader 向 Follower 单向流动**，只是 Follower 现在可以独立整理自己的数据了。

我们曾考虑另一种“Leader 驱动”的快照方法：由 Leader 创建快照并发送给所有 Follower。但这种方式有两个明显的缺点：

1. 会造成带宽浪费和快照过程延迟。Follower 本地已经拥有生成快照所需的全部信息，**自己生成快照比从网络接收更高效**；
2. 会使 Leader 实现更复杂：Leader 需要在发送快照的同时继续复制新的日志条目，避免阻塞客户端请求。

快照机制中还有两个影响性能的重要问题：

------

### 一、何时创建快照？

如果快照太频繁，浪费磁盘资源和能耗；如果太少，会占用大量存储并使重放日志变慢。一种简单策略是：**当日志大小达到某个固定字节数时进行快照**。只要该阈值大于典型快照大小，则磁盘带宽消耗仍可接受。

------

### 二、如何避免快照过程阻塞正常操作？

写快照可能花费较长时间，**但我们不希望因此阻塞接收新请求**。解决方案是使用“写时复制”（copy-on-write）机制，即在快照写入期间仍可继续接收并处理新操作。例如：

- 使用函数式数据结构的状态机天然支持 copy-on-write；
- 或者借助操作系统的写时复制功能（如 Linux 中的 `fork`），对状态机创建一个内存快照，我们的实现采用的就是这种方式。

------

如需，我可以补充图示说明 snapshot 的结构、流程图或 InstallSnapshot 的时序图。是否需要？





<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413145520094.png" alt="image-20250413145520094" style="zoom:50%;" />

## 🛠️ Receiver Implementation 接收端执行流程详解：

下面是 Follower 收到 `InstallSnapshot RPC` 后的处理逻辑：

| 步骤 | 说明                                                         |
| ---- | ------------------------------------------------------------ |
| 1️⃣    | 如果 `term` 小于当前任期，立即返回（说明 Leader 已过期）     |
| 2️⃣    | 如果是第一个块（`offset=0`），新建一个快照文件               |
| 3️⃣    | 将 `data[]` 写入快照文件的指定 offset 位置                   |
| 4️⃣    | 如果 `done=false`（说明快照还没传完），等待下一块数据        |
| 5️⃣    | 如果快照接收完毕： • 保存完整快照文件 • 丢弃旧快照（如果 `lastIncludedIndex` 更小） |
| 6️⃣    | 若本地日志中存在 **与快照相同的 index 和 term**： • 保留该条目之后的所有日志 • 返回 |
| 7️⃣    | 否则：清空整个日志（意味着快照是完全覆盖性的）               |
| 8️⃣    | 使用快照内容重置状态机，并加载快照中的集群配置               |



### 第 8 章 客户端交互

本章介绍了客户端如何与 Raft 集群交互，包括客户端如何寻找集群的领导者，以及 Raft 如何支持线性一致性语义 [10]。这些问题适用于所有基于共识的系统，Raft 的解决方案与其他系统相似。

Raft 的客户端将所有请求发送给当前的领导者。当客户端首次启动时，会连接到随机选择的一台服务器。如果该服务器不是领导者，它会拒绝客户端的请求，并返回它最近听说的领导者的信息（`AppendEntries` 请求中包含了领导者的网络地址）。如果领导者崩溃，客户端的请求将超时；此时客户端会再次随机选择一台服务器尝试发送请求。

我们的目标是实现线性一致性语义（即每个操作看起来都在其调用和响应之间的某个时间点被原子执行了一次且仅执行一次）。但如前所述，Raft 可能会多次执行同一个命令：例如，领导者提交了日志条目但在响应客户端前崩溃，客户端会将命令重试发送给新的领导者，导致命令被再次执行。为了解决这个问题，客户端为每个命令分配唯一的序列号。状态机会记录每个客户端已处理的最新序列号及其响应结果。如果再次收到具有相同序列号的命令，状态机会立即返回先前的响应，而不会重新执行该命令。

只读操作可以不写入日志即可完成。但如果不加控制，可能会返回陈旧数据，因为返回响应的领导者可能已被更高任期的领导者取代。为了保证线性一致性地读取数据，Raft 增加了两项保护机制：

1. **确保领导者已知最新的提交状态**。领导者一旦当选，可能并不知道哪些日志条目已经被提交。Leader Completeness Property 保证领导者拥有所有已提交的日志条目，但前提是它已提交自己任期内的一个条目。因此 Raft 要求每个新领导者在任期开始时写入一条空（no-op）日志条目以触发提交过程。
2. **确保领导者在处理只读请求之前仍然是当前合法领导者**。Raft 要求领导者在处理只读请求前先与多数派交换心跳消息确认其仍然是当前领导者。另一种做法是通过心跳形成租约（lease）机制 [9]，但这依赖时间保证，因此会牺牲安全性。

------

### 第 9 章 实现与评估

我们已经将 Raft 实现为 RAMCloud [33] 系统中一个用于存储配置信息和协调器故障切换的复制状态机组件。该 Raft 实现约有 2000 行 C++ 代码（不含测试、注释和空行），源代码是开源的 [23]。此外，已有约 25 个第三方的开源 Raft 实现处于不同开发阶段 [34]，也有多个公司部署了基于 Raft 的系统 [34]。

本章从三方面对 Raft 进行评估：**可理解性、正确性和性能**。

------

#### 9.1 可理解性

为了比较 Raft 与 Paxos 的可理解性，我们在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程中进行了一项用户实验。我们分别为 Raft 和 Paxos 制作了讲解视频和对应测验。Raft 的视频涵盖本论文除日志压缩以外的内容，Paxos 的视频包含了构建复制状态机所需的内容，包括单值 Paxos、多值 Paxos、动态配置以及若干实践中的优化（如领导者选举）。

每位参与者观看一个视频并完成相应测验，然后再观看另一个视频并测验。一半参与者先看 Paxos，另一半先看 Raft，以消除个体差异和顺序偏差。

实验结果显示：**参与者在 Raft 测验中的平均分比 Paxos 高 4.9 分**（满分 60，Raft 平均 25.7 分，Paxos 平均 20.8 分），如图 14 所示。配对 t 检验显示：Raft 测验的均值至少比 Paxos 高 2.5 分，置信度为 95%。

线性回归模型进一步表明：对于一个没有先验知识的新学生，Raft 测验的预测得分比 Paxos 高 12.5 分。值得注意的是，该模型还预测：如果学生先做了 Paxos 测验，其 Raft 得分会降低 6.3 分，尽管原因尚不明确，但具有统计显著性。

此外，我们还调查了参与者对实现与讲解难度的主观判断：**大多数参与者认为 Raft 更容易实现和讲解**（41 人中有 33 人选择 Raft）。

------

#### 9.2 正确性

我们使用 TLA+ [17] 形式化语言为 Raft 的一致性机制编写了形式化规范（约 400 行），并通过 TLA+ 证明系统机械化地验证了 Log Completeness Property。此外，我们还撰写了非形式化的 State Machine Safety Property 证明，约 3500 字，完整地基于该规范而来，表达较为精确。

------

#### 9.3 性能

Raft 的性能与其他共识算法（如 Paxos）相当。最关键的性能场景是：已有领导者持续追加日志条目时。Raft 能以最少的消息数完成复制（一次往返 RPC 即可），并支持批量处理与流水线优化。

我们用自己的 Raft 实现测试了领导者选举机制，主要回答两个问题：

1. **选举是否收敛快速？**
2. **领导者崩溃后的最小不可用时间是多少？**

测试方法：我们在五节点集群中不断使领导者崩溃并测量选出新领导者的时间。每轮试验中，服务器日志长度不同，模拟部分候选人无法当选的情况；我们还让旧领导者在崩溃前广播心跳（模拟崩溃前追加日志），使分票更可能发生。

结果如图 16 所示：

- 若不使用随机超时，因频繁分票，选举平均耗时 >10 秒；
- 加入 5ms 随机性后，**中位恢复时间为 287ms**；
- 加入 50ms 随机性后，**最差情况为 513ms（1000 次试验）**。

另外，我们尝试通过降低选举超时提高恢复速度：

- 将选举超时降至 12–24ms，**平均只需 35ms 即可选出新领导者**；
- 但继续降低超时会违反 Raft 的时序要求，导致频繁重新选举，系统可用性下降。

**建议使用保守的选举超时值（如 150–300ms）**，在保持良好可用性的同时，也能快速恢复





客户端为每个命令分配唯一的序列号。状态机会记录每个客户端已处理的最新序列号及其响应结果。如果再次收到具有相同序列号的命令，状态机会立即返回先前的响应，而不会重新执行该命令 请问 leader执行命令前崩溃了 怎么办？



###### ChatGPT 说：

这是 Raft 为了支持 **线性一致性（linearizability）** 而采用的一个关键机制。你问得很好：**如果 Leader 在执行命令之前就崩溃了，会发生什么？**

我们可以分两种情况讨论 **Leader 崩溃的位置**：

------

### ✅ **情况一：Leader 已提交命令，但崩溃前尚未响应客户端**

这种情况是 Raft 为防止“重复执行”专门处理的情形。处理流程如下：

1. 客户端发送命令给 Leader，Leader 将其作为日志条目追加到自己的日志中。
2. Leader 通过 `AppendEntries` 将该条目复制到多数节点（被认为已提交），但 **在它回复客户端前发生了崩溃**。
3. 客户端等待超时后，转向其他服务器尝试重发命令。
4. 新 Leader 当选后，**客户端将旧命令（带同样的序列号）重新提交**。
5. 新 Leader 会将该命令再次写入日志，但因为状态机上有一项记录：**“这个客户端最新处理过的序列号”**，状态机会发现这个序列号已经处理过，就**不会重复执行**，而是直接返回上次执行的结果。

🔁 **效果：命令只执行一次，客户端得到正确响应。**

------

### ❌ **情况二：Leader 崩溃时命令尚未提交到多数派**

1. 客户端命令到达 Leader，Leader 将其追加到日志，但还**没能复制给大多数 follower（日志未提交）**。
2. Leader 崩溃。
3. 客户端超时后，向新 Leader 重发命令（同样带唯一序列号）。
4. **新 Leader 没有这个日志条目**，所以会把客户端命令重新作为一条新的日志写入，并尝试复制。
5. 命令正常被提交并执行。

🔁 **状态机还是只会执行一次**，因为是新 Leader 执行的，新 Leader 也会记录下这个客户端的序列号。

------

### 💡 关键机制总结：

- 每个客户端在发起请求时带上一个唯一的 **`clientId + serialNumber`**。
- 状态机会记录最近一个已执行的 `(clientId, serialNumber)` 和对应的响应结果。
- 即使客户端因网络或崩溃重试相同请求，状态机也能**去重并返回旧响应**，避免多次执行。

------

### 📌 所以回答你的问题：

> **如果 Leader 执行命令前崩溃了怎么办？**

- 如果命令已经提交：新 Leader 不会重复执行命令，只会返回旧结果；
- 如果命令尚未提交：新 Leader 会按原样重新提交和执行；
- 无论哪种情况，**客户端的命令最终只会执行一次**，满足线性一致性要求。



你提到的这段内容非常关键，它讲的是 **Raft 在处理只读请求（read-only operations）时如何保持线性一致性（Linearizability）** 的两项核心机制。下面我来详细分解解释：

------

## 🧠 为什么“只读”也不能随意处理？

Raft 中的大多数命令是写操作，会被追加到日志中，再由状态机顺序执行。但有些操作是**只读请求**，例如读取配置、查询数据等。

这些操作**理论上不需要写入日志**，但问题是：

> ❗ 你无法信任当前的“领导者”一定是真的。

比如：

- 你是客户端，向当前 Leader 发起一个只读请求。
- 但实际上此时网络发生了分区，或者别的服务器已选出新 Leader。
- 原来的“Leader”并不知道自己已经失去了领导权。
- 如果它直接读取自己的状态并返回数据，结果可能是过期的。

> ✅ **Raft 保证线性一致性** —— 也就是说，所有请求都要像是**顺序地**、**瞬时地**执行在一个统一副本上一样。

因此，只读请求 **不能盲目执行**，Raft 必须确保两个前提：

------

## ✅ 机制一：确保 Leader 拥有最新的提交记录

### 🔍 问题：

- Raft 的 **Leader Completeness Property** 保证：

  > 如果某条日志条目在某个任期被提交了，那它一定会存在于所有之后当选的 Leader 的日志中。

但是这个“拥有”，仅仅是**物理存在日志里**；

> 它并不能保证**Leader 知道这些日志是否已被提交**！

### 🛠️ Raft 的做法：

> **新 Leader 一上任，就追加一条空日志（no-op entry）以触发提交流程**。

这条空日志的作用：

- 确保至少有一个该任期的日志条目能被复制到多数派；

- 一旦这条空日志被提交，说明 Leader 现在知道：

  > “我在当前任期内的日志条目可以被多数派接受，也就是说，我可以确认之前的提交状态。”

这样，Leader 就能安全地知道哪些日志被提交了。

------

## ✅ 机制二：确保 Leader 仍然有效（没有被取代）

### 🔍 问题：

- Raft 的 Leader 是通过选举得来的。
- 网络分区或失联时，一个 Leader **可能已经不是 Leader 了**，但它自己并不知道。
- 你如果让它直接处理只读请求，就有可能读出“过时世界”的状态。

### 🛠️ Raft 的做法：

> **Leader 在响应只读请求前，要与多数派交换一次心跳，确认自己仍然是 Leader。**

即：

- 发送一次空的 `AppendEntries` 心跳（可以 piggyback 多个只读请求）；
- 收到多数派回应后，才处理只读请求；
- 否则就暂缓。

这个行为等价于：

> “我确保大多数节点都还承认我是 Leader，所以我有资格读出当前状态并返回。”

------

## ❓那“租约机制（lease）”是怎么回事？

另一种优化是：

> 假设 Leader 在每隔一段时间内都能成功发心跳，就可以视作它在“这段时间内”拥有 Leader 的租约（lease）。

即：

- 你不用每次读都发心跳；
- 只要 Leader 的“租约”还没到期，就可以直接响应只读请求。

⚠️ **但问题在于：**

- 这种做法**依赖于时钟同步**和网络延迟有上界；
- 否则如果某个节点时钟漂移严重，它可能错误认为自己还在租约期间，结果就读出了老数据。

所以 Raft 作者认为：

> **使用 heartbeat 确认领导者身份更加安全，虽然稍微慢一点，但不会破坏一致性。**



## 🧠 问题回顾：

假设：

- Leader A 在 **任期 T** 中提交了一条日志；
- 然后 A 崩溃了；
- Follower B 还没来得及执行这条日志；
- 接着，B 成为下一任期 **U > T** 的新 Leader；
- ❓ 那 B 是不是可能 **没有 A 提交的那条日志**？

如果是这样，那就违背了 Raft 的一致性要求，怎么办？

------

## ✅ Raft 如何避免这种情况？Leader Completeness Property！

Raft 明确设计了一个规则来**禁止这种事发生**，这就是：

> **一个服务器只有在其日志包含所有已提交条目的前提下，才可能赢得选举，成为新 Leader。**

这依靠 **RequestVote RPC 的第二条限制条件** 实现的：

### 📜 投票规则回顾（见 Raft 论文 §5.2）：

当某个 candidate 请求投票时，Follower 会检查：

```
text


复制编辑
如果该 candidate 的日志不如我“新”，我不会投票给它。
```

#### 什么叫“新”？

Raft 定义了一个明确的比较方式：

- 如果 candidate 的 lastLogTerm > 当前 Follower 的 lastLogTerm → 更新；
- 如果 term 相同，比较 lastLogIndex → 更长的更新；
- 否则 → 不如我新。

因此：

> 一个 candidate 想赢得多数派的票，它的日志必须比大多数节点都要新！

------

## 🔒 这条规则怎么保证日志安全？

让我们回到你的问题情境：

- A（Leader, term T）提交了一条日志 `log[5]`
- 提交意味着：这条日志被**复制到了多数派节点上**
- 假设 A 挂了，B 成为了候选人

### ❌ 如果 B 没有 log[5]，会发生什么？

- 当 B 向其他节点（比如 C、D、E）请求投票时
- 这些节点中至少有一半拥有 `log[5]`（因为是已提交的！）
- 他们会比较 B 的日志，发现 B 的 lastLogTerm 比自己旧，拒绝投票
- 所以 **B 无法成为 Leader**

------

## ✅ 那 B 什么时候可以成为 Leader？

只有当：

- B **自己也包含了 log[5]**
- 并且 **它的日志是整个集群中最“新”的**

此时，它才有可能赢得多数票，成为 Leader。

这就实现了我们所说的 **Leader Completeness Property：**

> **如果某条日志已经在某个任期被提交，那么之后所有能当选的 Leader 都必须包含这条日志。**

------

## 🧩 再补充一个图示帮助理解：

```
css


复制编辑
任期 T：
Leader A 提交 log[5] （复制到了 A、C、D）

任期 T+1：
B 想当选，需要获得 C、D、E 的投票
但 C、D 有 log[5]，B 没有 → 拒绝投票 → B 落选

任期 T+2：
假如 B 也同步到了 log[5]，并比其他人日志更新 → 才能当选
```



## 🧩 前提：Raft 的**机制二**是为了支持「**线性一致的只读请求**」

也就是说：

> Raft 需要在处理只读请求时，**确保自己仍是当前合法 Leader**，以防返回陈旧（stale）数据。

------

## 🧠 你的疑问：

> 新 Leader 当选时不就追加了一条 no-op 日志条目了吗？这不就是已经证明了我是 Leader，并且日志也被复制了吗？为什么还要再加一次「与多数派确认 Leader 身份」的心跳（AppendEntries）步骤？

是的，你的理解是对的，**no-op entry 是为了机制一（已知最新的提交状态）**，但这还不够保证机制二！

我们来解释为什么这两者不同：

------

## ✅ 机制一 vs 机制二 的区别：

| 目的                                                     | 机制                                 | 行为                                                         |
| -------------------------------------------------------- | ------------------------------------ | ------------------------------------------------------------ |
| 确保新 Leader 知道哪些日志已提交（commit index）         | **机制一：no-op entry**              | 新 Leader 上任后追加一条空日志，并将其复制到多数派，以确认自己是最新的 |
| 确保自己**仍是合法 Leader**（防止出现更高任期新 Leader） | **机制二：和多数派交换心跳确认身份** | 每次只读请求处理前，向多数派发送心跳并获得响应，确保没人取代自己 |

------

## ❗ 为什么机制二是必须的？

想象这个场景：

1. **Leader A 在 term 5 当选，并追加了一条 no-op 日志，成功提交。**
2. **然后网络出现临时分区，Leader A 被隔离了（但它自己还不知道），集群其他机器在 term 6 推选了新的 Leader B。**
3. 这时 A 收到一个客户端只读请求。
4. ❗ 如果没有机制二，A 可能会直接根据自己保存的状态回复客户端。
5. 🧨 但实际上，它已经不是 Leader，返回的可能是 **陈旧状态**，破坏了线性一致性！

------

## 🔐 心跳机制的作用

为了避免上述问题，Raft 要求：

- **Leader 每次处理只读请求前，必须先通过一次心跳确认「自己还活着」且「任期未被别人提升」**；
- 心跳实际上就是一次 `AppendEntries` RPC（可以没有新日志），当**多数派回复并承认当前 term 时**，Leader 才能放心处理只读请求。

------

## 🧾 所以这两个机制各自负责不同的安全目标：

| 机制                      | 对应问题                                  | 实现方式                                     | 对应 Raft 安全性             |
| ------------------------- | ----------------------------------------- | -------------------------------------------- | ---------------------------- |
| 🧱 机制一：提交 no-op 日志 | 新 Leader 不知道自己有哪些日志是已提交的  | 当选时立即追加空日志，推动 commit index 前进 | **Leader Completeness 保证** |
| 🧠 机制二：确认身份不变    | Leader 不知道自己是否已经被新 Leader 取代 | 每次只读请求前，和多数派确认 term 未变       | **线性一致读保证**           |



**为什么一个新当选的 Leader，在刚刚当选时，不知道哪些日志是“已提交”的？**

------

## 🧠 原因总结：

> Raft 中日志条目的“已提交”（committed）状态，**不是靠是否存在于日志中决定的**，而是依赖于“它是否已经被复制到了多数节点上”。

而当一个新 Leader 当选时，它：

- **无法立即知道哪些日志已经被复制到多数派上**；
- 更无法确认“哪些旧条目已经被上一个 Leader 成功提交”。

所以，**必须通过追加一条当前任期的 no-op entry 并观察其提交状态，才能安全地推进 commit index。**

------

## ⚠️ 为什么不能直接信日志？

因为 **Raft 允许日志在不同节点之间存在差异（尤其在出现 leader 崩溃时）**，例如：

- 原 Leader 写入了一条日志，**但尚未复制到多数派**；
- 然后崩溃了；
- 新 Leader 当选后，**可能复制的是另一条内容（或覆盖了这条未提交的）**。

这会造成新 Leader 的日志中**“看起来存在”的条目，可能并未被多数派接受过，也未被视为 committed**。

------

## 🔄 举个具体例子：

假设 term 是 3，现在系统日志是：

```
makefile


复制编辑
节点日志（index: term）：
S1: [1:1, 2:2, 3:3] （原 Leader，崩溃前写了 index=3）
S2: [1:1, 2:2]
S3: [1:1, 2:2]
```

- term 3 的 Leader（S1）写了 index 3；
- 但还没来得及复制给多数派；
- 然后崩溃；
- S2 成为了 term 4 的新 Leader。

问题：S2 是否知道 index=3 是不是已提交的？

答案：**不知道！**

因为它只能看到自己的日志（没有 index=3），**它并不知道 S1 是否已经把 index=3 复制给了 S3。**

------

## ✅ Raft 的解决方式：追加 no-op entry

为了确定**哪些条目是可提交的**，Raft 采用了这样的规则：

> **只有当前任期内由 Leader 自己写入并被多数派复制的日志条目才可以被标记为 committed。**

所以新 Leader 当选时，做以下事情：

1. 写入一条 no-op 日志（空命令，当前 term）；
2. 将其复制给多数节点；
3. 一旦这条 no-op 日志被确认 committed，Leader 才能安全地推进 commit index；
4. 此时，**根据 Log Matching Property**，之前所有日志也就可以被视作间接 committed。

------

## 🎯 总结一句话：

> **新 Leader 不知道哪些旧日志是已提交的，因为 “已提交” 不是等价于 “写在日志里”，而是要确认它已被多数派持有。而新 Leader 上任时，并不知道其他人日志的精确状态。**

------





这张图（**Figure 16**）展示的是 **Raft 算法在领导者崩溃后多久能够检测并选出新 Leader 的性能评估**。它主要探索了两个变量对选举延迟（downtime）的影响：

------

## 🧪 测试设定

- 实验环境：一个包含 **5 个服务器节点的 Raft 集群**。
- 广播时间（即 RPC 来回时间）约为 **15ms**。
- 每组测试重复 **1000 次（或 100 次）**。
- 触发点：每次试验都主动「崩溃 Leader」，测量从崩溃到新 Leader 被选出所用的时间（即 *downtime*）。
- 横轴：崩溃后的时间（ms）。
- 纵轴：百分比（即多少比例的试验在此时间内完成了新 Leader 的选举）。

------

## 📊 上图（Top Graph）：**Election Timeout 的随机性影响**

### ❓比较了什么？

设定 election timeout 的固定下界（如 150ms），然后让上限略微增长，例如：

- 150–150ms（无随机性）
- 150–151ms
- 150–155ms
- … 到 150–300ms

### 🔍 观察重点：

- **随机性越小（比如150–151ms）→ 容易发生竞选冲突 → 更容易 split vote（选票分裂） → 选举更慢甚至超时失败。**
- **随机性越大（比如150–300ms）→ 节点之间 timeout 时间拉开 → 更容易单一候选人胜出 → 选举更快。**

### 📌 结论：

> ✅ **即使只加 5ms 的随机性，也能显著减少选举冲突，提高选举成功率。**

------

## 📊 下图（Bottom Graph）：**Election Timeout 的下界影响**

### ❓比较了什么？

直接将 election timeout 的区间整体前移，比如：

- 12–24ms
- 25–50ms
- … 到 150–300ms

### 🔍 观察重点：

- Election timeout 越小 → 新 Leader 出现越快。
- 但 **timeout 太小** → 更容易出现 Leader 频繁被踢下台的问题（因为 RPC 心跳来不及完成，follower 误以为 leader 失联）。
- 最小配置（12–24ms）在实验中可以达到 **35ms 平均 downtime**，最快 **15ms 以内**恢复。

### 📌 结论：

> ✅ **较小的 election timeout 可大幅减少无 Leader 的时间，但过小则易误判失败 → 建议设置为 150–300ms 区间**（可靠又可接受的延迟）。

------

## ✅ 总结一句话：

> Raft 的选举机制对 timeout 配置非常敏感。**适度的随机性 + 合理的超时时间** 是快速、高可用选举新 Leader 的关键。



<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250413180423437.png" alt="image-20250413180423437" style="zoom:50%;" />





## 🗳️ 一、投票触发时机

- 每个节点启动时默认为 **follower**。
- 如果在一定时间（**election timeout**）内未收到来自 leader 的心跳（AppendEntries RPC），它会变为 **candidate** 并发起选举。

------

## 🔁 二、投票流程

1. **candidate 增加 currentTerm 并自投一票（给自己）**
2. 向所有其他服务器并行发送 `RequestVote` RPC：
   - 包含：
     - `term`：自己的当前任期
     - `candidateId`：候选者 ID
     - `lastLogIndex`：候选者日志的最后条目索引
     - `lastLogTerm`：候选者日志最后条目的任期
3. **每个 follower 收到投票请求时会做出判断**：
   - 若请求中的 `term` < 当前 term → **拒绝**
   - 若已经投过票且不是投给这个 candidate → **拒绝**
   - 若 candidate 的日志不够新（§5.4）→ **拒绝**
   - 否则 → **投票给 candidate 并记录 votedFor**

------

## ✅ 三、日志是否“足够新”的判断方式（日志新鲜度）

```
text


复制编辑
(1) 如果 candidate 的 lastLogTerm > 自己的 lastLogTerm → 新
(2) 如果 lastLogTerm 相等，candidate 的 lastLogIndex ≥ 自己的 → 新
否则认为旧
```

------

## 🔢 四、票数统计（majority 判定）

- 每个 candidate 会收集来自各节点的 `voteGranted` 响应。

- **一旦收到“多数”节点投票（包括自己）**，即：

  voteCount≥⌊N2⌋+1\text{voteCount} \ge \left\lfloor \frac{N}{2} \right\rfloor + 1voteCount≥⌊2N⌋+1

  就立即成为 leader。

  | 集群节点总数 N | 赢得选举所需票数 |
  | -------------- | ---------------- |
  | 3              | 2                |
  | 5              | 3                |
  | 7              | 4                |

------

## 📉 五、投票失败的处理

- 若：
  - **选举超时**（没有在超时时间内拿到多数票）
  - 或收到更高 term 的 `AppendEntries`/`RequestVote`
- candidate 会回退为 follower，重启选举流程（term +1）

------

## 🧠 六、几个关键的设计细节

| 问题                       | 机制                                                      |
| -------------------------- | --------------------------------------------------------- |
| 防止双 Leader              | 多数派投票机制 + term 保护                                |
| 防止票数分裂（split vote） | **随机化 electionTimeout**，减少同时成为 candidate 的概率 |
| 投票只投一次               | 每个 term 只能投一次，并持久化 votedFor                   |

------

## 📦 七、投票计算代码示意（简化版）

```
go


复制编辑
rf.mu.Lock()
rf.currentTerm += 1
rf.state = Candidate
rf.votedFor = rf.me
votes := 1
rf.mu.Unlock()

for peer := range peers {
    go func(peer int) {
        reply := sendRequestVote(peer, args)
        rf.mu.Lock()
        defer rf.mu.Unlock()
        if reply.VoteGranted {
            votes += 1
            if votes >= majority {
                rf.becomeLeader()
            }
        } else if reply.Term > rf.currentTerm {
            rf.currentTerm = reply.Term
            rf.state = Follower
        }
    }()
}
```

------

如果你想进一步了解日志新鲜度判断、votedFor 的持久化操作，或整个选举过程中如何使用锁（mutex）来保证并发安全，我也可以继续讲解。



##### 您说：

若已经投过票且不是投给这个 candidate → 拒绝，那刚开始都投给自己怎么进行？



###### ChatGPT 说：

这是一个非常棒的问题，涉及到 **Raft 选举过程中的细节时序与投票约束机制**。我们来详细解答这个“投票一次”原则下候选人自投票为何不会阻止后续选举进行。

------

## ✅ 回答核心：**每个节点在每个 term 只能投票一次**，而不是“只能投给自己”。

Raft 的机制是：

> **每个节点在同一个 term（任期）中，只能投给一个 candidate（包括自己）**，投票之后就不能修改投票对象。

------

## 🌱 举个例子

设集群有 5 个节点（S1 ~ S5）：

### 第一轮：

1. S1 超时，发起选举（term = 2），**自投一票**，votedFor = S1。
2. S1 向其他服务器发送 `RequestVote(term=2, candidateId=S1)`
3. S2 收到请求：
   - 如果它的 currentTerm 是 2 或更小，且它还 **没投过票**（votedFor == null），且 S1 的日志够新 → **投票给 S1**
   - 投完后 S2 的 votedFor = S1

此过程不会影响 S1 自己已投票，因为它本来就只能投给一个人 —— 它自己。

------

### 第二轮（如果第一轮没选出来，比如 votes 没过半）

1. 过了一段时间，S3 超时，也发起选举（term = 3），**自投一票**，votedFor = S3
2. S3 向其他节点发送 `RequestVote(term=3, candidateId=S3)`
3. 节点们看 term 更高，会更新自己的 currentTerm → 3，votedFor 也被清空，然后可能投票给 S3

------

## 🔄 为什么每个节点投自己不会导致选举卡住？

因为：

- Raft 设计了 **随机选举超时**（Election Timeout）机制。
- 大多数时候只有一个节点最先 timeout 成为 candidate。
- 其他节点来不及发起选举，就会先投给这个 candidate，使其胜选。

> 只有在极端情况（如多个节点同时超时）才可能造成“split vote”平局，Raft 会自动在下一个 term 重启选举。

------

## 🧠 补充：votedFor 何时被重置？

- 在新 term 开始时，Raft 会：
  - **将 currentTerm 更新为更高值**
  - **重置 votedFor = null**（即“还没投”）
- 所以每个 term 都是一次“重新洗牌”的过程。

------

## ✅ 总结一句话：

Raft 并不是“刚开始都投自己导致选不出来”，而是：

> 每个节点只在**当前 term**中投一次票（可能是自己），一旦出现新 term，投票资格就被重置。配合随机 timeout，大概率能快速选出 leader。

------

需要我画个小时序图帮助你更清晰理解这个过程吗？



##### 您说：

假如 a，b 同时开始选举，a 和 b 投给自己，c 投给 a，如何计算票数？ 是不是得对每个候选者用一个数组里的索引记录票数？



###### ChatGPT 说：

是的，你的理解非常接近正确实现！下面我将详细解释 **Raft 投票计数的原理**，并解答你这个具体情境中 **如何计算票数** 和 **代码层面上如何组织数据结构**。

------

## ✅ 问题场景再描述：

- 假设有 5 个节点：a, b, c, d, e
- a 和 b 在某一任期 `term = T` 同时开始选举：
  - a 投给 a（自投）
  - b 投给 b（自投）
  - c 收到 a 的请求 → 投给 a
  - d、e 可能还没反应过来或投给其他人

问题是：

> 💡 如何统计谁拿了多少票？怎么实现？是不是要有一个数组来存票数？

------

## 📊 正确投票计数机制：**由每个 candidate 自己统计自己的票数**

### 🎯 每个 candidate 发起选举时，需要维护一个变量，比如：

```
go


复制编辑
votesReceived := 1 // 初始为 1，自投票
```

### 🌱 每当收到一个 `RequestVoteReply{VoteGranted: true}`，就执行：

```
go


复制编辑
votesReceived += 1
```

### ✅ 判断是否赢得选举：

```
go


复制编辑
if votesReceived > len(peers)/2 {
    // 成功当选 leader
}
```

> 👆你可以不需要为“每个候选人”分别维护一个数组来统计票数，因为 **你只需要统计当前这个 candidate 的得票数**，其他 candidate 的投票情况你根本不会去管。

------

## 🧠 为什么不需要“所有候选者的投票统计数组”？

因为：

- 每一轮选举里，**每个节点只能投一票**（记录在它的 `votedFor` 字段里）
- 每个节点在 **收到 RequestVote RPC** 时，只要满足条件就会投票
- 而每个 candidate 自己收集响应来判断“自己有没有赢”

因此，在 Raft 实现中，**只有当前发起选举的 candidate 需要记自己的票数**，不需要追踪其他 candidate 的得票数。

------

## 🧩 举个代码实现例子（简化版）：

```
go


复制编辑
func (rf *Raft) startElection() {
    rf.currentTerm += 1
    rf.votedFor = rf.me
    rf.state = Candidate
    votes := 1 // 自投

    for peer := range rf.peers {
        if peer == rf.me { continue }
        go func(peer int) {
            args := RequestVoteArgs{Term: rf.currentTerm, ...}
            reply := RequestVoteReply{}
            ok := rf.sendRequestVote(peer, &args, &reply)
            if ok && reply.VoteGranted {
                rf.mu.Lock()
                votes += 1
                if votes > len(rf.peers)/2 && rf.state == Candidate {
                    rf.becomeLeader()
                }
                rf.mu.Unlock()
            }
        }(peer)
    }
}
```

------

## ✅ 总结

| 内容                                    | 说明                                             |
| --------------------------------------- | ------------------------------------------------ |
| 是否用数组统计多个候选人票数            | ❌ 不需要                                         |
| 每个 candidate 是否维护一个投票计数变量 | ✅ 是的，例如 `votesReceived`                     |
| 投票逻辑存储                            | 每个 follower 的 `votedFor` 表示本轮 term 投给谁 |
| 选举胜出条件                            | 得票数 > 总节点数 / 2                            |





## ✳️ 简要回答

> **`matchIndex[]` 的作用是：帮助 Leader 判断哪些日志条目可以“安全提交”。**

------

## 🧠 背景回顾：Raft 是怎么“提交”日志的？

在 Raft 中，**日志条目并不是写入就算完成**，只有满足以下条件，才认为该条目是“已提交的”（committed）：

> **某条日志被 Leader 的当前任期复制到了“多数派”上（包括自己） → 才可以提交。**

一旦日志被提交：

- Leader 会执行日志（作用于状态机）；
- Leader 会通过 `commitIndex` 通知 followers 也去提交执行；
- 最终客户端才能得到响应。

------

## ❓为什么不能光靠 `nextIndex[]` 判断是否提交？

`nextIndex[i]` 是“接下来”要发的日志位置，它不一定准确地代表“已经成功复制了哪些日志”。

🧨 举个例子：

```
plaintext


复制编辑
nextIndex[i] = 7   // 想给 follower i 发送日志索引为 7 的条目
但 i 实际上只同步到了 index=4（因为 index=5,6 冲突）
```

此时你不能仅靠 `nextIndex[i] - 1` 推断 i 的最新日志是 index=6，这可能是错的。

------

## ✅ `matchIndex[]` 的关键作用

> `matchIndex[i]` 记录了 **follower[i] 已经确认存储** 的最大日志索引 —— 是 “确认成功复制” 的日志最大索引。

Leader 会收集所有 follower 的 `matchIndex[i]`，然后判断：

### 🚩是否有某个日志索引 `N` 满足：

- **大多数服务器（包含 leader 自身）都有 matchIndex ≥ N**
- 并且 **log[N].term == currentTerm（当前任期）**

> ✅ 那么日志索引 N 就可以被“安全提交”！

### 📌 为什么还要检查 `log[N].term == currentTerm`？

为了防止旧任期的日志条目被新 Leader 提交，违反安全性（Raft 5.4 节强调：**只能提交当前任期的日志**）。

------

## 🔁 工作流程总结

1. Leader 向 followers 发送日志。
2. followers 成功后，Leader 更新 `matchIndex[i]`。
3. Leader 定期检查所有 `matchIndex[]`：
   - 找到最大的 index N，满足：
     - **N ≤ matchIndex[i] for a majority**
     - **log[N].term == currentTerm**
4. Leader 将 `commitIndex` 设置为 N。

------

## 🎯 为什么不能用 log[] 长度判断？

因为：

- 不一定每个 follower 都同步了；
- followers 可能掉线、延迟或日志冲突；
- 所以必须基于“已确认的复制结果”来判断（即 matchIndex）。





## ✳️ 简要回答

> **`matchIndex[]` 的作用是：帮助 Leader 判断哪些日志条目可以“安全提交”。**

------

## 🧠 背景回顾：Raft 是怎么“提交”日志的？

在 Raft 中，**日志条目并不是写入就算完成**，只有满足以下条件，才认为该条目是“已提交的”（committed）：

> **某条日志被 Leader 的当前任期复制到了“多数派”上（包括自己） → 才可以提交。**

一旦日志被提交：

- Leader 会执行日志（作用于状态机）；
- Leader 会通过 `commitIndex` 通知 followers 也去提交执行；
- 最终客户端才能得到响应。

------

## ❓为什么不能光靠 `nextIndex[]` 判断是否提交？

`nextIndex[i]` 是“接下来”要发的日志位置，它不一定准确地代表“已经成功复制了哪些日志”。

🧨 举个例子：

```
plaintext


复制编辑
nextIndex[i] = 7   // 想给 follower i 发送日志索引为 7 的条目
但 i 实际上只同步到了 index=4（因为 index=5,6 冲突）
```

此时你不能仅靠 `nextIndex[i] - 1` 推断 i 的最新日志是 index=6，这可能是错的。

------

## ✅ `matchIndex[]` 的关键作用

> `matchIndex[i]` 记录了 **follower[i] 已经确认存储** 的最大日志索引 —— 是 “确认成功复制” 的日志最大索引。

Leader 会收集所有 follower 的 `matchIndex[i]`，然后判断：

### 🚩是否有某个日志索引 `N` 满足：

- **大多数服务器（包含 leader 自身）都有 matchIndex ≥ N**
- 并且 **log[N].term == currentTerm（当前任期）**

> ✅ 那么日志索引 N 就可以被“安全提交”！

### 📌 为什么还要检查 `log[N].term == currentTerm`？

为了防止旧任期的日志条目被新 Leader 提交，违反安全性（Raft 5.4 节强调：**只能提交当前任期的日志**）。

------

## 🔁 工作流程总结

1. Leader 向 followers 发送日志。
2. followers 成功后，Leader 更新 `matchIndex[i]`。
3. Leader 定期检查所有 `matchIndex[]`：
   - 找到最大的 index N，满足：
     - **N ≤ matchIndex[i] for a majority**
     - **log[N].term == currentTerm**
4. Leader 将 `commitIndex` 设置为 N。

------

## 🎯 为什么不能用 log[] 长度判断？

因为：

- 不一定每个 follower 都同步了；
- followers 可能掉线、延迟或日志冲突；
- 所以必须基于“已确认的复制结果”来判断（即 matchIndex）。

------

## ✅ 总结：为什么需要 `matchIndex[]`？

| 作用                                   | 说明                             |
| -------------------------------------- | -------------------------------- |
| **确认 follower 已同步日志的最大位置** | 用于准确判断复制进度             |
| **安全地推进 commitIndex**             | 通过多数派的 matchIndex 推进提交 |
| **确保日志不会被“错提交”**             | 尤其是旧任期的日志               |
| **辅助实现状态机一致性**               | 避免不一致状态或日志分叉         |





这是一个非常关键的问题，涉及到 **Raft 中状态持久化设计的最小性原则与必要性分析**。下面我们来详细解释：

------

## ✅ 问题本质

在 Raft 中，**哪些状态需要持久化（即写入磁盘）**，目的是为了保证**在节点崩溃重启后，依然能保证一致性和正确性**。

Raft 明确规定需要持久化的内容包括：

```go
Persistent state on all servers:
- currentTerm
- votedFor
- log[]
```

而不是包括：

```go
- commitIndex ❌
- lastApplied ❌
- nextIndex[] ❌
- matchIndex[] ❌
```

------

## 🔐 为什么必须持久化 `currentTerm`

- 当前任期 `currentTerm` 是 **Raft 的时间逻辑核心**。

- 如果不保存，节点重启后可能用旧的 term 参与投票、发送 RPC，**破坏选举安全性和日志一致性**。

- 示例：

  > 一个 follower 崩溃后重启，回到 term 1，然后对 term 3 的 candidate 投票，显然是错的。

------

## 🗳️ 为什么必须持久化 `votedFor`

- 每个节点 **在每个 term 中最多只能投票给一个 candidate**。
- `votedFor` 是实现这个“最多投一票”的机制的关键。
- 如果不持久化，节点重启后可能忘记自己已经投过票，重复投票，**导致多个候选人都拿到多数票，破坏选举安全性**。

------

## 📦 为什么必须持久化 `log[]`

- Raft 的核心就是复制日志（log replication）。
- 如果崩溃丢失日志，节点的状态机会回退，导致不一致，**破坏状态机安全性（state machine safety）**。
- 所以日志是必须完整保存在磁盘的。

------

## ❓为什么 *不* 需要持久化 `commitIndex`？

### ✳️ 原因 1：commitIndex 是 **易恢复的中间状态**

- `commitIndex` 是 leader 决定后广播给 follower 的。
- 即便 follower 崩溃、重启了，新的 leader 会在 `AppendEntries` RPC 中再次告诉它最新的 `commitIndex`。
- follower 只需更新它的 `commitIndex = min(leaderCommit, lastLogIndex)` 即可。

### ✳️ 原因 2：commitIndex 本质是“临时可导出状态”

- 它不是决定性状态，而是可以从 `matchIndex[]` 推导出的（对于 leader）。
- 对于 follower，来自 leader 的心跳或日志同步也会携带 `leaderCommit`。

### ✳️ 举个例子：

```text
Follower 崩溃前：
  log: [1,2,3,4,5]
  commitIndex: 3

Follower 重启后：
  log: 依然在
  commitIndex: 0

Leader 通过 AppendEntries RPC 带上 leaderCommit=3
Follower 比较后更新 commitIndex=3
```

**➡️ 不丢一致性，完全可恢复**

------

## ✅ 总结对比表格

| 状态变量       | 是否持久化 | 原因                                           |
| -------------- | ---------- | ---------------------------------------------- |
| `currentTerm`  | ✅ 是       | 防止重启后错投票，保证选举安全性               |
| `votedFor`     | ✅ 是       | 保证 term 内最多投一次票                       |
| `log[]`        | ✅ 是       | 状态机的指令源，必须一致                       |
| `commitIndex`  | ❌ 否       | 可以从 leader 处重新同步；是“导出状态”         |
| `lastApplied`  | ❌ 否       | 重启后可通过 replay log 恢复；不影响协议一致性 |
| `nextIndex[]`  | ❌ 否       | 仅用于 leader 日志同步的优化，不影响安全性     |
| `matchIndex[]` | ❌ 否       | 用于 leader 判断已复制情况，重启后重新计算     |

------

## 🧠 设计哲学

Raft 的核心设计原则之一是：

> ✅ **只持久化不可恢复但又影响一致性的关键状态**
>  ❌ **不持久化可由日志恢复或网络同步恢复的状态**

这样做的好处是：

- 保证正确性和一致性 ✅
- 减少持久化开销，提升性能 ✅
- 降低实现复杂度 ✅！





你提到的这几种日志条目的状态非常关键，是 Raft 实现中核心的部分，我们来逐条解释这些状态的含义，以及它们的生命周期和在持久化与恢复过程中的处理策略。

------

## ✅ 四类日志条目的状态详解

| 类型                       | 是否存在于 `log[]` 中 | 是否算作已提交 | 是否已应用到状态机 | 是否能被恢复                 |
| -------------------------- | --------------------- | -------------- | ------------------ | ---------------------------- |
| 1. 未提交的条目            | ✅ 是                  | ❌ 否           | ❌ 否               | ✅ 是（持久化于 log 中）      |
| 2. 已提交但未 apply 的条目 | ✅ 是                  | ✅ 是           | ❌ 否               | ✅ 是（commitIndex 记录了它） |
| 3. 已提交并 apply 的条目   | ✅ 是                  | ✅ 是           | ✅ 是               | ✅ 是（如未快照清理）         |
| 4. 被快照覆盖的条目        | ❌ 否                  | ✅ 是           | ✅ 是               | ❌ 否（log 中已删除）         |

------

## 1️⃣ 未提交的条目

### ✦ 定义：

- 这些是 **leader 收到客户端请求后写入本地日志**，但 **尚未复制到多数派** 节点上。

### ✦ 特点：

- 不可被 `commitIndex` 标记为已提交；
- 不会被应用到状态机；
- **随时可能被新 leader 回滚（overwrite）**；
- 存在于 leader 的 `log[]` 中；
- follower 会通过 `AppendEntries` 追加它。

### ✦ 作用：

临时缓冲区，是日志复制机制的一部分。

------

## 2️⃣ 已提交但未 apply 的条目

### ✦ 定义：

- 被多数节点复制成功；
- `leader` 推进了 `commitIndex`；
- **还没被状态机处理**（即 `lastApplied < commitIndex`）。

### ✦ 特点：

- 已被视为“**对外生效的决定**”，不能回滚；
- **状态机尚未处理命令逻辑**（比如修改键值对）；
- 存在于 `log[]`；
- 可通过 `commitIndex` 找到。

### ✦ 处理：

- 一个单独 goroutine 会不断将 `commitIndex` 范围内的新日志通过 `applyCh` 应用到状态机。

------

## 3️⃣ 已提交并 apply 的条目

### ✦ 定义：

- 这些日志条目 **不仅已被复制多数派并标记为已提交**，还已经被应用到了本地状态机。

### ✦ 特点：

- log 中依然存在（还未被快照清理）；
- 状态机状态已更新；
- **重启后状态可以从状态机快照恢复**；
- 可以用来支持线性一致的读请求。

------

## 4️⃣ 被快照覆盖的条目

### ✦ 定义：

- 这些日志已被 `snapshot` 替代，并被从 `log[]` 中移除；
- 对应的状态 **已经融合进快照中**；
- 通常 `lastIncludedIndex` 和 `lastIncludedTerm` 会记录它们的元信息。

### ✦ 特点：

- **在 `log[]` 中找不到**；
- 重启后只能通过快照恢复状态机；
- 不再可被用于日志回放；
- 若 follower 日志落后于 `lastIncludedIndex`，leader 会发送 InstallSnapshot RPC。

------

## 🔁 生命周期示意图

```
客户端写入请求
    ↓
日志追加到 leader log[]（未提交） ←【未提交】
    ↓
多数派复制成功
    ↓
commitIndex 推进 ←【已提交未 apply】
    ↓
applyCh 发送 ←【已提交已 apply】
    ↓
快照触发，旧日志清除 ←【快照覆盖】
```

------

## ✅ 为什么“快照覆盖的条目”不能恢复？

- 被快照覆盖意味着这些日志内容被压缩、删除，只保留了结果状态；
- 虽然状态机状态已保留，但原始命令（如 PUT X=5）已丢失；
- 若需要回滚日志或新节点追赶，则必须从 `InstallSnapshot` 中重建状态，**不能依靠日志**。

------

## 💡 典型应用中各状态如何持久化？

| 类型         | 持久化方式                                                  |
| ------------ | ----------------------------------------------------------- |
| 未提交的日志 | 写入 `log[]`（持久化）                                      |
| commitIndex  | 不持久化，但 leader 会通过 `AppendEntries` 同步             |
| lastApplied  | 不持久化，重启后由 `commitIndex` 恢复 replay                |
| 快照         | 写入 `snapshot` 文件，含状态机状态 + lastIncludedIndex/Term |

------

## 🧠 结语：为什么 Raft 不直接持久化 commitIndex？

- 因为 commit 状态可以通过 leader 恢复；
- 简化实现、提高性能；
- 所有副本状态最终会向 leader 看齐。

------

如果你希望，我可以画出一个「日志条目状态变化流程图」或「log + snapshot 的恢复时机图」，帮助你进一步巩固这个知识点。是否需要？





<img src="/Users/moon/Library/Application Support/typora-user-images/image-20250423234749784.png" alt="image-20250423234749784" style="zoom:50%;" />

图中展示的是 Raft 日志复制系统中，**新 leader（term 8）登场后在不同 follower 上可能看到的日志状态差异**，这正是你在 `TestBackup2B` 中遇到的问题的核心背景。我们逐一解释：

------

## ✅ 图中内容解析

每个横条表示一个 Raft 节点的日志状态，颜色和数字表示该日志条目的任期（Term）：

- 横向代表日志的 **log index**（1 到 12）。
- 每个方格是一个日志条目，数字是该条目的 **Term**。
- 顶部横条是新选出的 leader，处于 **Term 8**。
- 下方 (a) ~ (f) 是可能的 follower 日志状态。

------

## 🔍 每种 follower 状态详细解释

### ✅ (a) Missing some entries — **部分缺失**

```
Leader: 1 1 1 4 4 5 5 6 6 6
(a):     1 1 1 4 4 5 5 6 6
```

- 缺少最后一条 index=10 的日志。
- 原因：follower 在收到这条日志之前断连。

------

### ✅ (b) **严重缺失**

```
(b):     1 1 1 4
```

- 仅跟上了前面 4 条日志。
- 这可能是长期未连入网络或曾因崩溃重启导致数据未恢复。

------

### ✅ (c) **比 leader 更长的日志（未提交）**

```
(c):     1 1 1 4 4 5 5 6 6 6 6
```

- 多了一条 Term 6 的日志。
- 说明该 follower 曾在 term 6 时是 leader，**追加了还未 commit 的日志条目**。

------

### ✅ (d) **未提交的后续 term**

```
(d):     1 1 1 4 4 5 5 6 6 6 7 7
```

- 在 term 7 时曾是 leader 或接受了 leader 的日志，但这些都未被 commit。
- 这段未 commit 的日志会在 leader 成功后被 **截断并覆盖**。

------

### ✅ (e) **冲突日志**

```
(e):     1 1 1 4 4 4 4
```

- index 相同但 term 不一致。
- 显示该 follower 曾在 term 4 为 leader，写入了额外内容（但未 commit）。

------

### ✅ (f) **完全不同的分支**

```
(f):     1 1 1 2 2 2 3 3 3 3 3
```

- 曾在 term 2、term 3 成为 leader，写入了很多内容。
- 但由于从未 commit，这些都会被新的 leader 覆盖。

------

## 💡 图 7 的关键说明

当新的 leader（term 8）当选时：

> 它必须确保所有 follower 的日志最终与自己 **一致**。这包括：

1. **发现日志冲突**：通过 `PreLogIndex` 和 `PreLogTerm` 进行验证。
2. **覆盖冲突日志**：通过 `AppendEntries` RPC 中的 `Entries` 追加正确内容。
3. **推进 commitIndex**：只允许 commit 当前 term 的日志（图 8 原则）。

------

## 🚨 和你 `TestBackup2B` 的问题直接相关：

你的问题与图中的 (c)、(d)、(e)、(f) 高度一致：

- 有 follower（如 server 4）保留了旧 leader term 的日志条目（如 Term 6、Term 7）。
- 由于新 leader 没有正确触发 follower **截断日志**，导致旧日志被提交。
- 你看到的 `apply error: server 1 != server 4` 正是这个问题的结果。

------

## ✅ 修复建议的理论依据

图 7 告诉我们：

> **Raft 必须完全控制所有 follower 的日志状态，即使对方比你“长”或者 index 相同但内容不同，也要敢于覆盖！**

------

是否需要我接下来：

- 用图 7 的结构给出你当前系统中 server 日志错误提交的“复现情况”？
- 或者给出如何在 `AppendEntries` 和 `updateCommitIndex` 中引入 **图 7 场景防御逻辑**？

你想继续从哪边入手？