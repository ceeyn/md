### Why

本文讨论的场景是互联网业务中秒杀场景下热key的解决方案，秒杀场景不局限于电商活动，也包括：日常活动下发奖品的配额控制、春节的集卡瓜分现金、春晚的摇一摇抢红包等。更进一步的说，是探讨所有关于有热key的场景的一种解决方案。 

<img src="/Users/giffinhao/Downloads/笔记/pic/59dc2df8312a4cc6b05468f4778a57e0.png.webp" alt="59dc2df8312a4cc6b05468f4778a57e0.png" style="zoom:50%;" />

以上图为例，解释下各个模块： 发货模块：先查询库存，有库存后给用户发货，进入到账流程。

到账模块：把钱发到用户账户上。

使用模块：用户看到账户上的余额，进行提现/使用操作。

发货流程中，有高并发流量，有热点key。本文重点讨论这个流程。

到账流程中，有高并发流量，无热点key。一般只有库存数量的请求才会进入到这一步，用户的请求已经少了很多，而且这里可以按用户分区治理解决高并发问题（分set分库分表等）。本文不讨论此流程。

提现流程中，用户到账后，有提现需求的用户才会发起请求，本文不讨论此流程。

### How

按照业务规模和流量预估，将场景分成如下几类，分别探讨下热点的解决方案。

#### qps < 1k

<img src="/Users/giffinhao/Downloads/笔记/pic/0f7f9e3fab394d339986b62019bf4620.png.webp" alt="0f7f9e3fab394d339986b62019bf4620.png" style="zoom:50%;" />



直接利用DB存储库存、用户的结果数据。



1. <1kqps流量的服务，db层没有压力，直接利用DB做库存控制即可。



1. 可以利用从库分担读压力：



有库存时，已经抢到的人直接读取抢的结果； 没有库存时，没抢到的人直接根据读取的库存的结果，返回已经抢完。 还要谈一下安全的问题，接入层的节点可以定时cache黑产名单，提前拦截掉请求。

#### 1k< qps < 10w

<img src="/Users/giffinhao/Downloads/笔记/pic/0f7f9e3fab394d339986b62019bf4620.png.webp" alt="0f7f9e3fab394d339986b62019bf4620.png" style="zoom:50%;" />







这个qps下，库存的DB是瓶颈，假设采用redis，单key可以扛住10wqps的读写，用redis存储库存信息。 抢到的用户才会进入到账，这部分的并发量取决于库存数，因此这里开始把发货和到账流程分开。用户流程分几步：



1. 用户首先查询是否抢过，已经抢到的用户，可以直接返回



1. 扣库存成功后，保证到账请求成功，如果失败的话，可以异步补单重试保证最终一致。



1. 到账模块把用户结果存储后，需要构建出用户结果的cache，用来将已经抢过的用户信息存储下来，把再次请求的量挡在前面。
2. 如果库存没有了，直接返回，告诉用户已经抢完。



如果扣件库存失败了，怎么处理？



1. 如果是超时错误，此时不能确定库存是否扣成功，不能发起重试，直接告诉用户失败，让用户发起重试。这里会带来真实发出去的货比预估的库存少，可以通过定时对账算出补偿值修正库存值，补偿值=库存预估数- 真实到账数-库存余额。
2. 如果是非超时错误，直接返回失败，让用户发起重试。





#### 10w < qps < 100w

这个qps下，redis的单key也存在瓶颈了，可以采用分治的方法解决热key问题。 一般有两种方式：

1. redis中分10个key，库存数平均分布在每个key上，用户按照hash规则到固定的key上。优点是配额能够精准控制，缺点是： redis压力大，同时分key上的库存有碎片。被路由到在无库存的key上用户，哪怕先进来抢商品，也会提示没货。
2. 库存在存储中是单key，发货服务的每个机器节点定时从库存中取固定数量的库存，预扣库存方式放在本地cache做。优点是db压力小，缺点也很大：微服务节点如果被调度走的话，预扣的库存没有发货导致库存不准。同一个用户的体验单调性差，库存节点的预扣库存耗尽时，此时用户体验时无库存，节点再次拿到预扣库存后，会提示用户有库存。



这里提出基于瞬间高并发下热key的多分片本地cache方案，如下图所示。

<img src="/Users/giffinhao/Downloads/笔记/pic/ef629f70fd5a464f8e6c7f70ccb8be4f.png.webp" alt="ef629f70fd5a464f8e6c7f70ccb8be4f.png" style="zoom:50%;" />

库存存储按key分片，分片公式参考：max(min(库存数, qps/10w), 1)。新加进来的库存节点，从库存db中将数据loading到本地cache住。发货访问库存步骤如下：

1. 发货请求到库存节点，首先通过hash选取一个本地分片，本地分片查询到有库存后，去db扣减库存。
2. 如果本地cache和db数据不一致导致库存扣减失败，此时刚好更新本地cache值。如图所示，本地节点从1->0。
3. 重新挑选本地非0的分片。
4. 重复2的过程，此时发现有库存，但db中的库存数更少，需要更新本地cache9->6。

如果本地分片cache都是0，代表此时库存已经没有了，直接返回上游已经瓜分完毕。 本地的分片cache本质上是个bloom滤波器，告诉你有库存时不一定有，但是告诉你没有库存时一定没有。刚好应对高并发流量下，大量抢不到商品的用户应该快速返回告知用户结果。

这个方案也解决了用户时有时无问题，也解决了先来的用户瓜分不到，后面来的用户可以抢到的问题。对db的方案压力也不大。

需要特别说明的是，当db无库存时，当本地cache有库存时，会触发扣减库存，如果10个本地cache都不为0，会触发10次的扣减库存IO，但是很快会收敛到本地cache和db一致的情况，我们测试了20wqps下的请求的放大倍数在1.003756倍。

管理员补库存后，本地库存节点会定时取到库存，cache在本地节点。

<img src="/Users/giffinhao/Downloads/笔记/pic/11145b2fc81b41729de22cfae631a8f4.png.webp" alt="11145b2fc81b41729de22cfae631a8f4.png" style="zoom:50%;" />

这里引入了瓜分服务，分担发货服务的压力，针对没有资格的用户、已经瓜分过的用户，提前把流量拦截掉。

#### 100w < qps < 1000w

这个量级的秒杀系统，我自己没有做过，参考了微信2015年春节摇一摇红包的做法：

1. 红包业务独立部署，不影响其他业务
2. 全部逻辑在线逻辑放到接入层，峰值流量下减少峰值的调用
3. 红包模块做两个事情：a)控制速率，匀速下发红包，保证后续的到账、提现的负载在可控范围；b)定时回收没发完的红包
4. 用户开始抢红包，真正抢到的才走到账逻辑。

<img src="/Users/giffinhao/Downloads/笔记/pic/7563f13fe9e44d068dc424a2f0873e15.png.webp" alt="7563f13fe9e44d068dc424a2f0873e15.png" style="zoom:50%;" />

#### qps > 1000w

系统架构设计时，总是要穷尽所有情况，讨论下>1000wqps的场景，这里1000w也可以理解成你的系统容量上限，超过了怎么办，这就得回到海量服务的价值观，有损服务，接入层先把流量限制住。有损服务有多个策略，我会专门写文章来解释。

### conclusion

1. 能用db扛，优先db
2. 能用nosql单key扛，优先单key扛
3. 超过单key压力，分治
4. 流量大得离谱，分治+提前本地cache
5. 超出系统容量，有损服务







本文讲述的是支付有优惠的周末好心情活动，支付有优惠微信支付的一个权益小程序，用户可以通过使用微信支付完成任务，领取金币，然后使用金币兑换代金券、提现券、商家券等各种有优惠。

周末好心情是一个旨在通过活动提升用户对食品品类的优惠心智，通知打造用户对周末活动的认知和记忆点，活动主题为“周末好心情～食在有优惠”。 活动时间是每周五下午16:00到周日下午16:00点，每天一场秒杀。

活动预热期间进行订阅活动提醒。活动开始后，用户可以免费参与秒杀活动，秒杀银行立减金、优惠商家券用户可以在小程序首页、公众号推文等方式了解活动的开始时间及相关内容，并可在小程序。

其中一块尝鲜秒杀模块旨在提高商户的人气；银行立减模块在于帮助银行拉绑定卡、沉默用户促活，用户也可以享受到福利，平台也可以刺激更多用户在周末会场兑换奖品；也可以参与会场活动，直接使用金币兑换人气单品，或则自己想要的商家券。



因此在活动期间，我们必须满足各方的需求

1.作为用户，希望能够抢到自己中意的优惠

2.作为商户，希望券不超发，系统运行稳定

3.作为微信支付，希望能通过每周末活动提高业务口碑，同时保证资金安全

这意味着，我们的目标是高并发场景下保证系统的高可用以及数据的一致性

我们首先看看如何提高系统的可用性。 为了用户能够正常参与活动，那么首先我们需要保证用户能看到活动正常展示，如果不能看到，那么必然存在大量的客诉，所以我们需要优先保证用户能看到。

### 痛点1:秒杀活动轮次多，每轮展示奖品内容多(超过100个)

我们每周末都会有多个场次，每场展示的奖品超过100个，每个奖品都有大量的素材、图片等信息，同时还需要判断用户是否已经参与过秒杀、兑换过每个奖品，这里如果每次都实时拉取，毫无疑问会增加后台的负担。那么我们能不能将这些信息缓存下来，等来用户请求的时候，我们直接从本机读取即可，而不需要发起RPC，调用存储。

### 破局1:设计合理的缓存方案(查询峰值30W/min)

1. 因此我们可以区分出静态数据（活动配置、会场数据、奖品配置）和动态数据（用户兑换数据），针对静态数据（变化频率极低的数据）我们可以提前缓存到本地，而动态数据，因为需要实时计算，这里不进行缓存。这样我们就可以减少大量的RPC请求，从而达到保护系统的目的。
2. 但是这些静态数据仍然可能需要更新，因为运营同学可能需要修改奖金、额度等一些信息，因此我们需要有机制去更新缓存。更新机制一般分为同步和异步，这里我们采用一个daemon进程定时从存储组件中获取静态数据信息，然后写到本机的memcache中，同时过期方式设置为永不过期。这样系统就不存在缓存雪崩、缓存穿透问题。从而提高系统稳定性。
3. 同时针对用户的兑换数据，我们存储在TTLKV中，设置过期时间为活动的结束时间，同时每10s更新一次
   解决了看的问题，接下来我们需要解决抢的问题：

### 痛点2:秒杀活动瞬时请求量大(峰值20W/min）

秒杀活动一定会出现激增流量，因此我们需要想办法让用户的秒杀请求陆续到达后台，这样就可以通过前端保护后端的方式，提高系统的稳定性，使得用户可以正常秒杀奖品

### 破局2:流量削峰

1. 增加用户维度限流和接口维度限流，降低流量，达到保护商家，保护系统的目的

### 痛点3:商户系统不可控（每周末3场秒杀，涉及3个商户，普通奖品涉及商户更多）

商户系统属于外部系统，而发券其实强依赖于商户。

如果是同步发券，我们系统就会把所有流量打到营销系统，再由营销系统透传到商户系统，但是商户系统可能无法支撑这么大的请求量，最后引起商户系统雪崩，而用户就无法及时获得商家券，最终引起客诉。所以我们要避免这种情况。

### 破局3:异步发券

1. 因为我们可以将发券操作变为异步发券，以3000/s的QPS将发券请求发送到营销系统。然后营销系统再将发券请求通知到商户，这样商户系统的最高流量就变成了可预估的流程，从而达到保护商户的目的，降低潜在的客诉风险。
2. 所有参与秒杀的商户都必须配合联合压测，并提供压测报告，如压测结果不符合预期，我们可以更换商户，换用系统能够符合要求的商户参与活动

## 挑战2-一致性建设

<img src="/Users/giffinhao/Downloads/笔记/pic/8486f4817ab64eeca10c0ebfc97e3403.png.webp" alt="8486f4817ab64eeca10c0ebfc97e3403.png" style="zoom:50%;" />

我们首先看看如何提高系统的可用性。 为了用户能够正常参与活动，那么首先我们需要保证用户能看到活动正常展示，如果不能看到，那么必然存在大量的客诉，所以我们需要优先保证用户能看到。

### 痛点1:秒杀活动轮次多，每轮展示奖品内容多(超过100个)

我们每周末都会有多个场次，每场展示的奖品超过100个，每个奖品都有大量的素材、图片等信息，同时还需要判断用户是否已经参与过秒杀、兑换过每个奖品，这里如果每次都实时拉取，毫无疑问会增加后台的负担。那么我们能不能将这些信息缓存下来，等来用户请求的时候，我们直接从本机读取即可，而不需要发起RPC，调用存储。

### 破局1:设计合理的缓存方案(查询峰值30W/min)

1. 因此我们可以区分出静态数据（活动配置、会场数据、奖品配置）和动态数据（用户兑换数据），针对静态数据（变化频率极低的数据）我们可以提前缓存到本地，而动态数据，因为需要实时计算，这里不进行缓存。这样我们就可以减少大量的RPC请求，从而达到保护系统的目的。
2. 但是这些静态数据仍然可能需要更新，因为运营同学可能需要修改奖金、额度等一些信息，因此我们需要有机制去更新缓存。更新机制一般分为同步和异步，这里我们采用一个daemon进程定时从存储组件中获取静态数据信息，然后写到本机的memcache中，同时过期方式设置为永不过期。这样系统就不存在缓存雪崩、缓存穿透问题。从而提高系统稳定性。
3. 同时针对用户的兑换数据，我们存储在TTLKV中，设置过期时间为活动的结束时间，同时每10s更新一次
   解决了看的问题，接下来我们需要解决抢的问题：

### 痛点2:秒杀活动瞬时请求量大(峰值20W/min）

秒杀活动一定会出现激增流量，因此我们需要想办法让用户的秒杀请求陆续到达后台，这样就可以通过前端保护后端的方式，提高系统的稳定性，使得用户可以正常秒杀奖品

### 破局2:流量削峰

1. 增加用户维度限流和接口维度限流，降低流量，达到保护商家，保护系统的目的

### 痛点3:商户系统不可控（每周末3场秒杀，涉及3个商户，普通奖品涉及商户更多）

商户系统属于外部系统，而发券其实强依赖于商户。

如果是同步发券，我们系统就会把所有流量打到营销系统，再由营销系统透传到商户系统，但是商户系统可能无法支撑这么大的请求量，最后引起商户系统雪崩，而用户就无法及时获得商家券，最终引起客诉。所以我们要避免这种情况。

### 破局3:异步发券

1. 因为我们可以将发券操作变为异步发券，以3000/s的QPS将发券请求发送到营销系统。然后营销系统再将发券请求通知到商户，这样商户系统的最高流量就变成了可预估的流程，从而达到保护商户的目的，降低潜在的客诉风险。
2. 所有参与秒杀的商户都必须配合联合压测，并提供压测报告，如压测结果不符合预期，我们可以更换商户，换用系统能够符合要求的商户参与活动

## 挑战2-一致性建设

<img src="/Users/giffinhao/Downloads/笔记/pic/94167a5971ab47e190085bf03d7829bb.png.webp" alt="94167a5971ab47e190085bf03d7829bb.png" style="zoom:50%;" />

接下来我们看看如何提高系统的一致性。 在整个秒杀活动中，我们需要保证以下目标：

### 目标

1. 奖品不超发
2. 用户不多抢（一轮只能秒杀一次）
3. 有优惠系统、营销系统、商户系统数据一致性

那么我们需要如何做到以上的目标呢？

这是个类似分布式事务一致性的问题，我们调研了多种方案，包括代金券系统，消费券系统，行业权益系统等，这里列出了2个：

一个是代金券系统的方案，代金券要求在扣除券库存之后，记录预发记录，还要扣除用户限额也就是自然人限额，它是通过mysql事务，保证全库存与预发记录的一致性，而用户限额扣除后置，非原子操作，极端情况下可以突破自然人限制。

另一个方案是行业消费券的方案，消费券先预扣库存，再扣用户限额，在提交库存之后将订单落MQ，交由MQ处理后续事宜。如果有失败，会执行库存回滚。如果库存预扣、提交和回滚有任何不可预知失败，都会抛一个MQ事件，交由MQ驱动数据一致。为避免MQ故障，通过对账服务对出MQ未处理到的部分，再进行处理。

我们的场景有些不一样，我们这里因为包含以下的额度： 活动场次限额、奖品限额、用户账户（兑换奖品需要金币）。而其中活动场次库存由秒杀服务处理，奖品库存、用户账户都是由兑换服务处理。

### 实现

1. 通过秒杀服务直接扣库存（Quotakv），这样可以保证奖品不超发，这里没有二阶段的原因是，会增加系统的复杂性，只需要保证不超发，不保证少发。
2. 然后设计合理的幂等ID来保证一个用户只能参与一次秒杀。
3. 通过接入事件中心的方式来保证发券的成功，可以利用事件中心的反查机制，重试用户发券的流程，防止中间任何一步出现问题，保证发券成功
4. 同时对账机制，保障支付有优惠系统和营销系统的数据一致性，如果出现单边账，通过补发机制打到最终数据一致

## 挑战3-BCP、安全建设

<img src="/Users/giffinhao/Downloads/笔记/pic/9ca841a75e7748929b45dd7615b38c3a.png.webp" alt="9ca841a75e7748929b45dd7615b38c3a.png" style="zoom:50%;" />

虽然前面我们已经做好了各种可用性的保证，但是等到服务正式运行到过程中，仍然可能出现各种各样的可能会发生的情况，因此我们要对这些这些可能发生的情况准备好各种预案，降低系统风险，

主要包含以下几点：

1. QuotaKv故障，我们可以通过秒杀服务按照机器数量，在每台机器上预先加载部分库存，本地库存消耗完毕后，再调用quotakv消耗远端库存，并更新本地库存
2. TabeleKV故障，可以通过挂公告安抚用户，奖品24小时内陆续到帐
3. 秒杀服务故障，可以通过跨园区多副本部署，降低故障概率
4. 可靠事件中心故障，通过异步发券的方式，让MQ进行重试，同时接入双链路事件中心，降低故障概率

同时我们也应用STRIDE安全模型，针对系统可能存在的安全风险制定合理的安全消减的方案，保证业务的安全性

## 秒杀系统总结

![9ca841a75e7748929b45dd7615b38c3a.png](/Users/giffinhao/Downloads/笔记/pic/9ca841a75e7748929b45dd7615b38c3a.png.webp)

![b776f0eb7500403daa96fcb2a8c3de0d.png](/Users/giffinhao/Downloads/笔记/pic/b776f0eb7500403daa96fcb2a8c3de0d.png.webp)

通过以上措施，我们总结了在设计秒杀系统的过程中我们可以采取的一系类措施

### 可用性

1. 并发控制：接入限频组件，保证系统处理请求数量在可控范围内
2. 幂等重试：设计合理幂等方案保证接口可重入
3. 过载保护：系统不可处理时，快速拒绝
4. 流量削峰：前端保护后端，流量打散请求
5. 灰度策略：针对银行立减金奖品，只对定向人员进行展示

### 一致性

1. 基本可用：及时将结果展示给用户，降低用户不安情绪
2. 最终一致性：同步变异步，降低快慢系统之间的耦合
3. 对账补偿：通过对账发现数据不一致的地方

### BCP

1. 降级预案：准备降级预案，应为未知风险（屏蔽入口，暂停活动，秒级生效，用于黑天鹅事件事中处理）
2. 公告预案：准备公告方案，及时安抚用户情绪

## 服务拆分

<img src="/Users/giffinhao/Downloads/笔记/pic/e11cc129a0fe4ae28fcd6923a714d759.png.webp" alt="e11cc129a0fe4ae28fcd6923a714d759.png"  />

<img src="/Users/giffinhao/Downloads/笔记/pic/863602a21ff643aeaa87887977547c06.png.webp" alt="863602a21ff643aeaa87887977547c06.png"  />