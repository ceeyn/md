https://xie.infoq.cn/article/79db665cf73aed9fd4abf1088

id映射

​	冷启动

​	订阅id变更的流水

生成、同步bitmap

​	上游生成

​	同步（交互、查bitmap数据、反序列化、分段、存bitmap（元数据、分段数据））

下游使用：

​	bid判存

​	bitmap 分页拉取



冷启动时：

全量业务id 转换成 正向映射【uid-》bit】 反向映射【bit -》uid_corpId】，同时写到下游的kafka【目的：写正向映射，提供给生成bitmap】

有新注册的业务id：

我们消费【新注册上报】流水，生成正向和反向映射，同时写到下游的kafka

```
// UserGroupBmp 人群包数据
type UserGroupBmp struct {
	// UserGroupID 用户群id
	UserGroupID int64 `db:"user_group_id"`
	// ImpDate 日期分区
	ImpDate int64 `db:"imp_date"`
	// HashPart hash分段
	HashPart int64 `db:"hash_part"`
	// BmpCount 人群bitmap总数
	BmpCount int64 `db:"bmp_count"`
}

```

查starrocks

```
userGroupID = BitmapID
GetUserGroupBmps 根据userGroupID查询user_group_bmp表数据
GetBitmapSyncData调用GetUserGroupBmps返回userGroupBmps，然后分10000段，：select user_group_id, imp_date, bitmap_count(bmp) as bmp_count from user_group_bmp where user_group_id=%v", userGroupID 查出来rows，返回

// 反序列化db bitmap数据
bitmap, err := GetBitmap(ctx, bitmapData)
// bitmap分段
cardinality, length, step, segmentIndex, segmentBitmaps, err := Segment(ctx, bitmap)
bitmapSegmentSave 分段bitmap存redis 
key := fmt.Sprintf("%s_%v_%v", BitmapSegmentDataKey, info.BitmapID, k)
// bitmap 元数据信息使用hash存储 key BitmapID val hash
err := dao.SetBitmapSegmentInfo(ctx, info, expireTime)
// bitmap 分段数据序列化后使用kv存储 key id+段号（0-100）val 相应段转为base64
err = dao.SetBitmapSegmentData(ctx, info, expireTime)
```

上游生成一些人群包的bitmap（例如：最近三天参会的人，**非实时**）存入startRocks，**上游调用SyncBitmapData方法**【BidType，BitmapID】从startRocks查出来同步bitmap数据，分段后存入redis，本地缓存分段信息和分段数据，

当需要判存时，根据bitmapId获取本地缓存BitmapSegmentInfo，获取业务id对应bit位，计算业务id所属分段bitmap，从 BitmapSegmentInfo获取bitmap分段对应索引为0，说明该分段bitmap为空，bitmap分段对应索引不为，说明该分段bitmap有值，需要查询该分段bitmap，从BitmapSegmentDataCache中获取分段

需要分页拉取时，根据bitmapId获取本地缓存BitmapSegmentInfo，计算请求中的offset所属分段bitmap，获取分段bitmap

```
GetBitmapDataByPage 从数据库中分页获取 bitmap 数据，并将其解析为 int64 切片。
sql := fmt.Sprintf("select bitmap_to_string(sub_bitmap(bmp, %v, %v)) as bitmap_str from user_group_bmp "+
		"where user_group_id=%v AND imp_date=%v limit 1", offset, len, userGroupID, impDate)
```

在压测（性能测试）中，制定策略并进行实际测试时，需要考虑多个因素来确保系统在高负载下的表现符合预期。下面详细介绍压测策略及如何评估性能：



situation：构建人群画像需要人群包，之前是uid的集合，导致存储空间大，时间效率低，现在希望使用bitmap来构建，需要一种通用的bitmap服务

task：需要提供一种映射，一种bitmap，三种能力，业务id到bit位的映射，判存、分页拉取、位运算，我们需要做到push下发ms级

action：1.调研了多种映射方案（直接映射空洞太大，查表+直接，页表【解决了空洞，跨段操作，查询操作消耗性能】，hash不能反解，2.bitmap调研以后选用roaringbitmap（64位，支持压缩）3.分段

result：1.存储空间下降，2.redis的



**roaringbitmap   64位， 高32位分桶，低32位是一个container，有array（稀疏）、bitmap（密集，元素大于4096）、rle（连续）三种类型，【压缩算法】，序列化成redis的k-v，string类型**



把商品大宽表的 23 个多值字段由数组类型升级成 RoaringBitmap，整体优化收益如下：

●商品大宽表存储空间下降 **30% ~ 40%**，存储文件个数下降 **30% ~ 40%**（不同国家有所差异，整体在 30% ~ 40%）。文件个数减少有以下几个好处

   ○提高查询性能：【更少的文件意味着查询需要扫描的数据量和文件数量都较少，从而提高查询速度和系统整体性能。】

​    ○减少存储开销：【小文件会在硬盘上产生更多的碎片，导致磁盘空间利用率降低，文件个数减少有助于提高磁盘空间利用率。】

​    ○简化管理：【当文件数量较多时，管理和监控任务会变得更加复杂。文件个数减少能够简化管理和维护工作，提高运维效率。】

​    ○优化备份和恢复：【备份大量小文件通常比备份少量大文件所需的时间更长。文件个数减少可以缩短备份和恢复的时间，提高业务连续性。】

​    ○减轻元数据压力：文件系统需要跟踪每个文件的信息（如权限、大小等），大量的小文件会增加元数据的压力。文件个数减少有助于减轻元数据服务器的压力，保证系统的稳定运行。





```
在 Redis 中存储 RoaringBitmap 时，通常会将其序列化为字节数组格式（byte array）并进行压缩，然后以二进制的形式存入 Redis。RoaringBitmap 的压缩算法结合了位图压缩技术，例如 **Run-Length Encoding (RLE)** 和 **位块压缩 (Bit-block compression)**，以减少存储空间。

以下是 RoaringBitmap 的存储和压缩细节：

---

### **RoaringBitmap 在 Redis 中的存储格式**

1. **序列化为字节数组**：
   - RoaringBitmap 在存储到 Redis 前，会被序列化为字节数组（byte array），通常使用 `toByteArray()` 方法。
   - 序列化后的字节数组会包含 RoaringBitmap 的所有容器信息，包括 `ArrayContainer`、`BitmapContainer` 和 `RunContainer` 的数据。
   
2. **存储为二进制数据**：
   - 将序列化后的字节数组直接存储在 Redis 中，可以使用 Redis 的 `SET` 命令来存储字节数组。
   - 这种存储方式确保了 Redis 中的数据不会丢失任何 RoaringBitmap 的结构信息，方便日后将数据重新读取并恢复为 RoaringBitmap。

3. **可选的 Redis 序列化工具**：
   - 有些情况下，可以使用 Redis 自带的序列化机制（如 Redis String 或 ByteArray），也可以使用 Redis 插件（如 Redisson）来更方便地存储复杂数据结构。
   - 如果 Redis 使用了 Redisson 插件，可以直接将 RoaringBitmap 对象存储并序列化，不需要手动转换为字节数组。

---

### **RoaringBitmap 的压缩算法**

RoaringBitmap 在存储过程中使用了多种压缩算法，分别用于不同类型的数据分布：

1. **ArrayContainer**：
   - 当存储的整数数量较少且分布稀疏时，使用 `ArrayContainer`，以 **有序数组** 形式来存储元素。
   - 这种容器没有额外压缩，但由于数据较少，直接存储为数组更加节省空间。
   - 查询复杂度为 `O(log n)`，使用二分查找，适合稀疏数据。

2. **BitmapContainer**：
   - 当存储的整数数量较多且相对密集时，使用 `BitmapContainer`，它是一个 64KB 的位图，表示范围在 0 到 65535 的所有可能值。
   - 对于密集数据场景（即大部分 bit 位都为 1），直接存储位图效率更高，查询的时间复杂度为 `O(1)`，通过位运算快速判断某个元素是否存在。

3. **RunContainer**：
   - 当存储的整数呈现连续分布时，使用 `RunContainer`，这种容器采用 **运行长度编码 (Run-Length Encoding, RLE)** 表示连续的 bit 位。
   - 这种容器只存储区间（run），每个区间包含一个起始值和终止值，通过区间存储连续数据，大大减少了空间消耗。
   - `RunContainer` 尤其适合具有大量连续位的位图，可以显著节省存储空间，并且查询时直接基于区间查找，查询效率较高。

---

### **RoaringBitmap 在 Redis 中的压缩与读取**

在 Redis 中存储和读取 RoaringBitmap 时通常有以下步骤：

1. **存储过程**：
   - 通过调用 `toByteArray()` 将 RoaringBitmap 序列化为字节数组。
   - 将字节数组存入 Redis，可以使用 `SET` 命令直接存储为二进制数据。
   
2. **读取过程**：
   - 从 Redis 中取出字节数组，并使用 `RoaringBitmap.bitmapOf(bytes)` 或者 `RoaringBitmap.deserialize(byte[])` 等方法将字节数组反序列化为 RoaringBitmap。
   - 反序列化后，RoaringBitmap 中的容器（`ArrayContainer`、`BitmapContainer`、`RunContainer`）会自动恢复原始数据结构。

---

### **总结**

- RoaringBitmap 在 Redis 中以字节数组的二进制格式存储，通常通过 `toByteArray()` 序列化。
- RoaringBitmap 的压缩算法包括 `ArrayContainer`（适合稀疏数据）、`BitmapContainer`（适合密集数据）和 `RunContainer`（适合连续数据），它们结合了数组存储、位图存储和 RLE 编码。
- `RunOptimize` 是 RoaringBitmap 的优化方法，可以将大量连续 bit 设置转换为 `RunContainer`，大大减少存储空间并提高查询效率。
```



### 1. 直接映射方案

#### **描述**

将业务 ID【转为十进制】 直接映射到位图中的 bit 位，存储在 bitmap 数据结构中。如果支持 40 亿个业务 ID，使用大约 512MB 的位图（2^32 位），适用于业务 ID 在一定范围内且不重复的情况。

#### **实现步骤**

1. **映射：** 直接使用业务 ID 作为 bit 位的索引。
2. **存储：** 使用  bitmap 存储这些 bit 位。
3. **反解：** 可以直接从 bit 位判断对应业务 ID是否存在。

#### **优缺点**

- **优点：**
  - **无哈希冲突：** 避免了哈希冲突问题。
  - **反解简单：** 可以直接从 bit 位索引中获取业务 ID。
- **缺点：**
  - **业务 ID 范围限制：** 适用场景限制较多，业务 ID 需要在一定范围内（例如，0 到 40 亿）且不重复。
  - **内存消耗：** 可能会遇到内存消耗较大的问题，特别是在 ID 范围很大的情况下。

### 2.**映射表与位图结合方案**

#### **描述**

将业务 ID 和其对应的 bit 位存储在一个映射表中，并将实际数据存储在bitmap 中。此方案结合了映射表和位图的优点。

#### **实现步骤**

1. **创建映射表：** 使用一个关系型数据库或 NoSQL 数据库存储**业务 ID 和其对应的 bit 位**。例如：uid对应一个自增id【从0开始】
2. **存储位图数据：** 使用 bitmap 存储位图数据。
3. **反解：** 从位图中查询 bit 位，然后去映射表查询相应的id

#### **优缺点**

- **优点：**
  - **精确映射：** 映射表可以提供准确的 bit 位与业务 ID 的映射。
  - **可扩展性：** 映射表可以支持复杂的查询和操作。
- **缺点：**
  - **存储开销：** 映射表会增加额外的存储开销，特别是当业务 ID 数量很大时。
  - **维护复杂：** 需要维护两个数据存储系统（例如，bitmap和存储映射表的mysql），增加了系统的复杂性。

### 3.固定长度**分段位图方案**

将位图分段存储，每个段负责一部分 bit 位。通过分段位图方案可以管理大范围的 ID，并且支持高效的存取操作。如果支持 40 亿个业务 ID，可以分为40个段，每个段存储1亿。

反解：根据bitmap的段索引和偏移量还原业务id

#### 按照固定长度分段

- **实现方式**：

  1. **总位图长度**：例如 40 亿位
  2. **每段长度**：例如 1 亿位
  3. **段数**：40 个段（40 亿位 / 1 亿位）

  **计算方式**：

  - **段索引** = 业务 ID // 每段长度
  - **位位置** = 业务 ID % 每段长度

**优缺点**：

- **优点**：
  - 实现简单，段的大小一致，容易管理。
  - 可以直接将数据分布到每个段中，减少了内存开销。
  - **动态扩展时，只需增加更多段**。
- **缺点**：
  - 可能会有额外的性能开销，例如段定位和跨段操作的开销。
  - 需要管理多个段，增加系统复杂性。

### 4.高位和低位分段位图方案

假设有 40 亿业务 ID 需要管理，并且希望进行高位和低位分段：

1. **高位分段**：
   - 将业务 ID 的高位分为 10 个段，每个段处理 4 亿位。
   - 高位分段使得 ID 在各个大段中分布，每个段包含更小范围的数据。
2. **低位分段**：
   - 在每个高位段内部，根据低位进一步分段，每段处理 1 亿位。
   - 这样每个高位段内部可以处理更精细的数据，从而提升访问效率。
3. 反解：需要额外数据结构记录bit位到业务id的映射

**优缺点**：

- **优点**：

  - **大范围数据管理**：

    - **场景**：当需要处理的数据范围非常大，例如数十亿或数百亿的业务 ID。

    - **优势**：高位分段可以有效地管理和组织大规模的数据。低位分段可以进一步细化和优化在特定段内的数据访问。

  - **热点数据隔离**：
    - **场景**：业务数据访问存在明显的热点（即某些数据被访问的频率显著高于其他数据）。
    - **优势**：通过高位分段，可以将热点数据分散到不同的段中，从而避免热点数据集中在一个段内造成的性能瓶颈。
  - **数据动态扩展**：
    - **场景**：系统需要支持动态扩展，例如从 40 亿 ID 扩展到 80 亿 ID。
    - **优势**：高位分段可以使数据扩展更加灵活。在扩展时，只需增加新的段来处理新的 ID 范围，原有的段可以继续处理旧的数据。

- **缺点**：

  - 实现复杂度高，需要解析业务 ID 的不同部分。
  - 可能需要更复杂的计算和映射逻辑来确定具体的位位置。

### 分段优缺点

```
为了详细解释页表为什么要分段，我们通过一个具体的例子来展示单级页表和多级页表的内存开销及地址转换的过程。

### 一、假设的系统参数

- **虚拟地址空间**：32 位虚拟地址空间，即虚拟地址的范围为 0 到 4GB。
- **页大小**：4KB（也就是 2^12 字节），所以一个页表项管理 4KB 的内存。
- **物理内存**：16MB。
- **每个页表项大小**：4 字节。

### 二、单级页表

1. **虚拟地址到物理地址转换**：
   单级页表的概念非常简单。每个页表项存储虚拟页号到物理帧号的映射。虚拟地址被分为两部分：
   - 高 20 位表示虚拟页号，用于查找页表项。
   - 低 12 位表示页内偏移，用于确定页帧中的具体地址。

2. **页表大小**：
   虚拟地址空间为 32 位，其中低 12 位用于页内偏移，因此页表需要包含 2^20 个页表项。每个页表项占 4 字节，单级页表的总大小为：
```
   2^20 * 4 字节 = 4MB
   ```

   这意味着，即使程序只使用了很少的虚拟内存（比如 1MB），整个页表依然需要 4MB 的空间，哪怕其中大部分页表项都是无效的。这是单级页表的内存浪费问题。

3. **内存访问过程**：
   当 CPU 访问一个虚拟地址时，它会根据虚拟页号查找页表，并将找到的物理帧号与页内偏移结合，得到物理地址。整个查找过程只需访问一次页表。

### 三、多级页表（两级页表）

为了节省内存开销，我们引入两级页表。两级页表通过将页表拆分为多级，只有在需要时才分配下一级页表，减少内存开销。

1. **虚拟地址分割**：
   在两级页表中，虚拟地址的前 20 位被分成两部分：
   - **前 10 位**用于查找第一级页表（一级页表项）。
   - **中间 10 位**用于查找第二级页表（第二级页表项）。
   - **最后 12 位**表示页内偏移。

2. **两级页表的结构**：
   - **第一级页表**：包含 2^10 个页表项，每个页表项指向一个第二级页表。
   - **第二级页表**：每个一级页表项对应一个 4KB 的第二级页表，第二级页表包含 2^10 个页表项，每个页表项指向一个物理页帧。

3. **内存开销对比**：
   - **单级页表内存开销**：4MB（前面计算过）。
   - **两级页表内存开销**：假设程序只使用了虚拟地址空间中的前 1MB 和后 1MB（稀疏的虚拟地址空间），只需为这些区域分配页表项。

     - 第一级页表：始终需要分配，大小为 2^10 * 4 字节 = 4KB。
     - 第二级页表：前 1MB 和后 1MB 虚拟地址分别需要 1 个第二级页表，每个第二级页表大小为 4KB。因此只需要 2 个第二级页表，即 2 * 4KB = 8KB。

     **总内存开销**为：
   ```
     4KB（第一级页表） + 8KB（第二级页表） = 12KB
     ```

   对比可以看出，单级页表需要 4MB，而两级页表只需要 12KB，极大减少了页表的内存开销。

4. **内存访问过程**：
   - 当 CPU 访问虚拟地址时，首先根据虚拟地址的前 10 位查找第一级页表，得到对应的第二级页表地址。
   - 然后根据虚拟地址的中间 10 位查找第二级页表，得到对应的物理帧号。
   - 最后，将物理帧号和虚拟地址的页内偏移结合，得到最终的物理地址。

   虽然两级页表节省了内存开销，但每次内存访问需要访问两次页表，增加了地址转换的开销。

### 四、举例说明

假设程序需要访问虚拟地址 `0x12345678`，我们使用两级页表进行地址转换：

1. **虚拟地址划分**：
   - 高 10 位：`0x123`（用来查找第一级页表）。
   - 中间 10 位：`0x045`（用来查找第二级页表）。
   - 低 12 位：`0x678`（页内偏移）。

2. **第一级页表查找**：
   - 第一级页表项位于 `0x123` 位置，指向一个第二级页表。

3. **第二级页表查找**：
   - 根据 `0x045` 查找第二级页表，得到物理帧号。

4. **计算物理地址**：
   将物理帧号与页内偏移 `0x678` 结合，得到物理地址。

### 五、分段页表的优缺点总结

**优点**：
1. **节省内存开销**：在处理大而稀疏的虚拟地址空间时，多级页表能够显著减少页表占用的内存。
2. **按需分配**：只为实际使用的虚拟地址空间分配页表，避免了内存浪费。
3. **扩展性好**：多级页表可以轻松适应更大的虚拟地址空间，适合 64 位系统。

**缺点**：
1. **增加访问开销**：多级页表在地址转换时需要多次查找页表项，增加了内存访问的开销。
2. **实现复杂**：多级页表的实现需要操作系统和硬件支持，增加了系统设计的复杂性。

### 六、总结

页表分段（多级页表）的设计是为了平衡内存开销和地址转换性能。在实际应用中，多级页表能够显著减少页表的内存占用，特别是在虚拟地址空间稀疏使用的场景下。然而，它也引入了额外的访问开销，通常需要借助 TLB 缓存来减少这种开销。通过具体的例子，我们可以更好地理解多级页表如何工作以及它带来的内存节省效果。
```



### 5.**哈希映射方案**

使用哈希函数将业务 ID 映射到位图中的 bit 位，存储在 bitmap 数据结构中

#### **实现步骤**

1. **哈希函数映射：** 使用哈希函数（如 SHA-256）将业务 ID 映射到 bit 位。
2. **存储：** 使用 bitmap 存储映射后的 bit 位。
3. **反解：** 使用一个反向映射表来记录 bit 位到业务 ID 的映射，支持从 bit 位反解业务 ID。

#### **优缺点**

- **优点：**
  - **简单高效：** 哈希映射是简单且高效的方式，适合快速存储和访问，并且不许额外存储id到位的映射
- **缺点：**
  - **哈希冲突：** 可能存在哈希冲突，需要处理冲突情况（例如，通过链表或开放寻址法）。
  - **反向映射表：** 需要维护一个反向映射表来支持反解，增加存储和管理开销。

`RunOptimize` 是 RoaringBitmap 提供的一个优化方法，专门用于处理具有大量连续 bit 设置的位图。在某些情况下，位图中可能包含大量的连续数字（即连续的 bit 位都被设置为 1）。`RunOptimize` 可以将这些连续的 bit 位压缩成运行长度编码（Run-Length Encoding, RLE）的形式，从而大幅减少存储空间并提高查询效率。

#### 1. **`RunOptimize` 的工作原理**

- **检测连续 bit**：`RunOptimize` 会扫描 RoaringBitmap 中的所有容器，检测其中是否存在大量的连续 bit 位（例如，1000 到 2000 都被设置为 1）。
- **转换为 RunContainer**：对于检测到的连续 bit 位，`RunOptimize` 会将其转换为 `RunContainer`，这种容器使用 RLE（运行长度编码）来表示连续的 bit 位。通过这种方式，位图可以大幅压缩存储空间。
- **减少查询复杂度**：一旦连续 bit 位被转换为 `RunContainer`，在执行 `Contains`、`Select` 或其他查询操作时，可以直接通过区间查找，而不必逐 bit 遍历，极大提高查询速度。

#### **适用场景**

- **连续数据场景**：如果你的数据集包含大量的连续数据，例如在日志系统中时间戳范围内的数据，或用户 ID 的连续段，`RunOptimize` 可以显著提高存储和查询的效率。
- **存储空间优化**：在对空间敏感的场景中（如内存受限的环境），`RunOptimize` 可以通过压缩连续 bit 位来节省大量存储空间。

### 1. **`ArrayContainer`**

- **使用场景**：当存储的整数数量较少且分布较为稀疏时使用。

- **内部实现**：`ArrayContainer` 使用有序数组来存储元素。

- **容量条件**：当存储的整数数量少于一定的阈值（通常是 4096 个元素）时，`RoaringBitmap` 会选择使用 `ArrayContainer`。

- 特点

  - 存储效率高：因为数据量少，用数组比用位图更节省空间。
  - 查询复杂度为 `O(log n)`，使用二分查找。

**举例**：假设你有一个范围在 0 到 65535 之间的整数集合，但集合中只包含少数几个数字（如 10、100、1000）。此时，`RoaringBitmap` 会使用 `ArrayContainer` 存储这些值。

### 2. **`BitmapContainer`**

- **使用场景**：当存储的整数数量较多且相对密集时使用。

- **内部实现**：`BitmapContainer` 使用一个 64KB 的位图来表示 0 到 65535 范围内的所有可能值，每个 bit 对应一个值。

- **容量条件**：当存储的整数数量超过一定的阈值（通常是 4096 个元素），并且这些元素集中在较小的范围内时，`RoaringBitmap` 会选择使用 `BitmapContainer`。

- 特点

  - 查询效率高：查询的时间复杂度为 `O(1)`，通过位运算快速判断某个元素是否存在。
  - 空间效率适中：适用于数据分布密集的场景。

**举例**：如果你有一个范围在 0 到 65535 之间的整数集合，并且集合中包含了大量的值（如 30000 个不同的整数），`RoaringBitmap` 会选择使用 `BitmapContainer` 存储这些值。

### 3. **`RunContainer`**

- **使用场景**：当存储的整数呈现连续性分布时使用。

- **内部实现**：`RunContainer` 使用区间（runs）来存储连续的值。每个区间表示一个起始值和一个终止值，涵盖范围内的所有整数。

- 特点

  ：

  - 对于连续范围的数据，`RunContainer` 存储效率最高，能够极大压缩存储空间。
  - 查询效率较高，通常为 `O(log m)`，其中 `m` 是区间的数量。



在 RoaringBitmap 中，如果位图中存在大量连续的 bit 位，可以通过调用 `RunOptimize` 方法优化为 `RunContainer`。然后再使用专门处理连续位的算法，这样可以更快地判断一个范围内是否有已设置的位。

```


这是典型的大数据处理场景下的查询性能优化问题，但确实在 StarRocks 上具体表现会略有不同。针对您关心的腾讯会议使用 StarRocks 的场景，让我们通过具体例子来更深入理解优化前的性能问题，以及 StarRocks 在应对大规模数据和复杂查询上的表现。

------

### **腾讯会议的 StarRocks 优化前场景分析**

假设腾讯会议通过 StarRocks 存储和分析用户的多维度行为数据，包括多值字段（如参加的活动 `join_events`、兴趣标签 `interest_tags` 等），这些数据直接存储在 StarRocks 中。StarRocks 的原生结构和优化技术本身确实与普通关系型数据库不同，但对于一些特定场景的优化，还是可能面临优化前的瓶颈。以下通过具体案例来说明：

------

### **1. 表结构示例：存储用户行为数据**

假设我们有一个用户行为表 `user_behavior`，存储了用户参加的活动、兴趣标签和其他信息。表结构如下：

```
sql


复制代码
CREATE TABLE user_behavior (
    user_id BIGINT NOT NULL,
    join_events ARRAY<BIGINT>,      -- 用户参与的活动
    interest_tags ARRAY<BIGINT>,    -- 用户的兴趣标签
    device_types ARRAY<BIGINT>,     -- 用户的设备类型
    ds DATE NOT NULL,
    PRIMARY KEY (user_id, ds)
)
DISTRIBUTED BY HASH(user_id)
PARTITION BY RANGE(ds);
```

在优化前，`join_events` 和 `interest_tags` 字段都被定义为数组（`ARRAY`）类型，用于存储每个用户的多个行为或兴趣信息。这种存储方式会带来以下性能问题。

------

### **2. 查询性能问题：复杂的数组包含查询**

假设运营人员需要筛选出以下符合条件的用户：既参加了活动 A 和 B，又对 C 标签感兴趣。查询 SQL 如下：

```
sql


复制代码
SELECT user_id
FROM user_behavior
WHERE ds = '2024-10-10'
AND ARRAY_CONTAINS(join_events, A) 
AND ARRAY_CONTAINS(join_events, B) 
AND ARRAY_CONTAINS(interest_tags, C);
```

#### **性能瓶颈分析**

1. **数组包含查询（`ARRAY_CONTAINS`）的复杂性**：
   - 对于 `join_events` 和 `interest_tags` 数组字段，StarRocks 需要逐个遍历每个用户的数组，找到符合条件的元素。这种逐行扫描数组的方式耗费大量 CPU 和 IO 资源，特别是在数据量大时，查询性能急剧下降。
2. **缺乏索引支持**：
   - 在数组类型的多值字段上，StarRocks 没有办法直接创建索引。这意味着每次查询都需要全表扫描或分区扫描，难以有效利用索引进行快速数据定位。
3. **数据裁剪效率低**：
   - 由于数组存储在行级数据中，即使分区裁剪能够减少扫描的数据范围，StarRocks 依然需要对选定分区内的每行数据执行数组遍历。对于特定查询条件（如多个活动或兴趣交集），仍会进行大量无效数据的计算。

------

### **3. 优化前的场景：示例情况**

假设在腾讯会议系统中，每天有数百万用户的行为数据存入 StarRocks。运营人员需要筛选过去一周内的用户数据，以了解哪些用户在该时间范围内符合特定的活动参与和兴趣偏好条件。优化前的查询性能表现如下：

- **数据量**：每天数百万用户，`user_behavior` 表包含数亿行数据。
- **查询时间**：由于每次查询需要逐个扫描和遍历数组字段，响应时间长达 **数十秒甚至数分钟**。
- **CPU 利用率高**：频繁查询导致 CPU 负载接近 **100%**，影响系统的整体性能和其他任务的运行。
- **无法快速响应**：由于查询响应时间长，实时筛选和用户分群的体验较差，运营人员无法快速完成数据分析和反馈。

------

### **优化前的具体问题**

- **存储空间浪费**：使用数组存储多值字段，会导致空间冗余。尤其是数据稀疏时，每个数组字段可能包含大量空值或重复数据，浪费存储空间。
- **数组计算复杂**：在数组上执行多次交集或并集运算（如多活动、多标签的筛选条件）耗时长，查询响应慢。
- **资源消耗高**：由于 StarRocks 不支持直接对数组字段创建索引，查询过程需大量 CPU 和内存资源，尤其是多值字段数量大、数据量多时，系统资源消耗更高。

------

### **总结**

在腾讯会议的大规模数据分析场景中，使用 StarRocks 存储和查询多值字段时，可能会遇到以下问题：

1. **存储效率低**：数组结构的多值字段在存储上不够紧凑，增加了存储和维护成本。
2. **查询性能不佳**：对数组字段的交并集查询在数据量大时会导致响应延迟。
3. **高资源占用**：数组运算导致的 CPU 和内存消耗增加，影响其他查询任务，系统稳定性下降。

### **解决方案建议**

针对上述问题，可以通过 **Bitmap 替代数组** 和 **分层索引优化** 来提升查询性能，这样能够大幅减少 CPU 占用，并显著加快多维条件下的查询速度。





go
复制代码
rb.RunOptimize()
results := GetSetBitsInRange(rb, start, end)

```

### 优化点：

- **压缩连续位**：`RunContainer` 对连续 bit 位的压缩使得查询时无需逐 bit 检查，而是可以通过区间快速判断。
- **提高性能**：特别适合连续位比较多的场景，大幅提高查询效率。


为了详细解释 Lazada 选品平台升级为 Hologres 数据库后面临的两个主要问题——**大宽表查询**和**选品池操作瓶颈**，我将逐一解释背景、问题原因，并通过具体示例来说明这些问题。

---

### 1. **大宽表问题**

#### 背景：
大宽表（Wide Table）是指含有大量字段的数据库表，通常用于记录复杂对象（如商品）的多个维度数据。Lazada 平台中的大宽表包含接近 300 个字段，每个字段对应不同的商品属性或维度信息。例如：

- **商品基础信息**：商品 ID、名称、价格、库存等。
- **多值字段**：商品所属类目（可以属于多个类目）、报名活动（商品可以同时参加多个活动）、行业归属等。

这些多值字段（例如类目、活动）使用数组类型字段存储。为了对数据进行分析，运营人员可能会查询符合多个特定条件的商品，这时会涉及到多值字段的交集运算。

#### 问题说明：
假设运营人员想查询符合以下条件的商品：
- 属于“电子产品”类目。
- 正在参与“促销活动A”和“促销活动B”。
  

查询会对**商品所属类目**和**报名活动**字段执行数组交集运算，找出符合条件的商品。由于数据量庞大，这些多值字段可能包含成千上万个元素。数组的交集运算会将 CPU 占用率拉到 100%，这是因为：

- **数组交集的计算量大**：对于每个商品，需要遍历其所属类目数组和活动数组，检查是否存在符合条件的元素。
- **资源消耗高**：每次查询都需要对整个数组进行遍历和比较，导致 CPU 资源被过度消耗。

#### 示例：
假设表结构如下（简化版）：

```sql
CREATE TABLE product_table (
    product_id BIGINT,
    categories TEXT[],       -- 商品类目数组
    activities TEXT[],       -- 报名活动数组
    ...
);
```

运营查询的 SQL 示例：

```sql
SELECT product_id
FROM product_table
WHERE ARRAY['电子产品'] <@ categories
  AND ARRAY['促销活动A', '促销活动B'] <@ activities;
```

在这里，`<@` 是 PostgreSQL 中的“包含”操作符，它会对每个数组元素进行遍历和比较。在多值字段数据量大的情况下，这种遍历会耗费大量的 CPU 资源，导致 CPU 占用率冲高，甚至影响其他查询任务的性能。

---

### 2. **选品池操作瓶颈**

#### 背景：
选品池是一个集合，用于记录符合选品规则的商品或商家 ID。选品池数据量庞大，且每天会随着业务规则的调整进行更新。运营人员会根据最新规则打标，确定哪些商品或商家需要纳入选品池，或从选品池中去除。选品池的数据是按天记录的，这样可以对比当天和前一天的变化，实现增量更新。

#### 问题说明：
由于选品池数据量庞大，Lazada 平台的选品池操作面临以下两个核心问题：

1. **数据冗余**：每天的选品池数据需要与前一天进行比较，导致每天的数据都要重复存储。
2. **内存消耗高、操作耗时**：**当前选品池操作是在应用的内存中进行大量集合运算。如果选品池中的商品 ID 数量过多（例如超过 20 万），则容易触发 Java 应用的 Full GC，从而影响系统性能。Full GC 频繁会导致延迟增加，每个选品池调度耗时较长，整体任务调度可能需要 12 小时。**

#### 示例：
假设每天选品池的数据如下：

| 日期 | 商品 ID 集合                        |
| ---- | ----------------------------------- |
| 今天 | {1001, 1002, 1005, 1010, 1020, ...} |
| 昨天 | {1001, 1003, 1006, 1010, 1030, ...} |

业务逻辑要求每天将“今天”和“昨天”的商品 ID 集合进行对比，找出新增和移除的商品。这种集合操作需要在应用内存中执行，因此在数据量大时很容易触发 Full GC。

- **集合操作**：需要计算今天与昨天选品池的差集或交集，例如找出今天新增的商品和移除的商品。
- **Full GC 频繁触发**：如果商品 ID 集合超过 20 万，应用的 JVM 堆内存会消耗很大，频繁触发 Full GC，导致系统卡顿和延迟增加。

#### 集合操作的 SQL 示例（使用数组表示选品池）：

```sql
-- 查询今天新增的商品
SELECT product_id
FROM today_pool
WHERE product_id NOT IN (SELECT product_id FROM yesterday_pool);
```

由于 `NOT IN` 操作在数据量大时效率较低，容易造成数据库的性能瓶颈，进一步加重了 Full GC 的频率，导致系统的内存和 CPU 资源被大量占用。

---

### 问题总结

- **大宽表查询问题**：多值字段的数组交集计算导致 CPU 占用过高，影响数据库查询性能。
- **选品池操作瓶颈**：选品池集合操作在大数据量情况下内存消耗过大，频繁触发 Full GC，导致系统性能下降，调度时间延长。







![image-20241030151442704](/Users/haozhipeng/Library/Application Support/typora-user-images/image-20241030151442704.png)



### 示例背景

假设我们有一个用户群组分析系统，需要根据用户的行为事件和标签信息，对特定用户群体进行分析。假设系统中有一个用户 ID 为 `uid=123456`，企业 ID 为 `corp_id=789` 的用户，系统会根据这个用户的行为数据和标签信息，动态计算用户属于哪些群体，并通过图片弹窗等方式给用户反馈个性化信息。

### 详细流程解析

1. **uid-id 映射表（Iceberg uid 加密）**
   - 首先，用户 `uid=123456` 和企业 `corp_id=789` 的映射信息被存储在 Iceberg 表中，并加密保存。
   - Iceberg 表中有一张 `uid-id 映射表`，这个表用于将外部系统的用户 ID (`uid=123456`) 映射到内部系统 ID（比如 `id=10001`）。
   - 这样做的好处是确保系统内部使用的 ID 不直接暴露用户的真实 ID，提高了数据的安全性。
2. **行为事件存储（dwd 和 ods 表）**
   - 用户在平台上发生了一些事件（比如浏览、点击等），这些事件按照层次结构存储在 `dwd` 表中。
   - 事件还被进一步分桶，存储在 `ods` 表中，这样可以对不同用户的行为数据进行高效的分区管理，加速查询速度。
   - 比如：`uid=123456` 的用户在某天的浏览事件和点击事件会被分别存储在 `dwd` 和 `ods` 表的特定分区中，方便后续查询和分析。
3. **人群选择与压缩**
   - 系统需要对用户 `uid=123456` 进行人群分析，比如该用户是否属于“活跃用户群体”或“高消费用户群体”。
   - 首先，系统通过计算，将符合条件的用户群体生成一个 **bitmap**（位图），位图的每一位对应一个用户。如果用户属于某个群体，位图中对应的位置就标记为 `1`。
   - 例如，如果 `uid=123456` 属于“活跃用户群体”，则 bitmap 中第 `10001` 位（假设映射表中的 ID 为 `id=10001`）会被置为 `1`。
   - 然后，生成的人群 bitmap 会通过 `sr sql API` 导入到 StarRocks 集群的 `人群 bmp` 表中，用于后续的查询和分析。
4. **用户标签与图像标签**
   - 用户 `uid=123456` 还拥有多个标签，比如年龄、性别、兴趣等，这些信息存储在两个标签表中（`用户标签表1` 和 `用户标签表2`）。
   - 系统会对标签表中的标签进行 bitmap 压缩，生成对应的标签位图（`用户标签1-bmp` 和 `用户标签2-bmp`），便于快速判断用户是否具备某些特定标签。
   - 另外，系统还会生成用户的图像标签，比如用户的头像图像，经过 Spark 压缩后生成位图，存储在 `图像标签 bmp` 中，以供后续使用。
5. **数据查询与映射服务**
   - 当系统需要针对用户 `uid=123456` 进行推送或分析时，会使用 **uid, corp_id, id mapping 服务 p1** 从映射表中查询出用户在系统内部的 ID，比如 `id=10001`。
   - 如果需要进一步查询，可以通过 **反查 uid + 解密** 来获取原始的 `uid`，并从 **kvdb** 数据库中提取相关数据。
   - 例如，在系统内查询 `id=10001` 是否属于“活跃用户群体”，系统会读取 `人群 bmp` 中的第 `10001` 位来判断。如果该位为 `1`，则说明用户属于“活跃用户群体”。
6. **下发与弹窗**
   - 如果系统判定 `uid=123456` 属于某个目标群体（比如符合推送条件的用户群体），则会将数据下发到 **kvdb** 数据库中，并进行推送。
   - 具体推送时，会生成 **Text 组** 和 **group_id**，这用于判断用户是否会看到一个弹窗。
   - 比如，如果 `group_id=123` 的用户触发了推送条件，系统会显示个性化的弹窗，展示内容可能包括推荐产品或通知消息。
7. **图像计算服务**
   - 最后，系统还可以利用 **bmp 图像计算服务 p0** 生成一些基于位图的图像结果，这个服务会从 `图像标签 bmp` 中提取数据，计算得到图像结果。
   - 例如，系统可以根据用户 `uid=123456` 的标签，生成一个个性化的头像或展示图片，供用户在界面中查看。

------

### 外表模式与内表模式

- **外表模式**：当用户群体需要进行快速扩展时，系统会使用外表模式。外表模式支持在几分钟内更新人群选择的结果，因此适合用户群体随时变动的场景。
- **内表模式**：内表模式用于需要秒级响应的场景，保证人群提取和计算效率最高。比如在实时计算某个用户是否符合特定群体条件时，可以使用内表模式实现秒级反馈。

------

### 



### 实时性保证

```
1. 增量更新与异步生成机制
假设我们有一个电商平台，用户在页面上浏览了多种商品，系统会根据用户的浏览行为判断该用户是否属于“活跃用户”，并推送个性化的产品推荐。

增量更新：

用户 uid=12345 刚刚浏览了一个商品页面，触发了活跃状态的更新逻辑。
系统会立即在内存或内表中记录该用户的状态，将 uid=12345 标记为“活跃用户”。
这样，系统不需要等待生成完整的 bitmap，而是直接记录这次活跃状态的增量更新，以实现实时推送。
异步生成和批量导入：

系统的后台任务每隔一分钟会根据最新的用户行为数据生成完整的“活跃用户”bitmap，涵盖了所有活跃用户。
生成完 bitmap 后，系统会使用 sr sql API 将其导入 StarRocks 集群的 人群 bmp 表中。
这个表提供非实时查询，便于后续的分析或下次推送。
优点：增量更新提供了实时性，批量生成 bitmap 保证数据的一致性和高效存储。

2. 双通道机制：实时判断和批量计算结合
假设我们有一个新闻平台，当用户阅读了一定数量的文章后，系统会将该用户识别为“活跃用户”，并推送热门文章推荐。为满足实时性需求，系统采取了双通道机制：

实时判断通道：
用户 uid=56789 连续阅读了3篇新闻，系统即时判断该用户是否符合“活跃用户”条件。
系统通过规则引擎（例如 Apache Flink）判断用户 uid=56789 是否达到了活跃用户标准。如果符合，则立即将 uid=56789 标记为“活跃用户”，并触发个性化的推荐推送。
批量计算通道：
系统会每隔 10 分钟汇总过去 10 分钟内所有用户的阅读行为，生成新的活跃用户 bitmap。
然后系统会将这个 bitmap 导入 人群 bmp 表中，这样后续查询可以使用这个批量生成的数据，提高查询效率。
优点：实时判断通道提供了及时性，批量计算通道提供了数据的全面性。两者结合可以确保实时推送和批量分析的高效进行。

3. 缓存实时状态
在社交媒体平台中，用户的点赞、评论、分享等行为可以用于判断是否为“活跃用户”。平台通过缓存机制来实现实时判断：

实时状态缓存：
用户 uid=24680 在一段时间内点赞了多篇帖子，系统会在 Redis 缓存中将该用户标记为“活跃用户”。
该记录会存在 Redis 的 Set 或 Hash 中，例如 active_users_set。每当用户触发某个行为时，系统会实时更新这个缓存状态。
数据导入：
每小时，系统会将 Redis 缓存中所有活跃用户的数据导出，生成最新的“活跃用户” bitmap，并将其导入到 人群 bmp 表中。
人群 bmp 表中的数据用于非实时分析和生成用户群体报告。
优点：缓存机制确保了毫秒级的状态判断，适合高频访问。缓存中的数据定期写入 人群 bmp 表，保证了系统数据的完整性和一致性。

4. 分层判断逻辑
在金融理财平台上，系统会根据用户的登录频率和投资行为来判定用户的活跃程度。系统使用分层判断逻辑来处理实时和非实时的判断需求：

行为触发型活跃用户：
用户 uid=13579 当天连续登录3次，并完成了一次投资操作。系统直接将其判定为行为触发型活跃用户。
系统通过规则引擎即时判定，并将 uid=13579 添加到内表中的“活跃用户”列表，以便立即推送个性化理财产品推荐。
周期型活跃用户：
系统会定期（例如每天晚上）汇总用户的登录和投资记录，生成过去一周内所有“周期型活跃用户”的 bitmap。
比如，如果用户一周内登录次数超过 5 次，或者进行了多次投资交易，则标记为“周期型活跃用户”。
系统将这些周期型活跃用户的数据批量导入 人群 bmp 表，供第二天的业务使用。
优点：行为触发型活跃用户的判断满足了实时需求，周期型活跃用户的数据通过定期批量更新，满足非实时分析需求。这种分层逻辑提高了系统的灵活性和响应速度。


```

